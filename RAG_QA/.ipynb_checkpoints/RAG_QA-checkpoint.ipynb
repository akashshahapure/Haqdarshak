{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a350229a-b5ae-4116-a2c4-8ec36994c87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "with open(r'C:\\Python\\HQgeminiAPIKey.txt', 'r')as HQfile:\n",
    "    GOOGLE_API_KEY = HQfile.read()\n",
    "os.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY\n",
    "with open(r'C:\\Python\\geminiAPIKey.txt', 'r')as file:\n",
    "    GOOGL_EMBED_API_KEY = file.read()\n",
    "os.environ['GOOGL_EMBED_API_KEY'] = GOOGL_EMBED_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "819119cc-503e-45a5-88d9-6eda0d778e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing required libraries...\n",
      "Required libraries imported!\n"
     ]
    }
   ],
   "source": [
    "print('Importing required libraries...')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from glob import glob\n",
    "from datetime import datetime as dt\n",
    "import time\n",
    "import faiss\n",
    "from langchain.rate_limiters import InMemoryRateLimiter\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings as google_embed\n",
    "rate_limiter = InMemoryRateLimiter(requests_per_second=0.1,\n",
    "                                   check_every_n_seconds=0.1,  # How often the limiter checks if a request is allowed\n",
    "                                   max_bucket_size=10,)         # Maximum burst size\n",
    "gemini = ChatGoogleGenerativeAI(model='gemini-2.5-flash', google_api_key=GOOGLE_API_KEY, rate_limiter=rate_limiter)\n",
    "embedding = google_embed(model = 'models/gemini-embedding-001', google_api_key=GOOGL_EMBED_API_KEY, rate_limiter=rate_limiter)\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import PyMuPDFLoader, UnstructuredPDFLoader\n",
    "from langchain_community.document_loaders.parsers import TesseractBlobParser, RapidOCRBlobParser\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from IPython.display import display, Markdown\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "print('Required libraries imported!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0693ea8e-c471-4c00-883a-ff4581b2638e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consider using the pymupdf_layout package for a greatly improved page layout analysis.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-24T00:04:08+00:00', 'source': 'Language Models are Few-Shot Learners.pdf', 'file_path': './PDF\\\\Language Models are Few-Shot Learners.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': 'Language Models are Few-Shot Learners', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-07-24T00:04:08+00:00', 'trapped': '', 'modDate': 'D:20200724000408Z', 'creationDate': 'D:20200724000408Z', 'page': 0}, page_content='Language Models are Few-Shot Learners\\nTom B. Brown∗\\nBenjamin Mann∗\\nNick Ryder∗\\nMelanie Subbiah∗\\nJared Kaplan†\\nPrafulla Dhariwal\\nArvind Neelakantan\\nPranav Shyam\\nGirish Sastry\\nAmanda Askell\\nSandhini Agarwal\\nAriel Herbert-Voss\\nGretchen Krueger\\nTom Henighan\\nRewon Child\\nAditya Ramesh\\nDaniel M. Ziegler\\nJeffrey Wu\\nClemens Winter\\nChristopher Hesse\\nMark Chen\\nEric Sigler\\nMateusz Litwin\\nScott Gray\\nBenjamin Chess\\nJack Clark\\nChristopher Berner\\nSam McCandlish\\nAlec Radford\\nIlya Sutskever\\nDario Amodei\\nOpenAI\\nAbstract\\nRecent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training\\non a large corpus of text followed by ﬁne-tuning on a speciﬁc task. While typically task-agnostic\\nin architecture, this method still requires task-speciﬁc ﬁne-tuning datasets of thousands or tens of\\nthousands of examples. By contrast, humans can generally perform a new language task from only\\na few examples or from simple instructions – something which current NLP systems still largely\\nstruggle to do. Here we show that scaling up language models greatly improves task-agnostic,\\nfew-shot performance, sometimes even reaching competitiveness with prior state-of-the-art ﬁne-\\ntuning approaches. Speciﬁcally, we train GPT-3, an autoregressive language model with 175 billion\\nparameters, 10x more than any previous non-sparse language model, and test its performance in\\nthe few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or ﬁne-tuning,\\nwith tasks and few-shot demonstrations speciﬁed purely via text interaction with the model. GPT-3\\nachieves strong performance on many NLP datasets, including translation, question-answering, and\\ncloze tasks, as well as several tasks that require on-the-ﬂy reasoning or domain adaptation, such as\\nunscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same\\ntime, we also identify some datasets where GPT-3’s few-shot learning still struggles, as well as some\\ndatasets where GPT-3 faces methodological issues related to training on large web corpora. Finally,\\nwe ﬁnd that GPT-3 can generate samples of news articles which human evaluators have difﬁculty\\ndistinguishing from articles written by humans. We discuss broader societal impacts of this ﬁnding\\nand of GPT-3 in general.\\n∗Equal contribution\\n†Johns Hopkins University, OpenAI\\nAuthor contributions listed at end of paper.\\narXiv:2005.14165v4  [cs.CL]  22 Jul 2020')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers=[]\n",
    "\n",
    "for file_path in glob('./PDF/*.pdf'):\n",
    "    loader = PyMuPDFLoader(file_path, mode=\"page\", images_inner_format=\"html-img\", images_parser=TesseractBlobParser(), extract_tables=\"markdown\",)\n",
    "    data = loader.load()\n",
    "    for doc in data:\n",
    "        doc.metadata['source'] = file_path.split('\\\\')[-1]\n",
    "        doc.metadata['title'] = file_path.split('\\\\')[-1].split('.')[0]\n",
    "        papers.append(doc)\n",
    "\n",
    "papers[15]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8eb359bb-820b-454d-8ab1-e1c50cf76ec5",
   "metadata": {},
   "source": [
    "file_path = r'./PDF/RAG for Knowledge-Intensive NLP Tasks.pdf'\n",
    "'''loader = UnstructuredPDFLoader(file_path = file_path, stratergy='hi-res', infer_table_structure=True, extract_images=True,\n",
    "                               image_output_dir_path='./images', mode='elements', chunking_strategy=\"by_title\",)'''\n",
    "\n",
    "loader = PyMuPDFLoader(file_path, mode=\"page\", images_inner_format=\"html-img\", images_parser=RapidOCRBlobParser(), extract_tables=\"markdown\",)\n",
    "doc = loader.load()\n",
    "doc[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b559d4e4-fe1c-4ed6-959f-32bbed0a3de7",
   "metadata": {},
   "source": [
    "# Filtering Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "679a6330-b1bf-4640-8b48-63cdae9e0195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': 'Attention Is All You Need.pdf', 'file_path': './PDF\\\\Attention Is All You Need.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': 'Attention Is All You Need', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 0}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_papers = filter_complex_metadata(papers)\n",
    "filtered_papers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a94004e-3063-4fdd-9d57-e5a7cb5b5dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': 'Attention Is All You Need.pdf', 'file_path': './PDF\\\\Attention Is All You Need.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': 'Attention Is All You Need', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 0}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size = 1500, chunk_overlap = 300)\n",
    "chunks = splitter.split_documents(filtered_papers)\n",
    "chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d594d418-e9a4-4e50-aa72-d1601b787406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': 'Attention Is All You Need.pdf', 'file_path': './PDF\\\\Attention Is All You Need.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': 'Attention Is All You Need', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 0}, page_content='to-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3201ad-7838-4975-856d-f333af25ab98",
   "metadata": {},
   "source": [
    "# Storing data in FAISS vector store\n",
    "- ### Run below cell only once"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f5f8725f-a216-4e37-839c-98fba397fc93",
   "metadata": {},
   "source": [
    "# Run this only once\n",
    "if os.path.isfile('./FAISS/index.faiss') and os.path.isfile('./FAISS/index.pkl'):\n",
    "    try:\n",
    "        print('Adding chunks to exisitng FAISS vector DB.')\n",
    "        faiss_vector_store.load_local('./FAISS')\n",
    "        faiss_vector_store.add_documents(chunks, embedding)\n",
    "        print('Added chunks to exisitng FAISS vector DB.')\n",
    "    except ResourceExhausted:\n",
    "        print('Token quota exceeded hence waiting for 10 seconds')\n",
    "        time.sleep(10)\n",
    "        print('Adding chunks to exisitng FAISS vector DB.')\n",
    "        faiss_vector_store.load_local('./FAISS')\n",
    "        faiss_vector_store.add_documents(chunks, embedding)\n",
    "        print('Added chunks to exisitng FAISS vector DB.')\n",
    "else:\n",
    "    print('Creating and adding FAISS vector DB.')\n",
    "    faiss_vector_store = FAISS.from_documents(chunks, embedding)\n",
    "    faiss_vector_store.save_local('./FAISS')\n",
    "    print('Created and added FAISS vector DB.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31aafb68-7242-48ae-9dd3-c71fb7b7cefa",
   "metadata": {},
   "source": [
    "# Loading FAISS vector datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "511ae120-9c92-4c1d-bbce-a466d1d3e5a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x17ba0020bd0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading FAISS Database\n",
    "faiss_vector_store = FAISS.load_local('./FAISS', embedding, allow_dangerous_deserialization=True)\n",
    "faiss_vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74a84ecb-df58-47f1-9f4d-0316caed1d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = faiss_vector_store.as_retriever(search_type='similarity', k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "097e2520-f8b9-4dda-9611-a7c4dcbaecbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='4b24d341-cae0-40c4-9a4a-f7706858b509', metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-24T00:04:08+00:00', 'source': 'Language Models are Few-Shot Learners.pdf', 'file_path': './PDF\\\\Language Models are Few-Shot Learners.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': 'Language Models are Few-Shot Learners', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-07-24T00:04:08+00:00', 'trapped': '', 'modDate': 'D:20200724000408Z', 'creationDate': 'D:20200724000408Z', 'page': 66}, page_content='Figure H.10: All results for all Scramble tasks.\\nFigure H.11: All results for all Translation tasks.\\n67'),\n",
       " Document(id='49114c6b-b34a-426f-8497-af678d117e07', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': 'Attention Is All You Need.pdf', 'file_path': './PDF\\\\Attention Is All You Need.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': 'Attention Is All You Need', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 12}, page_content='|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16|Col17|Col18|Col19|Col20|Col21|Col22|Col23|Col24|Col25|Col26|Col27|Col28|Col29|Col30|Col31|Col32|Col33|Col34|Col35|Col36|Col37|Col38|Col39|Col40|\\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\\n|||||||||||||||||||||||||||||||||||||||||\\n|||||||||||||||||||||||||||||||||||||||||\\n|||||||||||||||||ts||||||||||||||||||||||||\\n|||||||||||||||n|n|en||||||||||on||||||||||||||\\n||||||||||||ty|||ca|ca|m||d|||||g|g||ati|||s||t||>|||||||\\n|||||||t|t||||ori|||ri|ri|ern|e|se|||e|9|in|in||str||ng|es|e|ul||S|d>|d>|d>|d>|d>|d>|\\n||||||is|iri|iri|at|at||aj|||me|me|ov|av|as|ew|ws|nc|00|ak|ak|e|gi||ti|oc|or|ffic||EO|pa|pa|pa|pa|pa|pa|\\n|It|It|is|is|in|th|sp|sp|th|th|a|m|of|of|A|A|g|h|p|n|la|si|2|m|m|th|re|or|vo|pr|m|di|.|<|<|<|<|<|<|<|\\n|||||||||||||||||||||||||||||||||||||||||\\n|It|It|is|is|in|is|rit|rit|at|at|a|ty|of|of|n|n|ts|e|d|w|s|e|9|g|g|e|n|or|g|ss|re|ult|.|>|>|>|>|>|>|>|\\n||||||th|pi|pi|th|th||ori|||ica|ica|en|av|se|ne|aw|inc|00|kin|kin|th|tio||tin|e|o|ic||OS|ad|ad|ad|ad|ad|ad|\\n|||||||s|s||||aj|||er|er|m|h|as||l|s|2|a|a||tra||vo|roc|m|iff||E|<p|<p|<p|<p|<p|<p|\\n||||||||||||m|||m|m|rn||p|||||m|m||gis|||p||d||<|||||||\\n|||||||||||||||A|A|ve||||||||||re||||||||||||||\\n|||||||||||||||||go||||||||||||||||||||||||\\n|||||||||||||||||||||||||||||||||||||||||\\n|||||||||||||||||||||||||||||||||||||||||'),\n",
       " Document(id='54c484a3-f245-4f30-abf1-c7e0a4a59be4', metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-24T00:04:08+00:00', 'source': 'Language Models are Few-Shot Learners.pdf', 'file_path': './PDF\\\\Language Models are Few-Shot Learners.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': 'Language Models are Few-Shot Learners', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-07-24T00:04:08+00:00', 'trapped': '', 'modDate': 'D:20200724000408Z', 'creationDate': 'D:20200724000408Z', 'page': 62}, page_content='2.00\\n0.55 3.15\\n4.00 12.1 19.6 73.0 99.6\\n2.00\\n4.10 3.50\\n4.50 8.90 11.9 55.5 100.0\\n2D-\\nacc\\nn/a\\n50\\n1.25\\n1.25 1.25\\n1.25 1.60 7.60 12.6 58.0\\n1.15\\n0.95 1.45\\n1.95 3.85 11.5 44.6 86.4\\n1.15\\n1.45 2.25\\n2.70 7.35 13.6 52.4 98.9\\n3D+\\nacc\\nn/a\\n50\\n0.10\\n0.10 0.05\\n0.10 0.10 0.25 1.40 34.2\\n0.15\\n0.00 0.10\\n0.30 0.45 0.95 15.4 65.5\\n0.15\\n0.45 0.30\\n0.55 0.75 0.90 8.40 80.4\\n3D-\\nacc\\nn/a\\n50\\n0.05\\n0.05 0.05\\n0.05 0.05 0.45 1.35 48.3\\n0.05\\n0.15 0.25\\n0.30 0.55 1.60 6.15 78.7\\n0.05\\n0.10 0.15\\n0.35 0.65 1.05 9.20 94.2\\n4D+\\nacc\\nn/a\\n50\\n0.05\\n0.05 0.00\\n0.00 0.05 0.05 0.15 4.00\\n0.00\\n0.00 0.10\\n0.00 0.00 0.10 0.80 14.0\\n0.00\\n0.05 0.05\\n0.00 0.15 0.15 0.40 25.5\\n4D-\\nacc\\nn/a\\n50\\n0.00\\n0.00 0.00\\n0.00 0.00 0.00 0.10 7.50\\n0.00\\n0.00 0.00\\n0.00 0.05 0.00 0.50 14.0\\n0.00\\n0.05 0.00\\n0.00 0.10 0.05 0.40 26.8\\n5D+\\nacc\\nn/a\\n50\\n0.00\\n0.00 0.00\\n0.00 0.00 0.00 0.00 0.65\\n0.00\\n0.00 0.00\\n0.00 0.00 0.00 0.05 3.45\\n0.00\\n0.00 0.00\\n0.00 0.00 0.00 0.05 9.30\\n5D-\\nacc\\nn/a\\n50\\n0.00\\n0.00 0.00\\n0.00 0.00 0.00 0.00 0.80\\n0.00\\n0.00 0.00\\n0.00 0.00 0.00 0.05 3.75\\n0.00\\n0.00 0.00\\n0.00 0.00 0.00 0.00 9.90\\n2Dx\\nacc\\nn/a\\n50\\n2.20\\n2.25 2.65\\n2.10 2.55 5.80 6.15 19.8\\n1.35\\n2.35 3.35\\n2.35 4.75 9.15 11.0 27.4\\n1.35\\n2.90 2.70\\n2.85 4.25 6.10 7.05 29.2\\n1DC\\nacc\\nn/a\\n50\\n1.25\\n2.95 2.75\\n0.05 0.30 2.35 0.75 9.75\\n1.90\\n2.80 2.85\\n3.65 6.45 9.15 8.20 14.3\\n1.70\\n2.15 3.90\\n5.75 6.20 7.60 9.95 21.3\\nCycled Letters\\nacc\\nn/a\\n100\\n0.62\\n0.71 2.85\\n0.00 0.63 1.35 2.58 3.66\\n1.67\\n4.36 5.68\\n6.46 6.25 9.41 15.1 21.7\\n4.63\\n9.27 10.7\\n14.5 16.7 21.9 27.7 37.9\\nAnagrams 1\\nacc\\nn/a\\n100\\n0.10\\n0.14 0.40'),\n",
       " Document(id='04ff8a2e-06a9-4f5f-9612-480c2e7b6e08', metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-24T00:04:08+00:00', 'source': 'Language Models are Few-Shot Learners.pdf', 'file_path': './PDF\\\\Language Models are Few-Shot Learners.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': 'Language Models are Few-Shot Learners', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-07-24T00:04:08+00:00', 'trapped': '', 'modDate': 'D:20200724000408Z', 'creationDate': 'D:20200724000408Z', 'page': 23}, page_content='Figure 3.11: Few-shot performance on the ﬁve word scrambling tasks for different sizes of model. There is generally\\nsmooth improvement with model size although the random insertion task shows an upward slope of improvement with\\nthe 175B model solving the task the majority of the time. Scaling of one-shot and zero-shot performance is shown in\\nthe appendix. All tasks are done with K = 100.\\nrandom insertions, 38.6% on cycling letters, 40.2% on the easier anagram task, and 15.1% on the more difﬁcult anagram\\ntask (where only the ﬁrst and last letters are held ﬁxed). None of the models can reverse the letters in a word.\\nIn the one-shot setting, performance is signiﬁcantly weaker (dropping by half or more), and in the zero-shot setting the\\nmodel can rarely perform any of the tasks (Table 3.10). This suggests that the model really does appear to learn these\\ntasks at test time, as the model cannot perform them zero-shot and their artiﬁcial nature makes them unlikely to appear\\nin the pre-training data (although we cannot conﬁrm this with certainty).\\nWe can further quantify performance by plotting “in-context learning curves”, which show task performance as a\\nfunction of the number of in-context examples. We show in-context learning curves for the Symbol Insertion task\\nin Figure 1.2. We can see that larger models are able to make increasingly effective use of in-context information,\\nincluding both task examples and natural language task descriptions.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"Scramble tasks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a42645c-79c5-46db-8722-0203a21dbabd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are an AI assistant who is a good & polite helper who will answer user questions from the given context only and if you won't find the answer in the \\ncontext then you will politely deny and will ask another question from user. Also, make sure you save the tokens as much as possible.\\n\\nquestion: {question}\\ncontext: {context}\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_prompt = '''You are an AI assistant who is a good & polite helper who will answer user questions from the given context only and if you won't find the answer in the \n",
    "context then you will politely deny and will ask another question from user. Also, make sure you save the tokens as much as possible.\n",
    "\n",
    "question: {question}\n",
    "context: {context}\n",
    "'''\n",
    "rag_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8c5054e-f4cd-4d6c-90d8-13d84379bf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_prompt_template = ChatPromptTemplate.from_template(rag_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5fa49228-a64e-45bb-a3bd-acf6b4969748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "  return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "setup_retrieval = RunnableParallel({'context' : retriever, 'question' : RunnablePassthrough()})\n",
    "\n",
    "qa_rag_chain = setup_retrieval.assign(answer = (RunnablePassthrough.assign(context=lambda x: format_docs(x['context']))\n",
    "                                                |rag_prompt_template\n",
    "                                                | gemini\n",
    "                                                | StrOutputParser()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d126c69-eced-4a66-8145-5c20e58de01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please tye your query here. If you want to exit the chat then please type 'Exit' or 'Quit':\n",
      " Decoders\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context': [Document(id='da231534-bb1e-45eb-9255-c82e3daeccf4', metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'source': 'RAG for Knowledge-Intensive NLP Tasks.pdf', 'file_path': './PDF\\\\RAG for Knowledge-Intensive NLP Tasks.pdf', 'total_pages': 19, 'format': 'PDF 1.5', 'title': 'RAG for Knowledge-Intensive NLP Tasks', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'trapped': '', 'modDate': 'D:20210413004838Z', 'creationDate': 'D:20210413004838Z', 'page': 5}, page_content='|Sun|. BART|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16|Col17|Col18|Col19|Col20|Col21|Col22|Col23|Col24|Col25|\\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\\n|`Also`<br>|`Also`<br>|` Rises\"` <br>|` Rises\"` <br>|` Rises\"` <br>|` Rises\"` <br>|` Rises\"` <br>|` Rises\"` <br>|` Rises\"` <br>|` Rises\"` <br>|` Rises\"` <br>|` Rises\"` <br>|` Rises\"` <br>|` Rises\"` <br>|` Rises\"` <br>|` Rises\"` <br>|` Rises\"` <br>|` Rises\"` <br>|` Rises\"` <br>|` Rises\"` <br>|` Rises\"` <br>|` Rises\"` <br>|` Rises\"` <br>|` Rises\"` <br>|` Rises\"` <br>|\\n|`Also`<br>|`Also`<br>|T will complete the partial decoding <br>|T will complete the partial decoding <br>|T will complete the partial decoding <br>|T will complete the partial decoding <br>|T will complete the partial decoding <br>|T will complete the partial decoding <br>|T will complete the partial decoding <br>|T will complete the partial decoding <br>|T will complete the partial decoding <br>|` \"The`<br>|` \"The`<br>|`  Sun`<br>|`   Also R`<br>|`   Also R`<br>|`    ises\"`<br>|`     is`<br>|`      a`<br>|`       novel`<br>|`        by`<br>|`         this`<br>|`          au`<br>|`          thor o`<br>|`           f \"A`<br>|'), Document(id='54c484a3-f245-4f30-abf1-c7e0a4a59be4', metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-24T00:04:08+00:00', 'source': 'Language Models are Few-Shot Learners.pdf', 'file_path': './PDF\\\\Language Models are Few-Shot Learners.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': 'Language Models are Few-Shot Learners', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-07-24T00:04:08+00:00', 'trapped': '', 'modDate': 'D:20200724000408Z', 'creationDate': 'D:20200724000408Z', 'page': 62}, page_content='2.00\\n0.55 3.15\\n4.00 12.1 19.6 73.0 99.6\\n2.00\\n4.10 3.50\\n4.50 8.90 11.9 55.5 100.0\\n2D-\\nacc\\nn/a\\n50\\n1.25\\n1.25 1.25\\n1.25 1.60 7.60 12.6 58.0\\n1.15\\n0.95 1.45\\n1.95 3.85 11.5 44.6 86.4\\n1.15\\n1.45 2.25\\n2.70 7.35 13.6 52.4 98.9\\n3D+\\nacc\\nn/a\\n50\\n0.10\\n0.10 0.05\\n0.10 0.10 0.25 1.40 34.2\\n0.15\\n0.00 0.10\\n0.30 0.45 0.95 15.4 65.5\\n0.15\\n0.45 0.30\\n0.55 0.75 0.90 8.40 80.4\\n3D-\\nacc\\nn/a\\n50\\n0.05\\n0.05 0.05\\n0.05 0.05 0.45 1.35 48.3\\n0.05\\n0.15 0.25\\n0.30 0.55 1.60 6.15 78.7\\n0.05\\n0.10 0.15\\n0.35 0.65 1.05 9.20 94.2\\n4D+\\nacc\\nn/a\\n50\\n0.05\\n0.05 0.00\\n0.00 0.05 0.05 0.15 4.00\\n0.00\\n0.00 0.10\\n0.00 0.00 0.10 0.80 14.0\\n0.00\\n0.05 0.05\\n0.00 0.15 0.15 0.40 25.5\\n4D-\\nacc\\nn/a\\n50\\n0.00\\n0.00 0.00\\n0.00 0.00 0.00 0.10 7.50\\n0.00\\n0.00 0.00\\n0.00 0.05 0.00 0.50 14.0\\n0.00\\n0.05 0.00\\n0.00 0.10 0.05 0.40 26.8\\n5D+\\nacc\\nn/a\\n50\\n0.00\\n0.00 0.00\\n0.00 0.00 0.00 0.00 0.65\\n0.00\\n0.00 0.00\\n0.00 0.00 0.00 0.05 3.45\\n0.00\\n0.00 0.00\\n0.00 0.00 0.00 0.05 9.30\\n5D-\\nacc\\nn/a\\n50\\n0.00\\n0.00 0.00\\n0.00 0.00 0.00 0.00 0.80\\n0.00\\n0.00 0.00\\n0.00 0.00 0.00 0.05 3.75\\n0.00\\n0.00 0.00\\n0.00 0.00 0.00 0.00 9.90\\n2Dx\\nacc\\nn/a\\n50\\n2.20\\n2.25 2.65\\n2.10 2.55 5.80 6.15 19.8\\n1.35\\n2.35 3.35\\n2.35 4.75 9.15 11.0 27.4\\n1.35\\n2.90 2.70\\n2.85 4.25 6.10 7.05 29.2\\n1DC\\nacc\\nn/a\\n50\\n1.25\\n2.95 2.75\\n0.05 0.30 2.35 0.75 9.75\\n1.90\\n2.80 2.85\\n3.65 6.45 9.15 8.20 14.3\\n1.70\\n2.15 3.90\\n5.75 6.20 7.60 9.95 21.3\\nCycled Letters\\nacc\\nn/a\\n100\\n0.62\\n0.71 2.85\\n0.00 0.63 1.35 2.58 3.66\\n1.67\\n4.36 5.68\\n6.46 6.25 9.41 15.1 21.7\\n4.63\\n9.27 10.7\\n14.5 16.7 21.9 27.7 37.9\\nAnagrams 1\\nacc\\nn/a\\n100\\n0.10\\n0.14 0.40'), Document(id='49114c6b-b34a-426f-8497-af678d117e07', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': 'Attention Is All You Need.pdf', 'file_path': './PDF\\\\Attention Is All You Need.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': 'Attention Is All You Need', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 12}, page_content='|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16|Col17|Col18|Col19|Col20|Col21|Col22|Col23|Col24|Col25|Col26|Col27|Col28|Col29|Col30|Col31|Col32|Col33|Col34|Col35|Col36|Col37|Col38|Col39|Col40|\\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\\n|||||||||||||||||||||||||||||||||||||||||\\n|||||||||||||||||||||||||||||||||||||||||\\n|||||||||||||||||ts||||||||||||||||||||||||\\n|||||||||||||||n|n|en||||||||||on||||||||||||||\\n||||||||||||ty|||ca|ca|m||d|||||g|g||ati|||s||t||>|||||||\\n|||||||t|t||||ori|||ri|ri|ern|e|se|||e|9|in|in||str||ng|es|e|ul||S|d>|d>|d>|d>|d>|d>|\\n||||||is|iri|iri|at|at||aj|||me|me|ov|av|as|ew|ws|nc|00|ak|ak|e|gi||ti|oc|or|ffic||EO|pa|pa|pa|pa|pa|pa|\\n|It|It|is|is|in|th|sp|sp|th|th|a|m|of|of|A|A|g|h|p|n|la|si|2|m|m|th|re|or|vo|pr|m|di|.|<|<|<|<|<|<|<|\\n|||||||||||||||||||||||||||||||||||||||||\\n|It|It|is|is|in|is|rit|rit|at|at|a|ty|of|of|n|n|ts|e|d|w|s|e|9|g|g|e|n|or|g|ss|re|ult|.|>|>|>|>|>|>|>|\\n||||||th|pi|pi|th|th||ori|||ica|ica|en|av|se|ne|aw|inc|00|kin|kin|th|tio||tin|e|o|ic||OS|ad|ad|ad|ad|ad|ad|\\n|||||||s|s||||aj|||er|er|m|h|as||l|s|2|a|a||tra||vo|roc|m|iff||E|<p|<p|<p|<p|<p|<p|\\n||||||||||||m|||m|m|rn||p|||||m|m||gis|||p||d||<|||||||\\n|||||||||||||||A|A|ve||||||||||re||||||||||||||\\n|||||||||||||||||go||||||||||||||||||||||||\\n|||||||||||||||||||||||||||||||||||||||||\\n|||||||||||||||||||||||||||||||||||||||||'), Document(id='28937342-8d9e-4cb6-830b-9563aec43198', metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-07-24T00:04:08+00:00', 'source': 'Language Models are Few-Shot Learners.pdf', 'file_path': './PDF\\\\Language Models are Few-Shot Learners.pdf', 'total_pages': 75, 'format': 'PDF 1.5', 'title': 'Language Models are Few-Shot Learners', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2020-07-24T00:04:08+00:00', 'trapped': '', 'modDate': 'D:20200724000408Z', 'creationDate': 'D:20200724000408Z', 'page': 62}, page_content='DROP\\nf1\\ndev\\n89.1\\n20\\n9.40\\n13.6 14.4\\n16.4 19.7 17.0 24.0 23.6\\n11.7\\n18.1 20.9\\n23.0 26.4 27.3 29.2 34.3\\n12.9\\n18.7 24.0\\n25.6 29.7 29.7 32.3 36.5\\nBoolQ\\nacc\\ndev\\n91.0\\n32\\n49.7\\n60.3 58.9\\n62.4 67.1 65.4 66.2 60.5\\n52.6\\n61.7 60.4\\n63.7 68.4 68.7 69.0 76.7\\n43.1\\n60.6 62.0\\n64.1 70.3 70.0 70.2 77.5\\n76.4\\nCB\\nacc\\ndev\\n96.9\\n32\\n0.00\\n32.1 8.93\\n19.6 19.6 28.6 19.6 46.4\\n55.4\\n53.6 53.6\\n48.2 57.1 33.9 55.4 64.3\\n42.9\\n58.9 53.6\\n69.6 67.9 60.7 66.1 82.1\\n75.6\\nCB\\nf1\\ndev\\n93.9\\n32\\n0.00\\n29.3 11.4\\n17.4 22.4 25.1 20.3 42.8\\n60.1\\n39.8 45.6\\n37.5 45.7 28.5 44.6 52.5\\n26.1\\n40.4 32.6\\n48.3 45.7 44.6 46.0 57.2\\n52.0\\nCopa\\nacc\\ndev\\n94.8\\n32\\n66.0\\n68.0 73.0\\n77.0 76.0 80.0 84.0 91.0\\n62.0\\n64.0 66.0\\n74.0 76.0 82.0 86.0 87.0\\n67.0\\n64.0 72.0\\n77.0 83.0 83.0 86.0 92.0\\n92.0\\nRTE\\nacc\\ndev\\n92.5\\n32\\n47.7\\n49.8 48.4\\n56.0 46.6 55.2 62.8 63.5\\n53.1\\n47.3 49.5\\n49.5 54.9 54.9 56.3 70.4\\n52.3\\n48.4 46.9\\n50.9 56.3 49.5 60.6 72.9\\n69.0\\nWiC\\nacc\\ndev\\n76.1\\n32\\n0.00\\n0.00 0.00\\n0.00 0.00 0.00 0.00 0.00\\n50.0\\n50.3 50.3\\n49.2 49.4 50.3 50.0 48.6\\n49.8\\n55.0 53.0\\n53.0 51.6 53.1 51.1 55.3\\n49.4\\nWSC\\nacc\\ndev\\n93.8\\n32\\n59.6\\n56.7 65.4\\n61.5 66.3 60.6 64.4 65.4\\n58.7\\n58.7 60.6\\n62.5 66.3 60.6 66.3 69.2\\n58.7\\n60.6 54.8\\n49.0 62.5 67.3 75.0 75.0\\n80.1\\nMultiRC\\nacc\\ndev\\n62.3\\n32\\n4.72\\n9.65 12.3\\n13.6 14.3 18.4 24.2 27.6\\n4.72\\n9.65 12.3\\n13.6 14.3 18.4 24.2 27.6\\n6.09\\n11.8 16.8\\n20.8 24.7 23.8 25.0 32.5\\n30.5\\nMultiRC\\nf1a\\ndev\\n88.2\\n32\\n57.0\\n59.7 60.4\\n59.9 60.0 64.5 71.4 72.9\\n57.0\\n59.7 60.4\\n59.9 60.0 64.5 71.4 72.9\\n45.0\\n55.9 64.2\\n65.4 69.5 66.4 69.3 74.8\\n75.4\\nReCoRD\\nacc\\ndev\\n92.5\\n32\\n70.8\\n78.5 82.1')], 'question': 'Decoders', 'answer': 'I apologize, but the provided context does not contain information defining \"Decoders\" or elaborating on what they are. It only mentions \"T will complete the partial decoding\" in relation to some text.\\n\\nWould you like to ask another question based on the provided context?'}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please tye your query here. If you want to exit the chat then please type 'Exit' or 'Quit':\n",
      " exit\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    usrQuery = input(\"Please tye your query here. If you want to exit the chat then please type 'Exit' or 'Quit':\\n\")\n",
    "    if usrQuery.lower() in ['exit', 'quit']:\n",
    "        break\n",
    "    else:\n",
    "        responce = qa_rag_chain.invoke(usrQuery)\n",
    "        print(responce)\n",
    "        #display(Markdown(responce))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "270ea34c-bbc8-41cc-bf2c-15d989814a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = qa_rag_chain.invoke('Transformers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3a2ada3d-f1d9-45e4-bedf-0af44d2875fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Attention Is All You Need.pdf'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['answer']\n",
    "list(set(context.metadata['source'] for context in response['context']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAI_venv",
   "language": "python",
   "name": "genai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
