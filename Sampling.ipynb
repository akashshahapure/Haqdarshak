{
 "cells": [
  {
   "cell_type": "raw",
   "id": "38c594c5-671e-4538-a532-5605034f3401",
   "metadata": {},
   "source": [
    "remove repeat mobile numbers\n",
    "remove hd mobile = citi mobile \n",
    "20% sampling including all HDs and their respective schemes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f16a999b-2a8d-41ec-9738-958986443fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import xlsxwriter, openpyxl\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1e0ef3-9d19-4926-899d-2d5f29a5e6a2",
   "metadata": {},
   "source": [
    "# Data Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef709ae3-a0b0-48af-838e-46df61dfd9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csvORexcel(file_Name):\n",
    "    global path\n",
    "    path = \"C:\\\\Python\\\\read\\\\\"+file_Name\n",
    "    try:\n",
    "        if file_Name.split('.')[-1].startswith('c'):\n",
    "            df = pd.read_csv(path)\n",
    "            return df\n",
    "        elif file_Name.split('.')[-1].startswith('x'):\n",
    "            df = pd.read_excel(path)\n",
    "            return df\n",
    "    except FileNotFoundError:\n",
    "        print(\"The file name {0} has not found\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23753d83-1f6f-42f6-9970-fa660f19b3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = {'AP':'Andhra Pradesh',\n",
    "'AR':'Arunachal Pradesh',\n",
    "'AS':'Assam',\n",
    "'BR':'Bihar',\n",
    "'BH':'Bihar',\n",
    "'CT':'Chhattisgarh',\n",
    "'CG':'Chhattisgarh',\n",
    "'DL':'Delhi', \n",
    "'GA':'Goa',\n",
    "'GJ':'Gujarat',\n",
    "'HR':'Haryana',\n",
    "'HP':'Himachal Pradesh',\n",
    "'JH':'Jharkhand',\n",
    "'KA':'Karnataka',\n",
    "'KL':'Kerala',\n",
    "'MP':'Madhya Pradesh',\n",
    "'MH':'Maharashtra',\n",
    "'MN':'Manipur',\n",
    "'ML':'Meghalaya',\n",
    "'MZ':'Mizoram',\n",
    "'NL':'Nagaland',\n",
    "'OR':'Odisha',\n",
    "'PB':'Punjab',\n",
    "'RJ':'Rajasthan',\n",
    "'SK':'Sikkim',\n",
    "'TN':'Tamil Nadu',\n",
    "'TG':'Telangana',\n",
    "'TR':'Tripura',\n",
    "'UP':'Uttar Pradesh',\n",
    "'UT':'Uttarakhand',\n",
    "'WB':'West Bengal'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edd39ea-47df-4fef-8891-05892fc93dd5",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "860de2cc-6906-4f55-8085-bbac724aebea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cleaner(data0):\n",
    "    # Remove last row.\n",
    "    #data0.drop(index = data0[data0.Createdon.isna()].index, inplace=True)\n",
    "    \n",
    "    # Replace null values\n",
    "    if 'Scheme/Doc GUID' in data0.columns.to_list():\n",
    "        data0['Scheme/Doc GUID'].fillna('a', inplace=True)\n",
    "    else:\n",
    "        try:\n",
    "            data0['Scheme Guid'].fillna('a', inplace=True)\n",
    "        except Exception:\n",
    "            pass\n",
    "    data0['Citizen Name'].fillna('a', inplace=True)\n",
    "    data0['HD Name'].fillna('blank', inplace=True)\n",
    "    data0['Citizen Mobile'].fillna(0, inplace=True)\n",
    "    \n",
    "    # Changing status values and keeping only \"Open/Submit/BR\"\n",
    "    #try:\n",
    "    #    data0['Status'] = data0['Status'].apply(lambda x: 'Open' if x == 'Data complete' else 'Submitted' if (x=='Docket submitted' or x=='Document ready') else \"Benefit Received\" if x=='Scheme/Document received' else x)\n",
    "    #except KeyError:\n",
    "    #    pass\n",
    "        \n",
    "    # Changing Case Organization values from state initials to full state name.\n",
    "    #try:\n",
    "    #    data0['Case Organization'] = data0['Case Organization'].apply(lambda x: states[x[:2]])\n",
    "    #except KeyError:\n",
    "    #    pass\n",
    "    \n",
    "    # Renaming column \"Case Organiisation\" & \"Case District\" to \"State\" & \"Disctrict\"\n",
    "    #try:\n",
    "    #    data0.rename(columns={\"Case Organization\":\"State\",\"Case District\":\"District\"}, inplace=True)\n",
    "    #except KeyError:\n",
    "    #    pass\n",
    "    \n",
    "    # Convert Mobile column from float to string for concatenation.\n",
    "    data0['Citizen Mobile'] = data0['Citizen Mobile'].apply(lambda x: str(x).strip())\n",
    "    #data0['HD Mobile'] = data0['HD Mobile'].apply(lambda x: str(x).strip())\n",
    "    #data0['Mobile'] = data0['Mobile'].astype('int64')\n",
    "    data0['Citizen Mobile'] = data0['Citizen Mobile'].astype('str')\n",
    "    #data0['HD Mobile'] = data0['HD Mobile'].astype('str')\n",
    "    \n",
    "    # Changing Scheme column name to 'Scheme Name'.\n",
    "    if 'Scheme Name' not in data0.columns.to_list():\n",
    "        for col in data0.columns.to_list():\n",
    "            if 'e/' in col:\n",
    "                data0.rename(columns={col:'Scheme Name'},inplace=True)\n",
    "                break\n",
    "                \n",
    "    # Checking duplicate records based on citizrn mobile number.\n",
    "    duplicates = data0[data0.duplicated(['Citizen Mobile'], keep=False)] # Keeping duplicate records\n",
    "    data0 = data0.drop(index = data0[data0.duplicated(['Citizen Mobile'], keep='last')].index) # Removing duplicate records.\n",
    "    \n",
    "    # Adding column to check if HD mobile = Citizen mobile.\n",
    "    #data0['mob_similarity'] = [True if i==j else False for i,j in zip(data0['Citizen Mobile'], data0['HD Mobile'])]\n",
    "    #mob_sim = data0[data0['mob_similarity'] == True]\n",
    "    #data0 = data0[data0['mob_similarity'] == False]\n",
    "        \n",
    "    # Change gender from initial letter to full form.\n",
    "    #try:\n",
    "    #    data0['Gender'] = data0['Gender'].apply(lambda x: 'Male' if x=='M' else 'Female' if x=='F' else 'Other' if x=='O' else x)\n",
    "    #except KeyError:\n",
    "    #    continue\n",
    "    \n",
    "    # Convert \"Createdon\", \"Docket Submitted Date\", \"Benefit received Date\" column data type to Datetime format\n",
    "    #dt_col = ['Createdon', 'Docket Submitted Date', 'Benefit received Date', 'DOB']\n",
    "    \n",
    "    #for col in dt_col:\n",
    "    #    try:\n",
    "    #        data0[col] = pd.to_datetime(data0[col], format='mixed', errors='ignore')\n",
    "    #    except KeyError:\n",
    "    #        continue\n",
    "    \n",
    "    # Deleting records with status \"Case Aborted\" and \"Application rejected\"\n",
    "    #rejectedDF = data0[(data0.Status == 'Case Aborted') | (data0.Status == 'Application rejected')] # Storing prev step deleted data\n",
    "    #data0 = data0[(data0['Status'] != 'Case Aborted') & (data0['Status'] != 'Application rejected')]\n",
    "    \n",
    "    # Fill missing HD IDs with 'Not Mapped'.\n",
    "    #data0[['HD ID', 'HD Name']] = data0[['HD ID', 'HD Name']].fillna('Not Mapped')\n",
    "    \n",
    "    # Removing DFL cases data.\n",
    "    #data0[\"Scheme Category\"] = data0['Scheme/Doc GUID'].apply(lambda x: \"DFL\" if (x==\"SH0009SW\" or x==\"SH000AG6\" or x==\"SH000A32\" or x==\"SH0009SW\" or x==\"SH000AG6\" or x==\"SH000BM6\") else \"E-Gov\")\n",
    "    #dfl = data0[data0[\"Scheme Category\"] == 'DFL']\n",
    "    #data0 = data0[data0[\"Scheme Category\"] == 'E-Gov']\n",
    "    \n",
    "    data0.reset_index(inplace=True, drop=True)\n",
    "    duplicates.reset_index(inplace=True, drop=True)\n",
    "    return(data0, duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d82540-11a2-454f-b45d-fefde7186c63",
   "metadata": {},
   "source": [
    "# Sampling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08558ec6-c32b-416d-bf7f-f1520f552dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(data0):\n",
    "    fr = round(int(input('Please provide sampling percentage'))/100,2)\n",
    "    samp = pd.DataFrame()\n",
    "    try:\n",
    "        for hd in data0['HD ID'].value_counts().index:\n",
    "            for sid in data0[data0['HD ID'] == hd]['Scheme/Doc GUID'].value_counts().index:\n",
    "                conditional_data = data0[(data0['HD ID'] == hd) & (data0['Scheme/Doc GUID'] == sid)]\n",
    "                if len(conditional_data) == 1:\n",
    "                    if len(samp) == 0:\n",
    "                        samp = conditional_data\n",
    "                    else:\n",
    "                        samp = pd.concat([samp, conditional_data])\n",
    "                elif len(conditional_data) == 2:\n",
    "                    if len(samp) == 0:\n",
    "                        samp = conditional_data.sample(n=1, random_state=1, replace=False)\n",
    "                    else:\n",
    "                        samp = pd.concat([samp,conditional_data.sample(n=1, random_state=1, replace=False)], ignore_index=False)\n",
    "                elif len(conditional_data) <= 4:\n",
    "                    if len(samp) == 0:\n",
    "                        samp = conditional_data.sample(frac=fr, random_state=1, replace=False)\n",
    "                    else:\n",
    "                        samp = pd.concat([samp,conditional_data.sample(frac=fr, random_state=1, replace=False)], ignore_index=False)\n",
    "                else:\n",
    "                    if len(samp) == 0:\n",
    "                        samp = conditional_data.sample(frac=fr, random_state=1, replace=False)\n",
    "                    else:\n",
    "                        samp = pd.concat([samp,conditional_data.sample(frac=fr, random_state=1, replace=False)], ignore_index=False)    \n",
    "    except Exception:\n",
    "        try:\n",
    "            for hd in data0['HD Name'].value_counts().index:\n",
    "                for sid in data0[data0['HD Name'] == hd]['Scheme Guid'].value_counts().index:\n",
    "                    conditional_data = data0[(data0['HD Name'] == hd) & (data0['Scheme Guid'] == sid)]\n",
    "                    if len(conditional_data) == 1:\n",
    "                        if len(samp) == 0:\n",
    "                            samp = conditional_data\n",
    "                        else:\n",
    "                            samp = pd.concat([samp, conditional_data])\n",
    "                    elif len(conditional_data) == 2:\n",
    "                        if len(samp) == 0:\n",
    "                            samp = conditional_data.sample(n=1, random_state=1, replace=False)\n",
    "                        else:\n",
    "                            samp = pd.concat([samp,conditional_data.sample(n=1, random_state=1, replace=False)], ignore_index=False)\n",
    "                    elif len(conditional_data) <= 4:\n",
    "                        if len(samp) == 0:\n",
    "                            samp = conditional_data.sample(frac=fr, random_state=1, replace=False)\n",
    "                        else:\n",
    "                            samp = pd.concat([samp,conditional_data.sample(frac=fr, random_state=1, replace=False)], ignore_index=False)\n",
    "                    else:\n",
    "                        if len(samp) == 0:\n",
    "                            samp = conditional_data.sample(frac=fr, random_state=1, replace=False)\n",
    "                        else:\n",
    "                            samp = pd.concat([samp,conditional_data.sample(frac=fr, random_state=1, replace=False)], ignore_index=False)\n",
    "        except Exception:\n",
    "            for hd in data0['HD Name'].value_counts().index:\n",
    "                for sid in data0[data0['HD Name'] == hd]['Scheme Name'].value_counts().index:\n",
    "                    conditional_data = data0[(data0['HD Name'] == hd) & (data0['Scheme Name'] == sid)]\n",
    "                    if len(conditional_data) == 1:\n",
    "                        if len(samp) == 0:\n",
    "                            samp = conditional_data\n",
    "                        else:\n",
    "                            samp = pd.concat([samp, conditional_data])\n",
    "                    elif len(conditional_data) == 2:\n",
    "                        if len(samp) == 0:\n",
    "                            samp = conditional_data.sample(n=1, random_state=1, replace=False)\n",
    "                        else:\n",
    "                            samp = pd.concat([samp,conditional_data.sample(n=1, random_state=1, replace=False)], ignore_index=False)\n",
    "                    elif len(conditional_data) <= 4:\n",
    "                        if len(samp) == 0:\n",
    "                            samp = conditional_data.sample(frac=fr, random_state=1, replace=True)\n",
    "                        else:\n",
    "                            samp = pd.concat([samp,conditional_data.sample(frac=fr, random_state=1, replace=False)], ignore_index=False)\n",
    "                    else:\n",
    "                        if len(samp) == 0:\n",
    "                            samp = conditional_data.sample(frac=fr, random_state=1, replace=False)\n",
    "                        else:\n",
    "                            samp = pd.concat([samp,conditional_data.sample(frac=fr, random_state=1, replace=False)], ignore_index=False)\n",
    "    \n",
    "    samp.reset_index(inplace=True, drop=True)\n",
    "    samp['sampling'] = str(fr*100)+'%' # Adding a column to identify sampled records\n",
    "    data0 = data0.merge(samp[['Case ID','sampling']], how='left', on='Case ID') # Merging sampled identified column with unique data\n",
    "    data0.sampling.fillna(value = str(100.0-(fr*100))+'%', inplace=True) # Filling missing values which have not identified\n",
    "    remain=data0[data0.sampling == str(100.0-(fr*100))+'%'] # Filtering remaining data and storing with new variable.\n",
    "    \n",
    "    # Removing sampling column\n",
    "    data0 = data0.drop(columns = 'sampling')\n",
    "    samp = samp.drop(columns = 'sampling')\n",
    "    remain = remain.drop(columns = 'sampling')\n",
    "    \n",
    "    return(samp, data0, remain, fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095eca05-9f1b-4f1b-b538-fbffa238dab1",
   "metadata": {},
   "source": [
    "# Data Exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fddb8fed-925b-48a3-9476-02028f763f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_excel(samp, data0, remain, duplicates, fn, fr):\n",
    "    # Exporting data of unique records to Excel file.\n",
    "    with pd.ExcelWriter('C:\\\\Python\\\\export\\\\sample_data_'+fn.split('.')[0]+'.xlsx') as writer:\n",
    "        samp.to_excel(writer, sheet_name=str(fr*100)+'% sampling_1', index=False)\n",
    "        remain.to_excel(writer, sheet_name=str(100.0-(fr*100))+'% sampling_2', index=False)\n",
    "        data0.to_excel(writer, sheet_name='unique data', index=False)\n",
    "        if duplicates.shape[0]>0:\n",
    "            duplicates.to_excel(writer, sheet_name='duplicates', index=False)\n",
    "        #mob_sim.to_excel(writer, sheet_name='mobile similarity', index=False)\n",
    "        #dfl.to_excel(writer, sheet_name='DFL data', index=False)\n",
    "        #rejectedDF.to_excel(writer, sheet_name='rejected cases', index=False)\n",
    "    print('Data of project {0} exported to excel.'.format(fn.split('.')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16dd39e7-a88d-4bef-9060-4f35f07a1eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please provide file name =  Valid Proof_AP - Colgate Palmolive India Limited (Phase 2)_703.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting sample data of  Valid Proof_AP - Colgate Palmolive India Limited (Phase 2)_703.xlsx\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please provide sampling percentage 80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data of project Valid Proof_AP - Colgate Palmolive India Limited (Phase 2)_703 exported to excel.\n"
     ]
    }
   ],
   "source": [
    "file_Name = input('Please provide file name = ')\n",
    "data0 = csvORexcel(file_Name)\n",
    "fn = file_Name\n",
    "print('Getting sample data of ',fn)\n",
    "data0, duplicates = cleaner(data0)\n",
    "samp, data0, remain, fr = sampling(data0)\n",
    "export_to_excel(samp, data0, remain, duplicates, fn, fr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
