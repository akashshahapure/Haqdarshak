{
 "cells": [
  {
   "cell_type": "raw",
   "id": "23974ffa-b6f4-4d01-95b4-928ed1d9a974",
   "metadata": {
    "id": "9b7be93c-4d33-4e20-83ea-ccfd2c943192"
   },
   "source": [
    "## ! python.exe -m pip install --upgrade pip\n",
    "! pip install pandas\n",
    "! pip install numpy\n",
    "! pip install openpyxl\n",
    "! pip install seaborn\n",
    "! pip install notify-py\n",
    "! pip install ai4bharat-transliteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "611ba109-33e4-4f44-bb67-b464c2bdd0ed",
   "metadata": {
    "collapsed": false,
    "id": "1c70450f",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "251e48fc-c722-494e-ff7b-43407df2f52a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*****Required libraries imported*****\n"
     ]
    }
   ],
   "source": [
    "#! pip install translate\n",
    "import pandas as pd, numpy as np, xlsxwriter, matplotlib.pyplot as plt, seaborn as sns, os, math, shutil\n",
    "from datetime import datetime as dt, timedelta\n",
    "from openpyxl import load_workbook\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from scipy.cluster.hierarchy import cut_tree\n",
    "from notifypy import Notify\n",
    "notification = Notify()\n",
    "from googletrans import Translator\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Importing Google sheet library and autorising to access google sheets.\n",
    "import gspread\n",
    "gc = gspread.oauth()\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(\"\\n*****Required libraries imported*****\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322a1231-0b81-46fe-a7b0-5d62282e72c2",
   "metadata": {
    "id": "322a1231-0b81-46fe-a7b0-5d62282e72c2"
   },
   "source": [
    "# Mention project raw data filename & PID below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31416ff4-4a2e-483c-801e-6e36298a96e0",
   "metadata": {
    "id": "df8b1bf2-6d57-4b67-8e2d-3dafaf322fa6",
    "outputId": "f406445e-af54-4c0b-f355-7c3064a62447"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter file name =  cases_report_Hyundai Foundation Phase2_(All States)_2025-12-29.csv\n",
      "Please provide PID of project =  PID/HYUND1/2025/DI/YK/0530\n"
     ]
    }
   ],
   "source": [
    "## Asking for project ID from the user\n",
    "file_Name = input('Please enter file name = ')\n",
    "\n",
    "# Asking for project ID from the user\n",
    "PID = input('Please provide PID of project = ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "715f9235-c2e3-4b0e-91a1-20c59ea00048",
   "metadata": {
    "id": "715f9235-c2e3-4b0e-91a1-20c59ea00048"
   },
   "outputs": [],
   "source": [
    "def csvORexcel():\n",
    "    global path\n",
    "    path = \"C:\\\\Python\\\\read\\\\\"+file_Name\n",
    "    try:\n",
    "        if file_Name.split('.')[-1].startswith('c'):\n",
    "            df = pd.read_csv(path)\n",
    "            return df\n",
    "        elif file_Name.split('.')[-1].startswith('x'):\n",
    "            df = pd.read_excel(path, engine=\"openpyxl\")\n",
    "            return df\n",
    "    except FileNotFoundError:\n",
    "        print(\"The file name {0} has not found\".format(path))\n",
    "\n",
    "fn = file_Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "583ee3b2-91bb-4983-95db-bab7b48e3976",
   "metadata": {
    "id": "583ee3b2-91bb-4983-95db-bab7b48e3976",
    "outputId": "e5745fd0-6ecc-4e49-981e-8389b2e97b4f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case Id</th>\n",
       "      <th>Case Created On</th>\n",
       "      <th>Scheme/Doc</th>\n",
       "      <th>Scheme/Doc GUID</th>\n",
       "      <th>Status</th>\n",
       "      <th>Docket Submitted Date</th>\n",
       "      <th>Benefit received Date</th>\n",
       "      <th>HD Suspected Cases</th>\n",
       "      <th>Case Organization</th>\n",
       "      <th>Case District</th>\n",
       "      <th>Citizen GUID</th>\n",
       "      <th>Citizen Name</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Mobile</th>\n",
       "      <th>Age</th>\n",
       "      <th>Citizen Block</th>\n",
       "      <th>Citizen Village</th>\n",
       "      <th>Family GUID</th>\n",
       "      <th>Family Name</th>\n",
       "      <th>HD/Agent ID</th>\n",
       "      <th>HD Display Name</th>\n",
       "      <th>HD Reporting to Opsco Name</th>\n",
       "      <th>HDs Associated AMS ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EP05ODZG</td>\n",
       "      <td>23-Dec-2025</td>\n",
       "      <td>Loyalty Yojana Card</td>\n",
       "      <td>SH0009RR</td>\n",
       "      <td>Scheme/Document received</td>\n",
       "      <td>23-Dec-2025</td>\n",
       "      <td>23-Dec-2025</td>\n",
       "      <td>Not Suspected</td>\n",
       "      <td>MH - Hyundai Motor India Foundation - Phase 2 ...</td>\n",
       "      <td>PUNE</td>\n",
       "      <td>PN05ABV6</td>\n",
       "      <td>Sanjay Narayan Kajale</td>\n",
       "      <td>M</td>\n",
       "      <td>7.218392e+09</td>\n",
       "      <td>41.0</td>\n",
       "      <td>MAVAL</td>\n",
       "      <td>Chikhalse</td>\n",
       "      <td>PG04Z1BT</td>\n",
       "      <td>Kajale</td>\n",
       "      <td>ashwini.modhave</td>\n",
       "      <td>Ashwini Rohidas Modhave</td>\n",
       "      <td>Ravindra Kathe</td>\n",
       "      <td>31254.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EP05OE0C</td>\n",
       "      <td>23-Dec-2025</td>\n",
       "      <td>Loyalty Yojana Card</td>\n",
       "      <td>SH0009RR</td>\n",
       "      <td>Scheme/Document received</td>\n",
       "      <td>23-Dec-2025</td>\n",
       "      <td>23-Dec-2025</td>\n",
       "      <td>Not Suspected</td>\n",
       "      <td>MH - Hyundai Motor India Foundation - Phase 2 ...</td>\n",
       "      <td>PUNE</td>\n",
       "      <td>PN05ABVQ</td>\n",
       "      <td>Jyoti Sanjay Kajale</td>\n",
       "      <td>F</td>\n",
       "      <td>9.823122e+09</td>\n",
       "      <td>34.0</td>\n",
       "      <td>MAVAL</td>\n",
       "      <td>Chikhalse</td>\n",
       "      <td>PG04Z1BT</td>\n",
       "      <td>Kajale</td>\n",
       "      <td>ashwini.modhave</td>\n",
       "      <td>Ashwini Rohidas Modhave</td>\n",
       "      <td>Ravindra Kathe</td>\n",
       "      <td>31254.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EP05OE2G</td>\n",
       "      <td>23-Dec-2025</td>\n",
       "      <td>Ekyc of Ration Card_MH</td>\n",
       "      <td>SH000DU6</td>\n",
       "      <td>Scheme/Document received</td>\n",
       "      <td>24-Dec-2025</td>\n",
       "      <td>24-Dec-2025</td>\n",
       "      <td>Not Suspected</td>\n",
       "      <td>MH - Hyundai Motor India Foundation - Phase 2 ...</td>\n",
       "      <td>PUNE</td>\n",
       "      <td>PN05ABVQ</td>\n",
       "      <td>Jyoti Sanjay Kajale</td>\n",
       "      <td>F</td>\n",
       "      <td>9.823122e+09</td>\n",
       "      <td>34.0</td>\n",
       "      <td>MAVAL</td>\n",
       "      <td>Chikhalse</td>\n",
       "      <td>PG04Z1BT</td>\n",
       "      <td>Kajale</td>\n",
       "      <td>ashwini.modhave</td>\n",
       "      <td>Ashwini Rohidas Modhave</td>\n",
       "      <td>Ravindra Kathe</td>\n",
       "      <td>31254.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EP05OFHB</td>\n",
       "      <td>23-Dec-2025</td>\n",
       "      <td>Ekyc of Ration Card_MH</td>\n",
       "      <td>SH000DU6</td>\n",
       "      <td>Scheme/Document received</td>\n",
       "      <td>24-Dec-2025</td>\n",
       "      <td>24-Dec-2025</td>\n",
       "      <td>Not Suspected</td>\n",
       "      <td>MH - Hyundai Motor India Foundation - Phase 2 ...</td>\n",
       "      <td>PUNE</td>\n",
       "      <td>PN05ABV6</td>\n",
       "      <td>Sanjay Narayan Kajale</td>\n",
       "      <td>M</td>\n",
       "      <td>7.218392e+09</td>\n",
       "      <td>41.0</td>\n",
       "      <td>MAVAL</td>\n",
       "      <td>Chikhalse</td>\n",
       "      <td>PG04Z1BT</td>\n",
       "      <td>Kajale</td>\n",
       "      <td>ashwini.modhave</td>\n",
       "      <td>Ashwini Rohidas Modhave</td>\n",
       "      <td>Ravindra Kathe</td>\n",
       "      <td>31254.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EP05OI9G</td>\n",
       "      <td>24-Dec-2025</td>\n",
       "      <td>Pradhan Mantri Jan Arogya Yojana (Ayushman Bha...</td>\n",
       "      <td>SH0005JK</td>\n",
       "      <td>Open</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not Suspected</td>\n",
       "      <td>MH - Hyundai Motor India Foundation - Phase 2 ...</td>\n",
       "      <td>PUNE</td>\n",
       "      <td>PN05AEPR</td>\n",
       "      <td>Ranjana Popat Bhalerao</td>\n",
       "      <td>F</td>\n",
       "      <td>7.218102e+09</td>\n",
       "      <td>59.0</td>\n",
       "      <td>MAVAL</td>\n",
       "      <td>Chikhalse</td>\n",
       "      <td>PG04Z3UV</td>\n",
       "      <td>Bhalerao</td>\n",
       "      <td>ashwini.modhave</td>\n",
       "      <td>Ashwini Rohidas Modhave</td>\n",
       "      <td>Ravindra Kathe</td>\n",
       "      <td>31254.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Case Id Case Created On  \\\n",
       "0  EP05ODZG     23-Dec-2025   \n",
       "1  EP05OE0C     23-Dec-2025   \n",
       "2  EP05OE2G     23-Dec-2025   \n",
       "3  EP05OFHB     23-Dec-2025   \n",
       "4  EP05OI9G     24-Dec-2025   \n",
       "\n",
       "                                          Scheme/Doc Scheme/Doc GUID  \\\n",
       "0                                Loyalty Yojana Card        SH0009RR   \n",
       "1                                Loyalty Yojana Card        SH0009RR   \n",
       "2                             Ekyc of Ration Card_MH        SH000DU6   \n",
       "3                             Ekyc of Ration Card_MH        SH000DU6   \n",
       "4  Pradhan Mantri Jan Arogya Yojana (Ayushman Bha...        SH0005JK   \n",
       "\n",
       "                     Status Docket Submitted Date Benefit received Date  \\\n",
       "0  Scheme/Document received           23-Dec-2025           23-Dec-2025   \n",
       "1  Scheme/Document received           23-Dec-2025           23-Dec-2025   \n",
       "2  Scheme/Document received           24-Dec-2025           24-Dec-2025   \n",
       "3  Scheme/Document received           24-Dec-2025           24-Dec-2025   \n",
       "4                      Open                   NaN                   NaN   \n",
       "\n",
       "  HD Suspected Cases                                  Case Organization  \\\n",
       "0      Not Suspected  MH - Hyundai Motor India Foundation - Phase 2 ...   \n",
       "1      Not Suspected  MH - Hyundai Motor India Foundation - Phase 2 ...   \n",
       "2      Not Suspected  MH - Hyundai Motor India Foundation - Phase 2 ...   \n",
       "3      Not Suspected  MH - Hyundai Motor India Foundation - Phase 2 ...   \n",
       "4      Not Suspected  MH - Hyundai Motor India Foundation - Phase 2 ...   \n",
       "\n",
       "  Case District Citizen GUID            Citizen Name Gender        Mobile  \\\n",
       "0          PUNE     PN05ABV6   Sanjay Narayan Kajale      M  7.218392e+09   \n",
       "1          PUNE     PN05ABVQ     Jyoti Sanjay Kajale      F  9.823122e+09   \n",
       "2          PUNE     PN05ABVQ     Jyoti Sanjay Kajale      F  9.823122e+09   \n",
       "3          PUNE     PN05ABV6   Sanjay Narayan Kajale      M  7.218392e+09   \n",
       "4          PUNE     PN05AEPR  Ranjana Popat Bhalerao      F  7.218102e+09   \n",
       "\n",
       "    Age Citizen Block Citizen Village Family GUID Family Name  \\\n",
       "0  41.0         MAVAL       Chikhalse    PG04Z1BT      Kajale   \n",
       "1  34.0         MAVAL       Chikhalse    PG04Z1BT      Kajale   \n",
       "2  34.0         MAVAL       Chikhalse    PG04Z1BT      Kajale   \n",
       "3  41.0         MAVAL       Chikhalse    PG04Z1BT      Kajale   \n",
       "4  59.0         MAVAL       Chikhalse    PG04Z3UV    Bhalerao   \n",
       "\n",
       "       HD/Agent ID          HD Display Name HD Reporting to Opsco Name  \\\n",
       "0  ashwini.modhave  Ashwini Rohidas Modhave             Ravindra Kathe   \n",
       "1  ashwini.modhave  Ashwini Rohidas Modhave             Ravindra Kathe   \n",
       "2  ashwini.modhave  Ashwini Rohidas Modhave             Ravindra Kathe   \n",
       "3  ashwini.modhave  Ashwini Rohidas Modhave             Ravindra Kathe   \n",
       "4  ashwini.modhave  Ashwini Rohidas Modhave             Ravindra Kathe   \n",
       "\n",
       "   HDs Associated AMS ID  \n",
       "0                31254.0  \n",
       "1                31254.0  \n",
       "2                31254.0  \n",
       "3                31254.0  \n",
       "4                31254.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src = r\"F:\\Haqdarshak Data\\Downloads\"\n",
    "dest = \"C:\\\\Python\\\\read\\\\\"\n",
    "if file_Name in os.listdir(src):\n",
    "    shutil.move(os.path.join(src,file_Name),os.path.join(dest,file_Name))\n",
    "exe_start = dt.now() # Recording execution start time89/*\n",
    "data0 = csvORexcel() # Reading exce/csv file\n",
    "init_file_size = round(os.path.getsize(path)/1000000,2) # Getting excel file size\n",
    "#data0 = pd.read_excel(r'C:\\Users\\akash\\Documents\\Haqdarshak\\Work\\Nassscom\\1.0\\cases_report__1_Nasscom1_uniques_till_27_Feb.xlsx', \"Unique DI Data\")\n",
    "data0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c99c4ce1-71b3-4d71-878a-a8c1f3d0c80a",
   "metadata": {
    "id": "c99c4ce1-71b3-4d71-878a-a8c1f3d0c80a",
    "outputId": "30e9397d-c7fc-4206-801a-ea260277c204"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 121303 entries, 0 to 121302\n",
      "Data columns (total 23 columns):\n",
      " #   Column                      Non-Null Count   Dtype  \n",
      "---  ------                      --------------   -----  \n",
      " 0   Case Id                     121302 non-null  object \n",
      " 1   Case Created On             121302 non-null  object \n",
      " 2   Scheme/Doc                  121302 non-null  object \n",
      " 3   Scheme/Doc GUID             121302 non-null  object \n",
      " 4   Status                      121302 non-null  object \n",
      " 5   Docket Submitted Date       104245 non-null  object \n",
      " 6   Benefit received Date       95247 non-null   object \n",
      " 7   HD Suspected Cases          121302 non-null  object \n",
      " 8   Case Organization           121302 non-null  object \n",
      " 9   Case District               121302 non-null  object \n",
      " 10  Citizen GUID                121302 non-null  object \n",
      " 11  Citizen Name                121302 non-null  object \n",
      " 12  Gender                      121302 non-null  object \n",
      " 13  Mobile                      121302 non-null  float64\n",
      " 14  Age                         121302 non-null  float64\n",
      " 15  Citizen Block               62056 non-null   object \n",
      " 16  Citizen Village             62084 non-null   object \n",
      " 17  Family GUID                 121302 non-null  object \n",
      " 18  Family Name                 121299 non-null  object \n",
      " 19  HD/Agent ID                 121302 non-null  object \n",
      " 20  HD Display Name             121302 non-null  object \n",
      " 21  HD Reporting to Opsco Name  121302 non-null  object \n",
      " 22  HDs Associated AMS ID       121302 non-null  float64\n",
      "dtypes: float64(3), object(20)\n",
      "memory usage: 21.3+ MB\n"
     ]
    }
   ],
   "source": [
    "data0.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f41ed24-4059-4add-8c74-d71f5ca0d896",
   "metadata": {
    "id": "6f41ed24-4059-4add-8c74-d71f5ca0d896"
   },
   "outputs": [],
   "source": [
    "states = {'AP':'Andhra Pradesh',\n",
    "'AR':'Arunachal Pradesh',\n",
    "'AS':'Assam',\n",
    "'BR':'Bihar',\n",
    "'BH':'Bihar',\n",
    "'CT':'Chhattisgarh',\n",
    "'CG':'Chhattisgarh',\n",
    "'DL':'Delhi',\n",
    "'GA':'Goa',\n",
    "'GJ':'Gujarat',\n",
    "'HR':'Haryana',\n",
    "'HP':'Himachal Pradesh',\n",
    "'JH':'Jharkhand',\n",
    "'KA':'Karnataka',\n",
    "'KL':'Kerala',\n",
    "'MP':'Madhya Pradesh',\n",
    "'MH':'Maharashtra',\n",
    "'MN':'Manipur',\n",
    "'ML':'Meghalaya',\n",
    "'MZ':'Mizoram',\n",
    "'NL':'Nagaland',\n",
    "'OR':'Odisha',\n",
    "'PB':'Punjab',\n",
    "'RJ':'Rajasthan',\n",
    "'SK':'Sikkim',\n",
    "'TN':'Tamil Nadu',\n",
    "'TG':'Telangana',\n",
    "'TR':'Tripura',\n",
    "'UP':'Uttar Pradesh',\n",
    "'UT':'Uttarakhand',\n",
    "'WB':'West Bengal'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede7e81e-d0d7-4904-855a-62b61818bd1c",
   "metadata": {
    "id": "ede7e81e-d0d7-4904-855a-62b61818bd1c"
   },
   "source": [
    "# Data Cleaning & Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "498cdfb4-67f7-4378-b6fe-6078ae563e2c",
   "metadata": {
    "id": "498cdfb4-67f7-4378-b6fe-6078ae563e2c"
   },
   "outputs": [],
   "source": [
    "# Defining a function to remove extra spaces between words.\n",
    "def rem_space(x):\n",
    "    try:\n",
    "        n = x.strip(' ').split(' ') # Remove extra spaces from begining and end then splitting the name.\n",
    "    except AttributeError:\n",
    "        n = str(x).strip(' ').split(' ') # Remove extra spaces from begining and end then splitting the name.\n",
    "\n",
    "    name = '' # Decalring a empty name\n",
    "\n",
    "    for w in n:\n",
    "        if w != '': # This will execute if non empty word found\n",
    "            if name == '': # This will run if \"name\" variable is empty\n",
    "                name = w # Storing name into \"name\" variable.\n",
    "            else: # This will run if \"name\" variable is not empty\n",
    "                name = name + ' ' + w # Joining the names with single spaces.\n",
    "\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e98da0ce-4f2b-4e37-af8c-247a701e09b7",
   "metadata": {
    "id": "e98da0ce-4f2b-4e37-af8c-247a701e09b7",
    "outputId": "d0aa3e2e-be89-4f8a-8d6a-edb5ed0781f5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 120332 entries, 0 to 120331\n",
      "Data columns (total 24 columns):\n",
      " #   Column                 Non-Null Count   Dtype         \n",
      "---  ------                 --------------   -----         \n",
      " 0   Case Id                120331 non-null  object        \n",
      " 1   Createdon              120332 non-null  datetime64[ns]\n",
      " 2   Scheme/Doc             120332 non-null  object        \n",
      " 3   Scheme/Doc GUID        120332 non-null  object        \n",
      " 4   Status                 120332 non-null  object        \n",
      " 5   Docket Submitted Date  104074 non-null  datetime64[ns]\n",
      " 6   Benefit received Date  95232 non-null   datetime64[ns]\n",
      " 7   HD Suspected Cases     120332 non-null  object        \n",
      " 8   State                  120332 non-null  object        \n",
      " 9   District               120332 non-null  object        \n",
      " 10  Citizen GUID           120332 non-null  object        \n",
      " 11  Citizen Name           120332 non-null  object        \n",
      " 12  Gender                 120332 non-null  object        \n",
      " 13  Mobile                 120332 non-null  object        \n",
      " 14  Age                    120332 non-null  float64       \n",
      " 15  Citizen Block          61709 non-null   object        \n",
      " 16  Citizen Village        61737 non-null   object        \n",
      " 17  Family GUID            120332 non-null  object        \n",
      " 18  Family Name            120329 non-null  object        \n",
      " 19  HD ID                  120332 non-null  object        \n",
      " 20  HD Name                120332 non-null  object        \n",
      " 21  Opsco name             120332 non-null  object        \n",
      " 22  AMS ID                 120332 non-null  float64       \n",
      " 23  isYC                   120332 non-null  object        \n",
      "dtypes: datetime64[ns](3), float64(2), object(19)\n",
      "memory usage: 22.0+ MB\n"
     ]
    }
   ],
   "source": [
    "data0.rename(columns={\"Case Created On\":\"Createdon\",\"HD/Agent ID\":\"HD ID\",\"HD Display Name\":\"HD Name\",\"HD Reporting to Opsco Name\":\"Opsco name\",\"HDs Associated AMS ID\":\"AMS ID\"}, inplace=True)\n",
    "\n",
    "# Remove last row.\n",
    "data0.drop(index = data0[data0[\"Createdon\"].isna()].index, inplace=True)\n",
    "\n",
    "# Replace null values\n",
    "data0['Scheme/Doc'].fillna('a', inplace=True)\n",
    "data0['Citizen Name'].fillna('a', inplace=True)\n",
    "data0['HD ID'].fillna('blank', inplace=True)\n",
    "data0.Mobile.fillna(0, inplace=True)\n",
    "\n",
    "# Changing status values and keeping only \"Open/Submit/BR\"\n",
    "data0['Status'] = data0['Status'].apply(lambda x: 'Open' if x == 'Data complete' else 'Submitted' if (x=='Docket submitted' or x=='Document ready') else \"Benefit Received\" if x=='Scheme/Document received' else x)\n",
    "\n",
    "# Changing Case Organization values from state initials to full state name.\n",
    "data0['Case Organization'] = data0['Case Organization'].apply(lambda x: states[x[:2]]+'-'+x.strip(' ')[-2:] if x.strip(' ')[-2:]=='LI' else states[x[:2]])\n",
    "\n",
    "# Renaming column \"Case Organiisation\" & \"Case District\" to \"State\" & \"Disctrict\"\n",
    "data0.rename(columns={\"Case Organization\":\"State\",\"Case District\":\"District\"}, inplace=True)\n",
    "\n",
    "# Filling missing values in \"Citizen District\" column with \"Not Captured\"\n",
    "if 'Citizen District' in data0.columns:\n",
    "    data0['Citizen District'].fillna('Not Captured', inplace=True)\n",
    "\n",
    "# Removing extra spaces in Opsco name\n",
    "data0['Opsco name'] = data0['Opsco name'].apply(lambda x: rem_space(x))\n",
    "\n",
    "# Convert Mobile column from float to string for concatenation.\n",
    "data0['Mobile'] = data0['Mobile'].apply(lambda x: str(x).strip())\n",
    "\n",
    "# Defining a function to remove row based on invalid mobile number.\n",
    "def ifNotNumber(x):\n",
    "    data0.drop(data0[data0.Mobile==x].index, inplace=True)\n",
    "    print(\"Row with mobile number {0} has removed!\".format(x))\n",
    "\n",
    "data0['Mobile'] = data0['Mobile'].apply(lambda x: ifNotNumber(x) if x.strip(\".\")[0].isnumeric() == False else x) # Remove rows with invalid mobile number\n",
    "\n",
    "# Change gender from initial letter to full form.\n",
    "data0['Gender'] = data0['Gender'].apply(lambda x: 'Male' if x=='M' else 'Female' if x=='F' else 'Other' if x=='O' else x)\n",
    "\n",
    "# Adding a boolean column to identify if citizen opted for YC & MSME YC\n",
    "YC_citizens = data0[(data0['Scheme/Doc GUID']=='SH0009RR') & (data0['Status']=='Benefit Received')]['Citizen GUID'].to_list()\n",
    "data0['isYC'] = data0['Citizen GUID'].apply(lambda x: 'Yes' if x in YC_citizens else 'No')\n",
    "if 'SH000DYR' in set(data0['Scheme/Doc GUID']):\n",
    "    MSMEYC_citizens = data0[(data0['Scheme/Doc GUID']=='SH000DYR') & (data0['Status']=='Benefit Received')]['Citizen GUID'].to_list()\n",
    "    data0['isMSMEYC'] = data0['Citizen GUID'].apply(lambda x: 'Yes' if x in MSMEYC_citizens else 'No')\n",
    "\n",
    "# Convert \"Createdon\", \"Docket Submitted Date\", \"Benefit received Date\" column data type to Datetime format\n",
    "dt_col = ['Createdon', 'Createdon.1', 'Docket Submitted Date', 'Benefit received Date', 'DOB']\n",
    "\n",
    "for col in dt_col:\n",
    "    try:\n",
    "        data0[col] = pd.to_datetime(data0[col], format='mixed', errors='ignore')\n",
    "        #data0[col] = data0[col].apply(lambda x: x.strftime('%d-%m-%Y') if type(x) != pd._libs.tslibs.nattype.NaTType else x)\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "# Deleting records with status \"Case Aborted\" and \"Application rejected\"\n",
    "rejectedDF = data0[(data0.Status == 'Case Aborted') | (data0.Status == 'Application rejected')] # Storing prev step deleted data\n",
    "data0 = data0[(data0['Status'] != 'Case Aborted') & (data0['Status'] != 'Application rejected')]\n",
    "\n",
    "data0.reset_index(inplace=True, drop=True)\n",
    "data0.info()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "327233bb-724b-467c-9443-359525a10c90",
   "metadata": {
    "id": "327233bb-724b-467c-9443-359525a10c90"
   },
   "source": [
    "case = 'EP023SJC'\n",
    "\n",
    "if len(data0[data0['Case Id']==case])==0:\n",
    "    print('rejected')\n",
    "    print(rejectedDF[rejectedDF['Case Id']==case])\n",
    "else:\n",
    "    print('data0')\n",
    "    print(data0[data0['Case Id']==case])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993cb01c-4c9c-472b-9928-dcda5eae1188",
   "metadata": {
    "id": "993cb01c-4c9c-472b-9928-dcda5eae1188"
   },
   "source": [
    "# Translate local language district name to English"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1f256f-c27c-404e-a68f-8cf599eec464",
   "metadata": {
    "id": "8d1f256f-c27c-404e-a68f-8cf599eec464"
   },
   "source": [
    "### Custom translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3b32844-0c05-4d9f-976f-e1fa9bc0eeaa",
   "metadata": {
    "id": "a3b32844-0c05-4d9f-976f-e1fa9bc0eeaa",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Trans(x):\n",
    "    t = Translator()\n",
    "    attempt = 0\n",
    "    max_attempts = 5\n",
    "    alph = [chr(i) for i in range(65,122)]\n",
    "\n",
    "    while attempt < max_attempts:\n",
    "        if x[0] not in alph:\n",
    "            try:\n",
    "                xlated = t.translate(x)\n",
    "                return xlated.text\n",
    "            except AttributeError as e:\n",
    "                if 'raise_Exception' in str(e):\n",
    "                    print(f\"Encountered rate limit error, attempt {attempt+1}/{max_attempts}. Retrying in 8 seconds...\")\n",
    "                    time.sleep(8)\n",
    "                    attempt += 1\n",
    "                else:\n",
    "                    raise\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                break\n",
    "            return \"Failed to translate after multiple attempts.\"\n",
    "        else:\n",
    "            return x.title()\n",
    "\n",
    "# Fnction to to create a dictionary for mapping correct district names.\n",
    "def distName(column):\n",
    "    dist = {}\n",
    "\n",
    "    for d in column.value_counts().index:\n",
    "        if d in [np.nan]:\n",
    "            dist[d] = d\n",
    "        elif d in dist.keys():\n",
    "            break\n",
    "        else:\n",
    "            dist[d] = Trans(d)\n",
    "\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddc174c5-7210-409e-a598-5c54d087d8c5",
   "metadata": {
    "id": "ddc174c5-7210-409e-a598-5c54d087d8c5",
    "outputId": "6fe68c64-69a5-4f6d-d177-6eaa3277fa5d",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "District\n",
       "Pune                 39787\n",
       "Gurugram             26849\n",
       "Nuh                  25123\n",
       "Faridabad            11744\n",
       "Nashik                4878\n",
       "South West            4203\n",
       "North                 2625\n",
       "South                 1990\n",
       "Haridwar              1946\n",
       "South East             284\n",
       "Pithoragarh            227\n",
       "Chhindwara             167\n",
       "Indore                 141\n",
       "Jhansi                 127\n",
       "Mirzapur                60\n",
       "West                    55\n",
       "Bhopal                  38\n",
       "Bahraich                31\n",
       "Lucknow                 18\n",
       "Udham Singh Nagar       14\n",
       "New Delhi               11\n",
       "Palwal                  10\n",
       "Varanasi                 1\n",
       "Jalgaon                  1\n",
       "Rewari                   1\n",
       "Dhanbad                  1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Renaming Case District name from local language to english\n",
    "\n",
    "dist = distName(data0['District'])\n",
    "data0['District'] = data0['District'].apply(lambda x: \"Dohad\" if x==\"દોહદ\" else \"Haridwar\" if x==\"हरिद्वार\" else \"UDHAM SINGH NAGAR\".title() if (x==\"उदम सिंह नगर\" or x==\"Udam Singh Nagar\" or x==\"UDAM SINGH NAGAR\")\n",
    "                             else \"Sitamarhi\" if x==\"सीतामढ़ी\" else \"Mahasamund\" if (x==\"महासमुंद\" or x==\"Mahasamand\")\n",
    "                             else \"Rajgarh\" if x==\"राजगढ़\" else \"Muzaffarpur\" if x==\"मुजफ्फरपुर\" else \"Nawada\" if x==\"नवादा\" else \"Balrampur\" if x==\"बलरामपुर\"\n",
    "                             else \"DAMOH\".title() if x==\"दमोह\" else \"Shravasti\" if x==\"श्रावस्ती\" else \"NARMADA\".title() if x==\"નર્મદા\" else \"Chhatarpur\" if (x==\"छतरपुर\" or x==\"Chhattarpur\")\n",
    "                             else 'East Singhbum' if x=='ईस्ट सिंघबम' else 'Chhindwara' if x=='छिंदवारा' else 'Jalna' if x=='जालना' else 'Dhule' if x=='धुळे' else \"Dhanbad\" if x==\"धनबाद\"\n",
    "                             else 'Banas Kantha' if x=='બનાસ કાંઠા' else 'Dhamtari' if x=='धमतरी' else 'Bilaspur' if x=='बिलासपुर' else dist[x])\n",
    "\n",
    "if \"Citizen District\" in data0.columns:\n",
    "    dist = distName(data0['Citizen District'])\n",
    "    data0['Citizen District'] = data0['Citizen District'].apply(lambda x: \"Dohad\" if x==\"દોહદ\" else \"Haridwar\" if x==\"हरिद्वार\" else \"UDHAM SINGH NAGAR\".title() if (x==\"उदम सिंह नगर\" or x==\"Udam Singh Nagar\" or x==\"UDAM SINGH NAGAR\")\n",
    "                                                                else \"Sitamarhi\" if x==\"सीतामढ़ी\" else \"Mahasamund\" if (x==\"महासमुंद\" or x==\"Mahasamand\")\n",
    "                                                                else \"Rajgarh\" if x==\"राजगढ़\" else \"Muzaffarpur\" if x==\"मुजफ्फरपुर\" else \"Nawada\" if x==\"नवादा\" else \"Balrampur\" if x==\"बलरामपुर\"\n",
    "                                                                else \"DAMOH\".title() if x==\"दमोह\" else \"Shravasti\" if x==\"श्रावस्ती\" else \"NARMADA\".title() if x==\"નર્મદા\" else \"Chhatarpur\" if (x==\"छतरपुर\" or x==\"Chhattarpur\")\n",
    "                                                                else 'East Singhbum' if x=='ईस्ट सिंघबम' else 'Chhindwara' if x=='छिंदवारा' else 'Jalna' if x=='जालना' else 'Dhule' if x=='धुळे' else \"Dhanbad\" if x==\"धनबाद\"\n",
    "                                                                else 'Banas Kantha' if x=='બનાસ કાંઠા' else 'Dhamtari' if x=='धमतरी' else 'Bilaspur' if x=='बिलासपुर' else dist[x])\n",
    "\n",
    "data0['District'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97e3d9c-4aeb-4b1a-83e8-c759b3f9d20a",
   "metadata": {
    "id": "a97e3d9c-4aeb-4b1a-83e8-c759b3f9d20a"
   },
   "source": [
    "# Mention orgwise scheme applied raw data filename below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1798acf-fde1-4fe6-850d-549e436a4272",
   "metadata": {
    "id": "c1798acf-fde1-4fe6-850d-549e436a4272"
   },
   "outputs": [],
   "source": [
    "file_Name = 'orgwise_schemes_applied.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a052ab6c-bce1-48aa-9145-4966e4a7faf2",
   "metadata": {
    "id": "a052ab6c-bce1-48aa-9145-4966e4a7faf2",
    "outputId": "0f50a0a6-bdee-468b-b724-51fe04699f09",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Org Name</th>\n",
       "      <th>Project Id</th>\n",
       "      <th>Scheme Id</th>\n",
       "      <th>status</th>\n",
       "      <th>Parent Scheme GUID</th>\n",
       "      <th>Parent Scheme</th>\n",
       "      <th>Scheme Name</th>\n",
       "      <th>Scheme type</th>\n",
       "      <th>Fee</th>\n",
       "      <th>Timeline</th>\n",
       "      <th>Benefit Value</th>\n",
       "      <th>Open</th>\n",
       "      <th>Submitted</th>\n",
       "      <th>Received</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Jharkhand</td>\n",
       "      <td>Jharkhand</td>\n",
       "      <td>PID/HYUND1/2025/DI/YK/0530</td>\n",
       "      <td>SH0009RR</td>\n",
       "      <td>True</td>\n",
       "      <td>PSH0009SG</td>\n",
       "      <td>Yojana Card</td>\n",
       "      <td>Loyalty Yojana Card</td>\n",
       "      <td>sch</td>\n",
       "      <td>85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>PID/HYUND1/2025/DI/YK/0530</td>\n",
       "      <td>DC0000Z5</td>\n",
       "      <td>True</td>\n",
       "      <td>PDC0009LO</td>\n",
       "      <td>BOCW card</td>\n",
       "      <td>UP-BOCW Construction Labour Card</td>\n",
       "      <td>sch</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>7721</td>\n",
       "      <td>16</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>PID/HYUND1/2025/DI/YK/0530</td>\n",
       "      <td>SH000DCC</td>\n",
       "      <td>True</td>\n",
       "      <td>SH000DCC</td>\n",
       "      <td>National Pension System (Individual)</td>\n",
       "      <td>National Pension System (Individual)</td>\n",
       "      <td>sch</td>\n",
       "      <td>200</td>\n",
       "      <td>20</td>\n",
       "      <td>332668</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>PID/HYUND1/2025/DI/YK/0530</td>\n",
       "      <td>DC000A26</td>\n",
       "      <td>True</td>\n",
       "      <td>PDC000D6C</td>\n",
       "      <td>Instant e-PAN(Central)</td>\n",
       "      <td>Instant e-PAN(Central)</td>\n",
       "      <td>doc</td>\n",
       "      <td>50</td>\n",
       "      <td>2 Days</td>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>PID/HYUND1/2025/DI/YK/0530</td>\n",
       "      <td>SH000BEO</td>\n",
       "      <td>True</td>\n",
       "      <td>PSH000D8M</td>\n",
       "      <td>Old Age Pension</td>\n",
       "      <td>Old Age Pension_UP</td>\n",
       "      <td>sch</td>\n",
       "      <td>0</td>\n",
       "      <td>30 - 90 da</td>\n",
       "      <td>60000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            State       Org Name                  Project Id Scheme Id  \\\n",
       "17      Jharkhand      Jharkhand  PID/HYUND1/2025/DI/YK/0530  SH0009RR   \n",
       "18  Uttar Pradesh  Uttar Pradesh  PID/HYUND1/2025/DI/YK/0530  DC0000Z5   \n",
       "19  Uttar Pradesh  Uttar Pradesh  PID/HYUND1/2025/DI/YK/0530  SH000DCC   \n",
       "20  Uttar Pradesh  Uttar Pradesh  PID/HYUND1/2025/DI/YK/0530  DC000A26   \n",
       "21  Uttar Pradesh  Uttar Pradesh  PID/HYUND1/2025/DI/YK/0530  SH000BEO   \n",
       "\n",
       "    status Parent Scheme GUID                         Parent Scheme  \\\n",
       "17    True          PSH0009SG                           Yojana Card   \n",
       "18    True          PDC0009LO                             BOCW card   \n",
       "19    True           SH000DCC  National Pension System (Individual)   \n",
       "20    True          PDC000D6C                Instant e-PAN(Central)   \n",
       "21    True          PSH000D8M                       Old Age Pension   \n",
       "\n",
       "                             Scheme Name Scheme type  Fee    Timeline  \\\n",
       "17                   Loyalty Yojana Card         sch   85         NaN   \n",
       "18      UP-BOCW Construction Labour Card         sch   50          30   \n",
       "19  National Pension System (Individual)         sch  200          20   \n",
       "20                Instant e-PAN(Central)         doc   50      2 Days   \n",
       "21                    Old Age Pension_UP         sch    0  30 - 90 da   \n",
       "\n",
       "   Benefit Value  Open  Submitted  Received  \n",
       "17            50     1        1.0       1.0  \n",
       "18          7721    16        8.0       NaN  \n",
       "19        332668     1        NaN       NaN  \n",
       "20           100     2        NaN       NaN  \n",
       "21         60000     1        1.0       NaN  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing Schemes data from Orgwise schemes applied report\n",
    "schemeDetails = csvORexcel() # Reading the excel file\n",
    "schemeDetails = schemeDetails[schemeDetails['Project Id'] == PID] # Filtering data based on Project ID.\n",
    "#schemeDetails = schemeDetails[~schemeDetails['Parent Scheme GUID'].isna()]\n",
    "schemeDetails[\"Parent Scheme GUID\"] = schemeDetails[[\"Parent Scheme GUID\",\"Scheme Id\"]].apply(lambda x: x[1] if x[0] in [np.nan] else x[0], axis=1) # Filling missing values in Parent Scheme ID with Child Scheme Id\n",
    "schemeDetails[\"Parent Scheme\"] = schemeDetails[[\"Parent Scheme\",\"Scheme Name\"]].apply(lambda x: x[1] if x[0] in [np.nan] else x[0], axis=1) # Filling missing values in Parent Scheme with Child Scheme Name\n",
    "schemeDetails['Org Name'] = schemeDetails['Org Name'].apply(lambda x: states[x[:2]]+'-'+x[-2:] if x[-2:]=='LI' else states[x[:2]]) # Updating org name with state name\n",
    "schemeDetails.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca710be9-cb08-4506-84a0-6ea518e604a9",
   "metadata": {
    "id": "ca710be9-cb08-4506-84a0-6ea518e604a9",
    "outputId": "9df77204-d813-4343-b6a4-dcdfc6a20869"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case Id</th>\n",
       "      <th>Createdon</th>\n",
       "      <th>Scheme/Doc</th>\n",
       "      <th>Scheme/Doc GUID</th>\n",
       "      <th>Status</th>\n",
       "      <th>Docket Submitted Date</th>\n",
       "      <th>Benefit received Date</th>\n",
       "      <th>HD Suspected Cases</th>\n",
       "      <th>State</th>\n",
       "      <th>District</th>\n",
       "      <th>Citizen GUID</th>\n",
       "      <th>Citizen Name</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Mobile</th>\n",
       "      <th>Age</th>\n",
       "      <th>Citizen Block</th>\n",
       "      <th>Citizen Village</th>\n",
       "      <th>Family GUID</th>\n",
       "      <th>Family Name</th>\n",
       "      <th>HD ID</th>\n",
       "      <th>HD Name</th>\n",
       "      <th>Opsco name</th>\n",
       "      <th>AMS ID</th>\n",
       "      <th>isYC</th>\n",
       "      <th>Parent Scheme</th>\n",
       "      <th>Scheme type</th>\n",
       "      <th>Benefit Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EP05ODZG</td>\n",
       "      <td>2025-12-23</td>\n",
       "      <td>Loyalty Yojana Card</td>\n",
       "      <td>SH0009RR</td>\n",
       "      <td>Benefit Received</td>\n",
       "      <td>2025-12-23</td>\n",
       "      <td>2025-12-23</td>\n",
       "      <td>Not Suspected</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>Pune</td>\n",
       "      <td>PN05ABV6</td>\n",
       "      <td>Sanjay Narayan Kajale</td>\n",
       "      <td>Male</td>\n",
       "      <td>7218392382.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>MAVAL</td>\n",
       "      <td>Chikhalse</td>\n",
       "      <td>PG04Z1BT</td>\n",
       "      <td>Kajale</td>\n",
       "      <td>ashwini.modhave</td>\n",
       "      <td>Ashwini Rohidas Modhave</td>\n",
       "      <td>Ravindra Kathe</td>\n",
       "      <td>31254.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yojana Card</td>\n",
       "      <td>Scheme</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EP05OE0C</td>\n",
       "      <td>2025-12-23</td>\n",
       "      <td>Loyalty Yojana Card</td>\n",
       "      <td>SH0009RR</td>\n",
       "      <td>Benefit Received</td>\n",
       "      <td>2025-12-23</td>\n",
       "      <td>2025-12-23</td>\n",
       "      <td>Not Suspected</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>Pune</td>\n",
       "      <td>PN05ABVQ</td>\n",
       "      <td>Jyoti Sanjay Kajale</td>\n",
       "      <td>Female</td>\n",
       "      <td>9823121639.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>MAVAL</td>\n",
       "      <td>Chikhalse</td>\n",
       "      <td>PG04Z1BT</td>\n",
       "      <td>Kajale</td>\n",
       "      <td>ashwini.modhave</td>\n",
       "      <td>Ashwini Rohidas Modhave</td>\n",
       "      <td>Ravindra Kathe</td>\n",
       "      <td>31254.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yojana Card</td>\n",
       "      <td>Scheme</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EP05OE2G</td>\n",
       "      <td>2025-12-23</td>\n",
       "      <td>Ekyc of Ration Card_MH</td>\n",
       "      <td>SH000DU6</td>\n",
       "      <td>Benefit Received</td>\n",
       "      <td>2025-12-24</td>\n",
       "      <td>2025-12-24</td>\n",
       "      <td>Not Suspected</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>Pune</td>\n",
       "      <td>PN05ABVQ</td>\n",
       "      <td>Jyoti Sanjay Kajale</td>\n",
       "      <td>Female</td>\n",
       "      <td>9823121639.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>MAVAL</td>\n",
       "      <td>Chikhalse</td>\n",
       "      <td>PG04Z1BT</td>\n",
       "      <td>Kajale</td>\n",
       "      <td>ashwini.modhave</td>\n",
       "      <td>Ashwini Rohidas Modhave</td>\n",
       "      <td>Ravindra Kathe</td>\n",
       "      <td>31254.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Ekyc of Ration Card_MH</td>\n",
       "      <td>Scheme</td>\n",
       "      <td>46480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EP05OFHB</td>\n",
       "      <td>2025-12-23</td>\n",
       "      <td>Ekyc of Ration Card_MH</td>\n",
       "      <td>SH000DU6</td>\n",
       "      <td>Benefit Received</td>\n",
       "      <td>2025-12-24</td>\n",
       "      <td>2025-12-24</td>\n",
       "      <td>Not Suspected</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>Pune</td>\n",
       "      <td>PN05ABV6</td>\n",
       "      <td>Sanjay Narayan Kajale</td>\n",
       "      <td>Male</td>\n",
       "      <td>7218392382.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>MAVAL</td>\n",
       "      <td>Chikhalse</td>\n",
       "      <td>PG04Z1BT</td>\n",
       "      <td>Kajale</td>\n",
       "      <td>ashwini.modhave</td>\n",
       "      <td>Ashwini Rohidas Modhave</td>\n",
       "      <td>Ravindra Kathe</td>\n",
       "      <td>31254.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Ekyc of Ration Card_MH</td>\n",
       "      <td>Scheme</td>\n",
       "      <td>46480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EP05OI9G</td>\n",
       "      <td>2025-12-24</td>\n",
       "      <td>Pradhan Mantri Jan Arogya Yojana (Ayushman Bha...</td>\n",
       "      <td>SH0005JK</td>\n",
       "      <td>Open</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Not Suspected</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>Pune</td>\n",
       "      <td>PN05AEPR</td>\n",
       "      <td>Ranjana Popat Bhalerao</td>\n",
       "      <td>Female</td>\n",
       "      <td>7218102454.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>MAVAL</td>\n",
       "      <td>Chikhalse</td>\n",
       "      <td>PG04Z3UV</td>\n",
       "      <td>Bhalerao</td>\n",
       "      <td>ashwini.modhave</td>\n",
       "      <td>Ashwini Rohidas Modhave</td>\n",
       "      <td>Ravindra Kathe</td>\n",
       "      <td>31254.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Pradhan Mantri Jan Arogya Yojana (Ayushman Bha...</td>\n",
       "      <td>Scheme</td>\n",
       "      <td>86564</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Case Id  Createdon                                         Scheme/Doc  \\\n",
       "0  EP05ODZG 2025-12-23                                Loyalty Yojana Card   \n",
       "1  EP05OE0C 2025-12-23                                Loyalty Yojana Card   \n",
       "2  EP05OE2G 2025-12-23                             Ekyc of Ration Card_MH   \n",
       "3  EP05OFHB 2025-12-23                             Ekyc of Ration Card_MH   \n",
       "4  EP05OI9G 2025-12-24  Pradhan Mantri Jan Arogya Yojana (Ayushman Bha...   \n",
       "\n",
       "  Scheme/Doc GUID            Status Docket Submitted Date  \\\n",
       "0        SH0009RR  Benefit Received            2025-12-23   \n",
       "1        SH0009RR  Benefit Received            2025-12-23   \n",
       "2        SH000DU6  Benefit Received            2025-12-24   \n",
       "3        SH000DU6  Benefit Received            2025-12-24   \n",
       "4        SH0005JK              Open                   NaT   \n",
       "\n",
       "  Benefit received Date HD Suspected Cases        State District Citizen GUID  \\\n",
       "0            2025-12-23      Not Suspected  Maharashtra     Pune     PN05ABV6   \n",
       "1            2025-12-23      Not Suspected  Maharashtra     Pune     PN05ABVQ   \n",
       "2            2025-12-24      Not Suspected  Maharashtra     Pune     PN05ABVQ   \n",
       "3            2025-12-24      Not Suspected  Maharashtra     Pune     PN05ABV6   \n",
       "4                   NaT      Not Suspected  Maharashtra     Pune     PN05AEPR   \n",
       "\n",
       "             Citizen Name  Gender        Mobile   Age Citizen Block  \\\n",
       "0   Sanjay Narayan Kajale    Male  7218392382.0  41.0         MAVAL   \n",
       "1     Jyoti Sanjay Kajale  Female  9823121639.0  34.0         MAVAL   \n",
       "2     Jyoti Sanjay Kajale  Female  9823121639.0  34.0         MAVAL   \n",
       "3   Sanjay Narayan Kajale    Male  7218392382.0  41.0         MAVAL   \n",
       "4  Ranjana Popat Bhalerao  Female  7218102454.0  59.0         MAVAL   \n",
       "\n",
       "  Citizen Village Family GUID Family Name            HD ID  \\\n",
       "0       Chikhalse    PG04Z1BT      Kajale  ashwini.modhave   \n",
       "1       Chikhalse    PG04Z1BT      Kajale  ashwini.modhave   \n",
       "2       Chikhalse    PG04Z1BT      Kajale  ashwini.modhave   \n",
       "3       Chikhalse    PG04Z1BT      Kajale  ashwini.modhave   \n",
       "4       Chikhalse    PG04Z3UV    Bhalerao  ashwini.modhave   \n",
       "\n",
       "                   HD Name      Opsco name   AMS ID isYC  \\\n",
       "0  Ashwini Rohidas Modhave  Ravindra Kathe  31254.0  Yes   \n",
       "1  Ashwini Rohidas Modhave  Ravindra Kathe  31254.0  Yes   \n",
       "2  Ashwini Rohidas Modhave  Ravindra Kathe  31254.0  Yes   \n",
       "3  Ashwini Rohidas Modhave  Ravindra Kathe  31254.0  Yes   \n",
       "4  Ashwini Rohidas Modhave  Ravindra Kathe  31254.0  Yes   \n",
       "\n",
       "                                       Parent Scheme Scheme type  \\\n",
       "0                                        Yojana Card      Scheme   \n",
       "1                                        Yojana Card      Scheme   \n",
       "2                             Ekyc of Ration Card_MH      Scheme   \n",
       "3                             Ekyc of Ration Card_MH      Scheme   \n",
       "4  Pradhan Mantri Jan Arogya Yojana (Ayushman Bha...      Scheme   \n",
       "\n",
       "   Benefit Value  \n",
       "0             50  \n",
       "1             50  \n",
       "2          46480  \n",
       "3          46480  \n",
       "4          86564  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data0['stSchCon'] = data0['State']+data0['Scheme/Doc GUID'] # Adding a concanetaed column for state and scheme ID\n",
    "schemeDetails['stSchCon'] = schemeDetails['Org Name']+schemeDetails['Scheme Id'] # Adding a concanetaed column for state and scheme ID\n",
    "\n",
    "# Removing colomuns except 'Scheme Id','Scheme type','Benefit Value' to merge with main dataframe\n",
    "for s in schemeDetails.columns:\n",
    "    if s not in ['Scheme type','Benefit Value','Parent Scheme','stSchCon']:\n",
    "        schemeDetails.drop(columns=s, inplace=True)\n",
    "\n",
    "# Merging scheme details with main dataframe to get data of Scheme type & Benefit Value.\n",
    "data0 = data0.merge(schemeDetails.drop_duplicates(subset=['stSchCon'], keep='last'), left_on='stSchCon', right_on=\"stSchCon\", how='left')\n",
    "\n",
    "# Removing non required column \"Scheme ID\"\n",
    "data0.drop(columns = 'stSchCon', inplace=True)\n",
    "\n",
    "# Changing short form to \"Scheme\" & \"Document\"\n",
    "data0['Scheme type'] = data0['Scheme type'].apply(lambda x: 'Scheme' if x=='sch' else 'Document' if x=='doc' else x)\n",
    "\n",
    "# Converting \"Benefit Value\" columns to integer type\n",
    "data0['Benefit Value'].fillna('0', inplace=True)\n",
    "data0['Benefit Value'] = data0['Benefit Value'].apply(lambda x: int(x) if x.isnumeric() else 0)\n",
    "data0['Benefit Value'] = data0['Benefit Value'].astype('int64')\n",
    "\n",
    "data0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d2fe7ac-12a7-43ab-9bca-2dfb67ccfca7",
   "metadata": {
    "id": "6d2fe7ac-12a7-43ab-9bca-2dfb67ccfca7",
    "outputId": "ddd73f38-317a-4a27-e7f3-1f7747a59b58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 120332 entries, 0 to 120331\n",
      "Data columns (total 27 columns):\n",
      " #   Column                 Non-Null Count   Dtype         \n",
      "---  ------                 --------------   -----         \n",
      " 0   Case Id                120331 non-null  object        \n",
      " 1   Createdon              120332 non-null  datetime64[ns]\n",
      " 2   Scheme/Doc             120332 non-null  object        \n",
      " 3   Scheme/Doc GUID        120332 non-null  object        \n",
      " 4   Status                 120332 non-null  object        \n",
      " 5   Docket Submitted Date  104074 non-null  datetime64[ns]\n",
      " 6   Benefit received Date  95232 non-null   datetime64[ns]\n",
      " 7   HD Suspected Cases     120332 non-null  object        \n",
      " 8   State                  120332 non-null  object        \n",
      " 9   District               120332 non-null  object        \n",
      " 10  Citizen GUID           120332 non-null  object        \n",
      " 11  Citizen Name           120332 non-null  object        \n",
      " 12  Gender                 120332 non-null  object        \n",
      " 13  Mobile                 120332 non-null  object        \n",
      " 14  Age                    120332 non-null  float64       \n",
      " 15  Citizen Block          61709 non-null   object        \n",
      " 16  Citizen Village        61737 non-null   object        \n",
      " 17  Family GUID            120332 non-null  object        \n",
      " 18  Family Name            120329 non-null  object        \n",
      " 19  HD ID                  120332 non-null  object        \n",
      " 20  HD Name                120332 non-null  object        \n",
      " 21  Opsco name             120332 non-null  object        \n",
      " 22  AMS ID                 120332 non-null  float64       \n",
      " 23  isYC                   120332 non-null  object        \n",
      " 24  Parent Scheme          120332 non-null  object        \n",
      " 25  Scheme type            120332 non-null  object        \n",
      " 26  Benefit Value          120332 non-null  int64         \n",
      "dtypes: datetime64[ns](3), float64(2), int64(1), object(21)\n",
      "memory usage: 24.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# Checking if number of data point has increased or not\n",
    "data0.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f08464b-2bd3-4750-963b-eab0e76d38ad",
   "metadata": {
    "id": "0f08464b-2bd3-4750-963b-eab0e76d38ad"
   },
   "source": [
    "# Mention rate card raw data file name below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7aef8fcb-cc21-4b21-befc-20849ba71f3a",
   "metadata": {
    "id": "7aef8fcb-cc21-4b21-befc-20849ba71f3a"
   },
   "outputs": [],
   "source": [
    "file_Name = 'rate_card.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1cb0345d-eae3-4ea5-9b55-b9b36a147733",
   "metadata": {
    "id": "1cb0345d-eae3-4ea5-9b55-b9b36a147733",
    "outputId": "0216fd0c-b624-41af-acfd-19e8979eb6f1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Org_PID</th>\n",
       "      <th>Org_Name</th>\n",
       "      <th>org_id</th>\n",
       "      <th>schemes_Guid</th>\n",
       "      <th>sch_name</th>\n",
       "      <th>sch id</th>\n",
       "      <th>ratecard_guid</th>\n",
       "      <th>open_price</th>\n",
       "      <th>Docket submitted price</th>\n",
       "      <th>scheme_document_received price</th>\n",
       "      <th>created_on</th>\n",
       "      <th>mapping date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81083</th>\n",
       "      <td>PID/HYUND1/2025/DI/YK/0530</td>\n",
       "      <td>Haryana</td>\n",
       "      <td>1756</td>\n",
       "      <td>DC0000YY</td>\n",
       "      <td>Aadhaar Card (Central)</td>\n",
       "      <td>1258</td>\n",
       "      <td>RC011N48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2025-08-11 10:57:28</td>\n",
       "      <td>2025-08-25 15:39:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81104</th>\n",
       "      <td>PID/HYUND1/2025/DI/YK/0530</td>\n",
       "      <td>Haryana</td>\n",
       "      <td>1756</td>\n",
       "      <td>DC000E93</td>\n",
       "      <td>HR-Economically Weaker Section (EWS) Certificate</td>\n",
       "      <td>18471</td>\n",
       "      <td>RC011N48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2025-08-11 10:57:28</td>\n",
       "      <td>2025-08-25 15:50:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81106</th>\n",
       "      <td>PID/HYUND1/2025/DI/YK/0530</td>\n",
       "      <td>Haryana</td>\n",
       "      <td>1756</td>\n",
       "      <td>DC0000XY</td>\n",
       "      <td>HR-Income Certificate</td>\n",
       "      <td>1222</td>\n",
       "      <td>RC011N48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2025-08-11 10:57:28</td>\n",
       "      <td>2025-08-25 15:45:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81140</th>\n",
       "      <td>PID/HYUND1/2025/DI/YK/0530</td>\n",
       "      <td>Haryana</td>\n",
       "      <td>1756</td>\n",
       "      <td>SH000DQQ</td>\n",
       "      <td>Registration of contributory workers with Hary...</td>\n",
       "      <td>17810</td>\n",
       "      <td>RC011N48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2025-08-11 10:57:28</td>\n",
       "      <td>2025-08-25 15:45:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81108</th>\n",
       "      <td>PID/HYUND1/2025/DI/YK/0530</td>\n",
       "      <td>Haryana</td>\n",
       "      <td>1756</td>\n",
       "      <td>DC0000XX</td>\n",
       "      <td>HR-Resident Certificate</td>\n",
       "      <td>1221</td>\n",
       "      <td>RC011N48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2025-08-11 10:57:28</td>\n",
       "      <td>2025-08-25 15:46:47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Org_PID Org_Name  org_id schemes_Guid  \\\n",
       "81083  PID/HYUND1/2025/DI/YK/0530  Haryana    1756     DC0000YY   \n",
       "81104  PID/HYUND1/2025/DI/YK/0530  Haryana    1756     DC000E93   \n",
       "81106  PID/HYUND1/2025/DI/YK/0530  Haryana    1756     DC0000XY   \n",
       "81140  PID/HYUND1/2025/DI/YK/0530  Haryana    1756     SH000DQQ   \n",
       "81108  PID/HYUND1/2025/DI/YK/0530  Haryana    1756     DC0000XX   \n",
       "\n",
       "                                                sch_name  sch id  \\\n",
       "81083                             Aadhaar Card (Central)    1258   \n",
       "81104   HR-Economically Weaker Section (EWS) Certificate   18471   \n",
       "81106                              HR-Income Certificate    1222   \n",
       "81140  Registration of contributory workers with Hary...   17810   \n",
       "81108                            HR-Resident Certificate    1221   \n",
       "\n",
       "      ratecard_guid  open_price  Docket submitted price  \\\n",
       "81083      RC011N48         0.0                     0.0   \n",
       "81104      RC011N48         0.0                     0.0   \n",
       "81106      RC011N48         0.0                     0.0   \n",
       "81140      RC011N48         0.0                     0.0   \n",
       "81108      RC011N48         0.0                     0.0   \n",
       "\n",
       "       scheme_document_received price          created_on        mapping date  \n",
       "81083                             0.0 2025-08-11 10:57:28 2025-08-25 15:39:54  \n",
       "81104                             0.0 2025-08-11 10:57:28 2025-08-25 15:50:36  \n",
       "81106                             0.0 2025-08-11 10:57:28 2025-08-25 15:45:49  \n",
       "81140                             0.0 2025-08-11 10:57:28 2025-08-25 15:45:20  \n",
       "81108                             0.0 2025-08-11 10:57:28 2025-08-25 15:46:47  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rate_card = csvORexcel()\n",
    "rate_card = rate_card[rate_card['Org_PID'] == PID].sort_values('created_on')\n",
    "# Changing Case Organization values from state initials to full state name.\n",
    "rate_card['Org_Name'] = rate_card['Org_Name'].apply(lambda x: states[x[:2]]+'-'+x[-2:] if x[-2:]=='LI' else states[x[:2]])\n",
    "rate_card.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db7a6295-a274-4884-8ea5-9221b906a0ac",
   "metadata": {
    "id": "e3e1d37b-37b4-4780-9988-d2aa46cda974"
   },
   "outputs": [],
   "source": [
    "# Defining a function for HD payment calculation which will take cases report and rate card as input and return final cases report with hd payment column\n",
    "\n",
    "def hdPayment_new(data0, rate_card):\n",
    "    open_price = []\n",
    "    Docket_submitted_price = []\n",
    "    scheme_document_received_price\t= []\n",
    "    HD_Payment = []\n",
    "    nonMappedSch = []\n",
    "\n",
    "    for idx in data0.index:\n",
    "        if data0['Scheme/Doc GUID'][idx] in rate_card['schemes_Guid'].to_list():\n",
    "            sch_rate_card = rate_card[(rate_card['Org_Name'] == data0['State'][idx]) & (rate_card['schemes_Guid'] == data0['Scheme/Doc GUID'][idx])].sort_values('created_on').reset_index(drop=True)\n",
    "            # This condition is for 1 rate card update\n",
    "            if len(sch_rate_card) == 0:\n",
    "                open_price.append(0.0)\n",
    "                Docket_submitted_price.append(0.0)\n",
    "                scheme_document_received_price.append(0.0)\n",
    "                nonMappedSch.append(data0['State'][idx]+\":\"+data0['Scheme/Doc GUID'][idx])\n",
    "                #print(\"Scheme {0} from org {1} not mapped in rate card!\".format(data0['Scheme/Doc GUID'][idx],data0['State'][idx]))\n",
    "            elif len(sch_rate_card) == 1:\n",
    "                open_price.append(sch_rate_card['open_price'][0])\n",
    "                Docket_submitted_price.append(sch_rate_card['Docket submitted price'][0])\n",
    "                scheme_document_received_price.append(sch_rate_card['scheme_document_received price'][0])\n",
    "            else:\n",
    "                # This condition is for 2 or more rate card update\n",
    "                for i in sch_rate_card.index:\n",
    "                    if data0['Createdon'][idx].date() <= sch_rate_card['created_on'][i].date():\n",
    "                        open_price.append(sch_rate_card['open_price'][i])\n",
    "                        Docket_submitted_price.append(sch_rate_card['Docket submitted price'][i])\n",
    "                        scheme_document_received_price.append(sch_rate_card['scheme_document_received price'][i])\n",
    "                        break\n",
    "                    elif data0['Createdon'][idx].date() >= sch_rate_card['created_on'].iloc[-1].date():\n",
    "                        open_price.append(sch_rate_card['open_price'].iloc[-1])\n",
    "                        Docket_submitted_price.append(sch_rate_card['Docket submitted price'].iloc[-1])\n",
    "                        scheme_document_received_price.append(sch_rate_card['scheme_document_received price'].iloc[-1])\n",
    "                        break\n",
    "                    elif (data0['Createdon'][idx].date() > sch_rate_card['created_on'][i].date()) & (data0['Createdon'][idx].date() < sch_rate_card['created_on'][i+1].date()):\n",
    "                        open_price.append(sch_rate_card['open_price'][i])\n",
    "                        Docket_submitted_price.append(sch_rate_card['Docket submitted price'][i])\n",
    "                        scheme_document_received_price.append(sch_rate_card['scheme_document_received price'][i])\n",
    "                        break\n",
    "                    else:\n",
    "                        open_price.append(sch_rate_card['open_price'][i])\n",
    "                        Docket_submitted_price.append(sch_rate_card['Docket submitted price'][i])\n",
    "                        scheme_document_received_price.append(sch_rate_card['scheme_document_received price'][i])\n",
    "                        break\n",
    "        else:\n",
    "            open_price.append(0.0)\n",
    "            Docket_submitted_price.append(0.0)\n",
    "            scheme_document_received_price.append(0.0)\n",
    "\n",
    "    data0['open_price'] = open_price\n",
    "    data0['Docket submitted price'] = Docket_submitted_price\n",
    "    data0['scheme_document_received price'] = scheme_document_received_price\n",
    "\n",
    "    for idx in data0.index:\n",
    "        if data0.Status[idx] == 'Open':\n",
    "            HD_Payment.append(data0.open_price[idx])\n",
    "        elif data0.Status[idx] == 'Submitted':\n",
    "            HD_Payment.append(data0.open_price[idx] + data0['Docket submitted price'][idx])\n",
    "        else:\n",
    "            HD_Payment.append(data0.open_price[idx] + data0['Docket submitted price'][idx] + data0['scheme_document_received price'][idx])\n",
    "\n",
    "    data0['HD_Payment'] = HD_Payment # Adding column for HD payment.\n",
    "\n",
    "    if len(nonMappedSch) > 0:\n",
    "        nonMapped = dict()\n",
    "        for item in nonMappedSch:\n",
    "            key, value = item.split(\":\")\n",
    "            if key in nonMapped:\n",
    "                nonMapped[key].add(value)\n",
    "            else:\n",
    "                nonMapped[key] = {value}\n",
    "        print(\"Below schemes are not mapped in rate card:\\n\",nonMapped)\n",
    "\n",
    "    return data0"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3f07b862-5fd6-499f-be42-13ad2c0fd5cf",
   "metadata": {
    "id": "e3e1d37b-37b4-4780-9988-d2aa46cda974"
   },
   "source": [
    "# Defining a function for HD payment calculation which will take cases report and rate card as input and return final cases report with hd payment column\n",
    "\n",
    "def hdPayment_new(data0, rate_card):\n",
    "    open_price = []\n",
    "    Docket_submitted_price = []\n",
    "    scheme_document_received_price\t= []\n",
    "    HD_Payment = []\n",
    "    nonMappedSch = []\n",
    "\n",
    "    for idx in data0.index:\n",
    "        if data0['Scheme/Doc GUID'][idx] in rate_card['schemes_Guid'].to_list():\n",
    "            sch_rate_card = rate_card[(rate_card['Org_Name'] == data0['State'][idx]) & (rate_card['schemes_Guid'] == data0['Scheme/Doc GUID'][idx])].sort_values('created_on').reset_index(drop=True)\n",
    "            # This condition is for 1 rate card update\n",
    "            if len(sch_rate_card) == 0:\n",
    "                open_price.append(0.0)\n",
    "                Docket_submitted_price.append(0.0)\n",
    "                scheme_document_received_price.append(0.0)\n",
    "                nonMappedSch.append(data0['State'][idx]+\":\"+data0['Scheme/Doc GUID'][idx])\n",
    "                #print(\"Scheme {0} from org {1} not mapped in rate card!\".format(data0['Scheme/Doc GUID'][idx],data0['State'][idx]))\n",
    "            elif len(sch_rate_card) == 1:\n",
    "                open_price.append(sch_rate_card['open_price'][0])\n",
    "                Docket_submitted_price.append(sch_rate_card['Docket submitted price'][0])\n",
    "                scheme_document_received_price.append(sch_rate_card['scheme_document_received price'][0])\n",
    "            else:\n",
    "                # This condition is for 2 or more rate card update\n",
    "                for i in sch_rate_card.index:\n",
    "                    if data0['Createdon'][idx].date() <= sch_rate_card['created_on'][i].date():\n",
    "                        open_price.append(sch_rate_card['open_price'][i])\n",
    "                        break\n",
    "                    elif data0['Createdon'][idx].date() >= sch_rate_card['created_on'].iloc[-1].date():\n",
    "                        open_price.append(sch_rate_card['open_price'].iloc[-1])\n",
    "                        break\n",
    "                    elif (data0['Createdon'][idx].date() > sch_rate_card['created_on'][i].date()) & (data0['Createdon'][idx].date() < sch_rate_card['created_on'][i+1].date()):\n",
    "                        open_price.append(sch_rate_card['open_price'][i])\n",
    "                        break\n",
    "                    else:\n",
    "                        open_price.append(sch_rate_card['open_price'][i])\n",
    "                        break\n",
    "                for i in sch_rate_card.index:\n",
    "                    if pd.isnull(data0['Docket Submitted Date'][idx]):\n",
    "                        if data0['Createdon'][idx].date() <= sch_rate_card['created_on'][i].date():\n",
    "                            Docket_submitted_price.append(sch_rate_card['Docket submitted price'][i])\n",
    "                            break\n",
    "                        elif data0['Createdon'][idx].date() >= sch_rate_card['created_on'].iloc[-1].date():\n",
    "                            Docket_submitted_price.append(sch_rate_card['Docket submitted price'].iloc[-1])\n",
    "                            break\n",
    "                        elif (data0['Createdon'][idx].date() > sch_rate_card['created_on'][i].date()) & (data0['Createdon'][idx].date() < sch_rate_card['created_on'][i+1].date()):\n",
    "                            Docket_submitted_price.append(sch_rate_card['Docket submitted price'][i])\n",
    "                            break\n",
    "                        else:\n",
    "                            Docket_submitted_price.append(sch_rate_card['Docket submitted price'][i])\n",
    "                            break\n",
    "                    else:\n",
    "                        if data0['Docket Submitted Date'][idx].date() <= sch_rate_card['created_on'][i].date():\n",
    "                            Docket_submitted_price.append(sch_rate_card['Docket submitted price'][i])\n",
    "                            break\n",
    "                        elif data0['Docket Submitted Date'][idx].date() >= sch_rate_card['created_on'].iloc[-1].date():\n",
    "                            Docket_submitted_price.append(sch_rate_card['Docket submitted price'].iloc[-1])\n",
    "                            break\n",
    "                        elif (data0['Docket Submitted Date'][idx].date() > sch_rate_card['created_on'][i].date()) & (data0['Docket Submitted Date'][idx].date() < sch_rate_card['created_on'][i+1].date()):\n",
    "                            Docket_submitted_price.append(sch_rate_card['Docket submitted price'][i])\n",
    "                            break\n",
    "                        else:\n",
    "                            Docket_submitted_price.append(sch_rate_card['Docket submitted price'][i])\n",
    "                            break\n",
    "                for i in sch_rate_card.index:\n",
    "                    if pd.isnull(data0['Benefit received Date'][idx]):\n",
    "                        if data0['Createdon'][idx].date() <= sch_rate_card['created_on'][i].date():\n",
    "                            scheme_document_received_price.append(sch_rate_card['scheme_document_received price'][i])\n",
    "                            break\n",
    "                        elif data0['Createdon'][idx].date() >= sch_rate_card['created_on'].iloc[-1].date():\n",
    "                            scheme_document_received_price.append(sch_rate_card['scheme_document_received price'].iloc[-1])\n",
    "                            break\n",
    "                        elif (data0['Createdon'][idx].date() > sch_rate_card['created_on'][i].date()) & (data0['Createdon'][idx].date() < sch_rate_card['created_on'][i+1].date()):\n",
    "                            scheme_document_received_price.append(sch_rate_card['scheme_document_received price'][i])\n",
    "                            break\n",
    "                        else:\n",
    "                            scheme_document_received_price.append(sch_rate_card['scheme_document_received price'][i])\n",
    "                            break\n",
    "                    else:\n",
    "                        if data0['Benefit received Date'][idx].date() <= sch_rate_card['created_on'][i].date():\n",
    "                            scheme_document_received_price.append(sch_rate_card['scheme_document_received price'][i])\n",
    "                            break\n",
    "                        elif data0['Benefit received Date'][idx].date() >= sch_rate_card['created_on'].iloc[-1].date():\n",
    "                            scheme_document_received_price.append(sch_rate_card['scheme_document_received price'].iloc[-1])\n",
    "                            break\n",
    "                        elif (data0['Benefit received Date'][idx].date() > sch_rate_card['created_on'][i].date()) & (data0['Benefit received Date'][idx].date() < sch_rate_card['created_on'][i+1].date()):\n",
    "                            scheme_document_received_price.append(sch_rate_card['scheme_document_received price'][i])\n",
    "                            break\n",
    "                        else:\n",
    "                            scheme_document_received_price.append(sch_rate_card['scheme_document_received price'][i])\n",
    "                            break\n",
    "        else:\n",
    "            open_price.append(0.0)\n",
    "            Docket_submitted_price.append(0.0)\n",
    "            scheme_document_received_price.append(0.0)\n",
    "\n",
    "    data0['open_price'] = open_price\n",
    "    data0['Docket submitted price'] = Docket_submitted_price\n",
    "    data0['scheme_document_received price'] = scheme_document_received_price\n",
    "\n",
    "    for idx in data0.index:\n",
    "        if data0.Status[idx] == 'Open':\n",
    "            HD_Payment.append(data0.open_price[idx])\n",
    "        elif data0.Status[idx] == 'Submitted':\n",
    "            HD_Payment.append(data0.open_price[idx] + data0['Docket submitted price'][idx])\n",
    "        else:\n",
    "            HD_Payment.append(data0.open_price[idx] + data0['Docket submitted price'][idx] + data0['scheme_document_received price'][idx])\n",
    "\n",
    "    data0['HD_Payment'] = HD_Payment # Adding column for HD payment.\n",
    "\n",
    "    if len(nonMappedSch) > 0:\n",
    "        nonMapped = dict()\n",
    "        for item in nonMappedSch:\n",
    "            key, value = item.split(\":\")\n",
    "            if key in nonMapped:\n",
    "                nonMapped[key].add(value)\n",
    "            else:\n",
    "                nonMapped[key] = {value}\n",
    "        print(\"Below schemes are not mapped in rate card:\\n\",nonMapped)\n",
    "\n",
    "    return data0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d82347a2-3ef6-4069-a350-68c9da73f052",
   "metadata": {
    "id": "d82347a2-3ef6-4069-a350-68c9da73f052"
   },
   "outputs": [],
   "source": [
    "# Defining a function for HD payment calculation which will take cases report and rate card as input and return final cases report with hd payment column\n",
    "\n",
    "def hdPayment(data0, rate_card):\n",
    "    price = {'index':[], 'Scheme/Doc GUID':[], 'open_price':[], 'Docket submitted price':[], 'scheme_document_received price':[]} # Declaring a blank dictionary for storing price.\n",
    "\n",
    "    for SID, di in zip(data0['Scheme/Doc GUID'], data0['Scheme/Doc GUID'].index): # Getting Scheme GUID and index number from schemes data.\n",
    "        price['Scheme/Doc GUID'].append(SID) # Storing Scheme GUID from schemes data.\n",
    "        price['index'].append(di) # Storing index number from schemes data.\n",
    "        rate_sch = rate_card[rate_card.schemes_Guid == SID].sort_values('created_on') # Storing scheme GUID based filtered data from rate_card.\n",
    "        rate_sch['created_on'] = rate_sch.created_on.apply(lambda x: x.strftime(\"%d-%m-%Y\"))\n",
    "        for d in rate_sch.created_on: # To check if multiple rates present under single scheme GUID based on rate card Createdon date.\n",
    "            date_sch = rate_sch[rate_sch.created_on == d].sort_values('created_on') # Get all records for same date.\n",
    "            if date_sch.shape[0] > 1:\n",
    "                rate_sch.drop(index = date_sch.index.min(), inplace=True) # Keep only latest record based on index number.\n",
    "        rate_sch.created_on = pd.to_datetime(rate_sch.created_on, format='mixed', errors='ignore')\n",
    "        if rate_sch.shape[0] > 1: # Checking if filtered data has more than 1 results.\n",
    "            for i in rate_sch.index: # Iterating through filtered results.\n",
    "                if data0.Createdon.loc[di] >= rate_sch.created_on.iloc[-1]: # Checking if cases created date is later than the rate card defined date.\n",
    "                    price['open_price'].append(rate_sch.open_price.iloc[-1]) # Storing open price from rate card to \"price\" dictionary.\n",
    "                    price['Docket submitted price'].append(rate_sch['Docket submitted price'].iloc[-1]) # Storing DS price from rate card to \"price\" dictionary.\n",
    "                    price['scheme_document_received price'].append(rate_sch['scheme_document_received price'].iloc[-1]) # Storing BR price from rate card to \"price\" dictionary.\n",
    "                    #print('for if = {0} >= {1}'.format(data0.Createdon.loc[di],rate_sch.created_on.iloc[-1]))\n",
    "                    break\n",
    "                elif data0.Createdon.loc[di] <= rate_sch.created_on.loc[i]: # Checking if cases created date is earlier than the rate card defined date.\n",
    "                    price['open_price'].append(rate_sch.open_price.loc[i]) # Storing open price from rate card to \"price\" dictionary.\n",
    "                    price['Docket submitted price'].append(rate_sch['Docket submitted price'].loc[i]) # Storing DS price from rate card to \"price\" dictionary.\n",
    "                    price['scheme_document_received price'].append(rate_sch['scheme_document_received price'].loc[i]) # Storing BR price from rate card to \"price\" dictionary.\n",
    "                    #print('for elif = {0} <= {1}'.format(data0.Createdon.loc[di],rate_sch.created_on.loc[i]))\n",
    "                    break\n",
    "        else: # This will execute if filtered data has only single result.\n",
    "            #print(SID)\n",
    "            try:\n",
    "                price['open_price'].append(rate_sch.open_price.loc[rate_sch.index[0]]) # Storing open price from rate card to \"price\" dictionary.\n",
    "                price['Docket submitted price'].append(rate_sch['Docket submitted price'].loc[rate_sch.index[0]]) # Storing DS price from rate card to \"price\" dictionary.\n",
    "                price['scheme_document_received price'].append(rate_sch['scheme_document_received price'].loc[rate_sch.index[0]]) # Storing BR price from rate card to \"price\" dictionary.\n",
    "                #print('if-else')\n",
    "            except: # This will execute if Scheme not available in rate card under selected PID\n",
    "                #print(SID)\n",
    "                price['open_price'].append(0)\n",
    "                price['Docket submitted price'].append(0)\n",
    "                price['scheme_document_received price'].append(0)\n",
    "\n",
    "    price = pd.DataFrame(price) # Converting price dictionary to pandas dataframe.\n",
    "    data0 = data0.merge(price.drop(columns=['Scheme/Doc GUID']), left_on=data0.index, right_on='index', how='left') # Merging \"price\" datafram with \"cases report\".\n",
    "    data0.drop(columns=['index'], inplace=True) # Removing index column\n",
    "    data0['HD_Payment'] = data0.open_price + data0['Docket submitted price'] + data0['scheme_document_received price'] # Adding column for HD payment.\n",
    "    return data0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081cfee3-7033-42c5-9e8b-e6aac35c9101",
   "metadata": {
    "id": "081cfee3-7033-42c5-9e8b-e6aac35c9101",
    "outputId": "9541d312-31ed-4a89-ad2e-7e3f72ca0334",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    data0 = hdPayment_new(data0, rate_card)\n",
    "    print(\"New HD Payment function executed for Data!\")\n",
    "except:\n",
    "    data0 = hdPayment(data0, rate_card)\n",
    "    print(\"Old HD Payment function executed for Data!\")\n",
    "\n",
    "try:\n",
    "    rejectedDF = hdPayment_new(rejectedDF, rate_card)\n",
    "    print(\"New HD Payment function executed for Rejected Data!\")\n",
    "except:\n",
    "    rejectedDF = hdPayment(rejectedDF, rate_card)\n",
    "    print(\"Old HD Payment function executed for Rejected Data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d78f5ed-4cc2-43ab-b82b-623e3fd69735",
   "metadata": {
    "id": "7d78f5ed-4cc2-43ab-b82b-623e3fd69735",
    "outputId": "9390b2dc-1a92-4d5f-b5c6-5c67a5a5101c"
   },
   "outputs": [],
   "source": [
    "data0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b865d9e5-323d-4014-bda6-6ec7b70df6f1",
   "metadata": {
    "id": "b865d9e5-323d-4014-bda6-6ec7b70df6f1",
    "outputId": "f180f50e-9462-4f18-800f-e105fae68eb1"
   },
   "outputs": [],
   "source": [
    "data0.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4384520-2f43-4e8f-a4ce-0f98cf1ae1f0",
   "metadata": {
    "id": "d4384520-2f43-4e8f-a4ce-0f98cf1ae1f0"
   },
   "source": [
    "# DFL Schemes\n",
    "\n",
    "- SH0009SW = Digital productivity Service_ Basic\n",
    "- SH000BM6 = Digital productivity Service_Basic\n",
    "- SH000AG6 = Digital Productivity Services_Advanced\n",
    "- SH000A32 = Long Training on Digital & Financial Inclusion_Private\n",
    "- SH0009SW = Short Training on Digital & Financial Inclusion_Private\n",
    "- SH000AG6 = Digital Productivity Services and and employability training_Advanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58b3adc-5b74-48d4-974a-6b32218837e6",
   "metadata": {
    "id": "f58b3adc-5b74-48d4-974a-6b32218837e6"
   },
   "source": [
    "# Removing DFL data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d02f57-2452-4472-abbb-dd35393a0ef3",
   "metadata": {
    "id": "88d02f57-2452-4472-abbb-dd35393a0ef3",
    "outputId": "8969ad08-a4f3-4bb1-a6e8-c70e0b3a8a91",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Adding column \"Scheme Type\" to differentiate Schemes and DFL\n",
    "data0[\"Scheme Category\"] = data0['Scheme/Doc GUID'].apply(lambda x: \"DFL\" if (x==\"SH0009SW\" or x==\"SH000AG6\" or x==\"SH000A32\" or x==\"SH000AG6\" or x==\"SH000BM6\") else \"YC\" if x==\"SH0009RR\" else 'MSMEYC' if x in ['SH000DYR','SH000DYS'] else \"EDP\" if x in ['SH000E7Q','SH000E9C','SH000EAA','SH000EAB'] else 'MSME_Sch' if x in ['DC0008R0','DC00096J'] else \"E-Gov\")\n",
    "og_DF=data0.copy()\n",
    "dfl = data0[(data0['Scheme/Doc GUID'] == 'SH0009SW') | (data0['Scheme/Doc GUID'] == 'SH000AG6') | (data0['Scheme/Doc GUID'] == 'SH000A32') | (data0['Scheme/Doc GUID']=='SH0009SW') | (data0['Scheme/Doc GUID']=='SH000AG6') | (data0['Scheme/Doc GUID']=='SH000BM6')]\n",
    "data0 = data0[(data0['Scheme/Doc GUID'] != 'SH0009SW') & (data0['Scheme/Doc GUID'] != 'SH000AG6') & (data0['Scheme/Doc GUID'] != 'SH000A32') & (data0['Scheme/Doc GUID']!='SH0009SW') & (data0['Scheme/Doc GUID']!='SH000AG6') & (data0['Scheme/Doc GUID']!='SH000BM6')]\n",
    "print(\"DFL Count={0}\\nE-Gov Count={1}\".format(len(dfl),len(data0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62bf991-440a-473b-9c7c-40b2d0dc7ce7",
   "metadata": {
    "id": "a62bf991-440a-473b-9c7c-40b2d0dc7ce7",
    "outputId": "11e83494-6ece-4a6b-9cf8-41d3766d85bd"
   },
   "outputs": [],
   "source": [
    "data0.info()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "be27e228-a74a-4925-b6b5-bd0ed4a963da",
   "metadata": {
    "id": "be27e228-a74a-4925-b6b5-bd0ed4a963da"
   },
   "source": [
    "# Replace null values for all columns\n",
    "for i in data0.columns:\n",
    "    if data0[i].dtype == 'float64':\n",
    "        data0[i].fillna(0.0, inplace=True)\n",
    "    elif data0[i].dtype == 'int64':\n",
    "        data0[i].fillna(0, inplace=True)\n",
    "    else:\n",
    "        data0[i].fillna('a', inplace=True)\n",
    "\n",
    "# Note : This step has been skipped because Dtype of the columns also changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd755cd-950a-4ed3-aa35-604942da7883",
   "metadata": {
    "id": "9dd755cd-950a-4ed3-aa35-604942da7883"
   },
   "source": [
    "# Duplicate Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84edf59d-1a9b-44a3-8bd9-f6a7f57d1208",
   "metadata": {
    "id": "84edf59d-1a9b-44a3-8bd9-f6a7f57d1208",
    "outputId": "24087bb5-9a40-4d1e-f1d4-17c7d320cee8"
   },
   "outputs": [],
   "source": [
    "# Filling missing values of \"Parent Scheme\" from \"Scheme/Doc\" column\n",
    "for i in data0[data0['Parent Scheme'].isna()].index:\n",
    "    data0['Parent Scheme'][i] = data0['Scheme/Doc'][i]\n",
    "\n",
    "# Creating column with name 'duplicate' by concatenation\n",
    "data0['Mobile'] = data0['Mobile'].astype('str')\n",
    "data0['duplicate'] = data0['Scheme/Doc'] + data0['Citizen Name'] + data0['Mobile'] # Concatenation using scheme name\n",
    "dfl['duplicate'] = dfl['Scheme/Doc'] + dfl['Citizen Name'] + dfl['Mobile']\n",
    "data0['duplicate'] = data0['duplicate'].apply(lambda x: x.lower()) # Converting duplicate column in lower case because python considers ASCII values of each character while checking for duplicates.\n",
    "dfl['duplicate'] = dfl['duplicate'].apply(lambda x: x.lower())\n",
    "data0['parent_duplicate'] = data0['Parent Scheme'] + data0['Citizen Name'] + data0['Mobile'] # Concatenation using scheme name\n",
    "data0['parent_duplicate'] = data0['parent_duplicate'].apply(lambda x: x.lower())\n",
    "data0.Mobile = data0.Mobile.apply(lambda x : int(x.split('.')[0])) # Converting \"Mobile\" column to interger data type\n",
    "#dfl.Mobile = dfl.Mobile.apply(lambda x : int(x.split('.')[0])) # Converting \"Mobile\" column to interger data type\n",
    "\n",
    "data0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402f6ceb-2aa6-4913-a7be-01338c5b474a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e15d03-7854-43cb-9bc6-f6ae9d43942f",
   "metadata": {
    "id": "b0e15d03-7854-43cb-9bc6-f6ae9d43942f"
   },
   "source": [
    "# Duplicate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3f27b1-dbdd-41da-9647-88a044b9f98d",
   "metadata": {
    "id": "fa3f27b1-dbdd-41da-9647-88a044b9f98d",
    "outputId": "b7f7edc9-9610-40f0-a597-985f4111e440",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking number of all duplicate records\n",
    "duplicateData = data0[data0.duplicated(['duplicate'],keep=False)].sort_values('duplicate')\n",
    "parentDuplicateData = data0[data0.duplicated(['parent_duplicate'],keep=False)].sort_values('parent_duplicate')\n",
    "dflDuplicates = dfl[dfl.duplicated(['duplicate'],keep=False)].sort_values('duplicate')\n",
    "duplicateData = pd.concat([duplicateData,dflDuplicates], ignore_index=True)\n",
    "duplicateData.reset_index(inplace=True, drop = True)\n",
    "\n",
    "try:\n",
    "    duplicateData.drop(index=duplicateData.index[-1], inplace=True)\n",
    "except IndexError:\n",
    "    print(duplicateData.shape)\n",
    "\n",
    "duplicateData.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2d6cfe-660a-4c1f-948b-b3699c2f9dc3",
   "metadata": {
    "id": "4f2d6cfe-660a-4c1f-948b-b3699c2f9dc3"
   },
   "source": [
    "# Unique Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0816c5f6-c4df-4264-8e6f-341f1d79209d",
   "metadata": {
    "id": "0816c5f6-c4df-4264-8e6f-341f1d79209d",
    "outputId": "80c786a4-f9c6-4587-d1a6-0ade39a1734c"
   },
   "outputs": [],
   "source": [
    "# Keeping uniques excluding duplicates\n",
    "unique_data = data0.drop(index = data0[data0.duplicated(['duplicate'], keep='last')].index)\n",
    "dfl_uniques = dfl.drop(index = dfl[dfl.duplicated(['duplicate'], keep = 'last')].index)\n",
    "#unique_data = data0.drop(index = data0[data0.duplicated(['parent_duplicate'], keep='last')].index)\n",
    "unique_data.reset_index(inplace=True, drop = True)\n",
    "dfl_uniques.reset_index(inplace=True, drop = True)\n",
    "\n",
    "try:\n",
    "    unique_data.drop(index=unique_data.index[-1], inplace=True)\n",
    "except IndexError:\n",
    "    print(unique_data.shape)\n",
    "\n",
    "try:\n",
    "    dfl_uniques.drop(index=dfl_uniques.index[-1], inplace=True)\n",
    "except IndexError:\n",
    "    print(dfl_uniques.shape)\n",
    "\n",
    "unique_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea09448-10f0-4406-a69e-b7fdddc04280",
   "metadata": {
    "id": "dea09448-10f0-4406-a69e-b7fdddc04280"
   },
   "source": [
    "# For NASSCOM FOUNDATION 1.0 Only\n",
    "\n",
    "#### Remove non listed district data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5779628-786b-4fa0-99a7-221d81c84f15",
   "metadata": {
    "id": "c5779628-786b-4fa0-99a7-221d81c84f15",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 'Nasscom1.0_All' in fn:\n",
    "    NoListDict = pd.DataFrame(unique_data['District'].value_counts(normalize=True).reset_index())\n",
    "    for dist in list(NoListDict[NoListDict['proportion']<0.001].District):\n",
    "        unique_data = unique_data[unique_data.District!=dist]\n",
    "    unique_data.District.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6d952e-24db-4915-b647-7c9dcf309995",
   "metadata": {
    "id": "1f6d952e-24db-4915-b647-7c9dcf309995"
   },
   "source": [
    "# Adding No. of cases column\n",
    "\n",
    "#### This is to find out the no. of cases per unique citizen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9413ad9b-7438-444c-ace9-a543119a41d2",
   "metadata": {
    "id": "9413ad9b-7438-444c-ace9-a543119a41d2",
    "outputId": "68815f26-5b38-46f5-ced2-89c08beb11c3"
   },
   "outputs": [],
   "source": [
    "# Creating table citizen to scheme ratio.\n",
    "cit_sch_ratio = pd.DataFrame(unique_data['Citizen GUID'].value_counts())\n",
    "cit_sch_ratio.rename(columns={'count':'No of cases'}, inplace=True)\n",
    "cit_sch_ratio.reset_index(inplace=True)\n",
    "\n",
    "# Merging no of cases with main project data.\n",
    "#unique_data = unique_data.merge(cit_sch_ratio.drop_duplicates(subset=['Citizen GUID']), on = 'Citizen GUID', how = 'left')\n",
    "unique_data = unique_data.merge(cit_sch_ratio, on = 'Citizen GUID', how = 'left')\n",
    "unique_data['No of cases'].fillna(0.0, inplace=True)\n",
    "unique_data['No of cases'] = unique_data['No of cases'].astype('int64')\n",
    "unique_data.reset_index(inplace=True, drop = True)\n",
    "unique_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bce017-3827-4a73-9c44-196353a59124",
   "metadata": {
    "id": "08bce017-3827-4a73-9c44-196353a59124",
    "outputId": "deef9c27-6ecb-440c-b0cd-944fbe5c154a"
   },
   "outputs": [],
   "source": [
    "sns.boxplot(data=unique_data,x='Age')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd86569-d2e5-4b3f-b947-306696178159",
   "metadata": {
    "id": "fbd86569-d2e5-4b3f-b947-306696178159"
   },
   "source": [
    "# Project wise O / S / BR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2f9750-5d7b-48ea-b80e-50d1d545a5e8",
   "metadata": {
    "id": "6f2f9750-5d7b-48ea-b80e-50d1d545a5e8",
    "outputId": "ec78d954-e62c-4e55-f93a-448f431921f7"
   },
   "outputs": [],
   "source": [
    "def func(pct, allvalues):\n",
    "    absolute = int(pct / 100.*np.sum(allvalues))\n",
    "    return \"{:.1f}%\\n{:d}\".format(pct, absolute)\n",
    "\n",
    "# Changing status values to O_S_BR\n",
    "projectwise_O_S_BR = pd.DataFrame(unique_data.Status.value_counts())\n",
    "projectwise_O_S_BR.rename(columns={'count':'Total Application'}, inplace=True)\n",
    "projectwise_O_S_BR.reset_index(inplace=True)\n",
    "projectwise_O_S_BR.loc[len(projectwise_O_S_BR.index)] = ['Grand Total', projectwise_O_S_BR['Total Application'].sum()]\n",
    "\n",
    "try:\n",
    "    projectwise_O_S_BR = projectwise_O_S_BR.iloc[[0,1]]\n",
    "except Exception:\n",
    "    projectwise_O_S_BR = projectwise_O_S_BR.iloc[[1,0,2]]\n",
    "    plt.pie(projectwise_O_S_BR['Total Application'][0:2], labels=projectwise_O_S_BR['Status'][0:2], rotatelabels=True, autopct=lambda pct: func(pct, projectwise_O_S_BR['Total Application'][0:2]), explode=[0.01,0.05], textprops={'fontsize' : 'small'}, labeldistance = 0.8)\n",
    "except Exception:\n",
    "    projectwise_O_S_BR = projectwise_O_S_BR.iloc[[1,2,0,3]]\n",
    "    plt.pie(projectwise_O_S_BR['Total Application'][0:3], labels=projectwise_O_S_BR['Status'][0:3], rotatelabels=False, autopct=lambda pct: func(pct, projectwise_O_S_BR['Total Application'][0:3]), explode=[0.01,0.05,0.09], textprops={'fontsize' : 'small'}, labeldistance = 0.8)\n",
    "else:\n",
    "    projectwise_O_S_BR\n",
    "\n",
    "projectwise_O_S_BR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e4be43-d6e8-4cea-acd5-b262c594a326",
   "metadata": {
    "id": "31e4be43-d6e8-4cea-acd5-b262c594a326"
   },
   "source": [
    "# Districtwise Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34c6a06-c36d-4526-8803-efd8cc2f4d7f",
   "metadata": {
    "id": "c34c6a06-c36d-4526-8803-efd8cc2f4d7f",
    "outputId": "08479e70-23cc-4ea5-ff7e-e5fef0e0d45c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "districtWise = pd.pivot_table(data=unique_data, index='District', columns='Status', values='Case Id', aggfunc='count', fill_value=0).reset_index()\n",
    "\n",
    "try:\n",
    "    if 'Benefit Received' not in list(unique_data.Status.value_counts().index):\n",
    "        districtWise.loc[len(districtWise)] = ['Grand Total',districtWise['Open'].sum(),districtWise['Submitted'].sum()]\n",
    "        districtWise['Total'] = districtWise['Open']+districtWise['Submitted']\n",
    "        districtWise = districtWise[['District', 'Open', 'Submitted', 'Total']]\n",
    "\n",
    "    elif 'Submitted' not in list(unique_data.Status.value_counts().index):\n",
    "        districtWise.loc[len(districtWise)] = ['Grand Total',districtWise['Benefit Received'].sum(),districtWise['Open'].sum()]\n",
    "        districtWise['Total'] = districtWise['Benefit Received']+districtWise['Open']\n",
    "        districtWise = districtWise[['District', 'Open', 'Benefit Received', 'Total']]\n",
    "\n",
    "    elif 'Open' not in list(unique_data.Status.value_counts().index):\n",
    "        districtWise.loc[len(districtWise)] = ['Grand Total',districtWise['Benefit Received'].sum(),districtWise['Submitted'].sum()]\n",
    "        districtWise['Total'] = districtWise['Benefit Received']+districtWise['Submitted']\n",
    "        districtWise = districtWise[['District', 'Submitted', 'Benefit Received', 'Total']]\n",
    "    else:\n",
    "        districtWise.loc[len(districtWise)] = ['Grand Total',districtWise['Benefit Received'].sum(),districtWise['Open'].sum(),districtWise['Submitted'].sum()]\n",
    "        districtWise['Total'] = districtWise['Benefit Received']+districtWise['Submitted']+districtWise['Open']\n",
    "        districtWise = districtWise[['District', 'Open', 'Submitted', 'Benefit Received', 'Total']]\n",
    "except:\n",
    "    pass\n",
    "\n",
    "districtWise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9c7076-00cf-43bf-b544-c60985cfa170",
   "metadata": {
    "id": "db9c7076-00cf-43bf-b544-c60985cfa170"
   },
   "source": [
    "# Orgwise Scheme Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67899d1f-41b9-4a97-b69e-db469e393fa8",
   "metadata": {
    "id": "67899d1f-41b9-4a97-b69e-db469e393fa8",
    "outputId": "714d451e-9c24-421a-a37e-54e6788d265d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orgSchDiver = unique_data.pivot_table(index=['State','Scheme/Doc'], values='Case Id', aggfunc='count') # Pivoting unique data with \"Case Organization\" & \"Scheme/Doc\" rows and count of column \"Case Id\"\n",
    "orgSchDiver.reset_index(inplace=True)\n",
    "Orgwise_Scheme_Diversity = pd.DataFrame(orgSchDiver['State'].value_counts()).reset_index().rename(columns={'count':'Count of unique schemes'}).sort_values('State') # Converting pivot table to pandas data frame\n",
    "Orgwise_Scheme_Diversity['Total Applications'] = orgSchDiver.groupby(by = 'State')['Case Id'].sum().values # Adding \"Total no. of cases\" column\n",
    "\n",
    "'''# 18-35 - DFL Advance/Basic BR\n",
    "digital_Adult = unique_data[(unique_data['Age'] >= 18) & (unique_data['Age'] <= 35) & (unique_data['Status'] == 'Benefit Received')]\n",
    "digital_Adult = pd.pivot_table(data=digital_Adult, index = 'Scheme/Doc', values = 'Case Id', aggfunc='count').reset_index()\n",
    "try:\n",
    "    Orgwise_Scheme_Diversity['18-35 - DFL Advance/Basic BR'] = digital_Adult[(digital_Adult['Scheme/Doc'] == 'Digital productivity Service_ Basic') |\n",
    "                                                                         (digital_Adult['Scheme/Doc'] == 'Digital Productivity Services_Advanced')].sum()[1]\n",
    "except IndexError:\n",
    "    Orgwise_Scheme_Diversity['18-35 - DFL Advance/Basic BR'] = digital_Adult[(digital_Adult['Scheme/Doc'] == 'Digital productivity Service_ Basic') |\n",
    "                                                                         (digital_Adult['Scheme/Doc'] == 'Digital Productivity Services_Advanced')].sum()[0]'''\n",
    "\n",
    "# Shcemes with more than 10% application\n",
    "orgDict = {} # Declaring a empty dictionary to store Shcemes with more than 10% application\n",
    "for org in Orgwise_Scheme_Diversity['State']:\n",
    "    maxApp = pd.DataFrame(orgSchDiver[orgSchDiver['State'] == org].groupby('Scheme/Doc')['Case Id'].sum()\n",
    "                          >\n",
    "                          int(orgSchDiver[orgSchDiver['State'] == org]['Case Id'].sum()/10)) # Getting list of more then 10% application\n",
    "    orgDict[org] = list(maxApp[maxApp['Case Id'] == True].index)\n",
    "Orgwise_Scheme_Diversity['Shcemes with more than 10% application'] = orgDict.values() # Adding \"Shcemes with more than 10% application\" column\n",
    "\n",
    "Orgwise_Scheme_Diversity.rename(columns={\"Count of unique schemes\":\"Total unique schemes\"}, inplace=True)\n",
    "Orgwise_Scheme_Diversity[['State', 'Total unique schemes', 'Total Applications']].set_index('State').plot(kind='bar', title='Statewise achievement', subplots=True)\n",
    "Orgwise_Scheme_Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8465dd10-c9d1-4e83-95f5-d0c76dc9d5e8",
   "metadata": {
    "id": "8465dd10-c9d1-4e83-95f5-d0c76dc9d5e8",
    "outputId": "694ad387-0f89-46ff-b038-2907ebb2f59d"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2)\n",
    "\n",
    "bar_container1 = ax[0].bar(Orgwise_Scheme_Diversity.State, Orgwise_Scheme_Diversity['Total unique schemes'], color = '#FF796C')\n",
    "bar_container2 = ax[1].bar(Orgwise_Scheme_Diversity.State, Orgwise_Scheme_Diversity['Total Applications'], color = '#029386')\n",
    "\n",
    "ax[0].tick_params(axis = 'y', labelsize = 7.0)\n",
    "ax[1].tick_params(axis = 'y', labelsize = 7.0)\n",
    "\n",
    "ax[0].bar_label(bar_container1, fmt='{:,.0f}', fontsize=6.0)\n",
    "ax[1].bar_label(bar_container2, fmt='{:,.0f}', fontsize=6.0)\n",
    "\n",
    "ax[0].set_xticklabels(Orgwise_Scheme_Diversity.State, fontsize=5.0, rotation=45)\n",
    "ax[1].set_xticklabels(Orgwise_Scheme_Diversity.State, fontsize=5.0, rotation=45)\n",
    "\n",
    "if len(Orgwise_Scheme_Diversity.State)>1:\n",
    "    ax[0].set(title = 'Total unique schemes')\n",
    "    ax[1].set(title = 'Total Applications')\n",
    "else:\n",
    "    ax[0].set(title = 'Total unique schemes', ylim = (math.floor(Orgwise_Scheme_Diversity['Total unique schemes'].sum())-1, math.ceil(Orgwise_Scheme_Diversity['Total unique schemes'].sum())+2))\n",
    "    ax[1].set(title = 'Total Applications', ylim = (math.floor(Orgwise_Scheme_Diversity['Total Applications'].sum()), math.ceil(Orgwise_Scheme_Diversity['Total Applications'].sum())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d5d3a0-d143-41c0-9f90-08512799c0c9",
   "metadata": {
    "id": "61d5d3a0-d143-41c0-9f90-08512799c0c9"
   },
   "source": [
    "# Citizen Scheme Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac27dde4-5a01-4d9a-ab64-126701330aac",
   "metadata": {
    "id": "ac27dde4-5a01-4d9a-ab64-126701330aac",
    "outputId": "19866bf9-6847-449c-8d85-125a99760590"
   },
   "outputs": [],
   "source": [
    "# Scheme variety wise application ratio\n",
    "cit_sch_ratio = {'Scheme Variety':[],\n",
    "                 'Total Citizens':[],\n",
    "                 'Total Cases':[]}\n",
    "\n",
    "no_of_cases = list(set(unique_data['No of cases'].value_counts().index))\n",
    "no_of_case = []\n",
    "no_of_cit = []\n",
    "for n in no_of_cases:\n",
    "    if n == 0:\n",
    "        unique_data.drop(index=(unique_data[unique_data['No of cases'] == n].index), inplace=True)\n",
    "\n",
    "    elif n>0 and n<=3:\n",
    "        cit_sch_ratio['Scheme Variety'].append('With {0} scheme'.format(n))\n",
    "        cit_sch_ratio['Total Citizens'].append(len(unique_data[unique_data['No of cases'] == n]['Citizen GUID'].value_counts()))\n",
    "        cit_sch_ratio['Total Cases'].append(len(unique_data[unique_data['No of cases'] == n]))\n",
    "\n",
    "    elif n>3:\n",
    "        if 'More than 3 schemes' in cit_sch_ratio['Scheme Variety']:\n",
    "            no_of_case.append(len(unique_data[unique_data['No of cases'] == n]))\n",
    "            no_of_cit.append(len(unique_data[unique_data['No of cases'] == n]['Citizen GUID'].value_counts()))\n",
    "\n",
    "        else:\n",
    "            cit_sch_ratio['Scheme Variety'].append('More than 3 schemes')\n",
    "            no_of_case.append(len(unique_data[unique_data['No of cases'] == n]))\n",
    "            no_of_cit.append(len(unique_data[unique_data['No of cases'] == n]['Citizen GUID'].value_counts()))\n",
    "\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Adding sum of cases and citizens against \"More than 3 schemes\"\n",
    "if n>3:\n",
    "    cit_sch_ratio['Total Cases'].append(sum(no_of_case))\n",
    "    cit_sch_ratio['Total Citizens'].append(sum(no_of_cit))\n",
    "\n",
    "# Grand Total\n",
    "cit_sch_ratio['Scheme Variety'].append('Grand Total')\n",
    "cit_sch_ratio['Total Citizens'].append(sum(cit_sch_ratio['Total Citizens']))\n",
    "cit_sch_ratio['Total Cases'].append(sum(cit_sch_ratio['Total Cases']))\n",
    "\n",
    "# More than 7 schemes\n",
    "if len(unique_data[unique_data['No of cases'] >= 7]) > 0:\n",
    "    cit_sch_ratio['Scheme Variety'].append('More than 7 schemes')\n",
    "    cit_sch_ratio['Total Cases'].append(len(unique_data[unique_data['No of cases'] >= 7]))\n",
    "    cit_sch_ratio['Total Citizens'].append(len(unique_data[unique_data['No of cases'] >= 7]['Citizen GUID'].value_counts()))\n",
    "\n",
    "cit_sch_ratio = pd.DataFrame(cit_sch_ratio)\n",
    "cit_sch_ratio[~(cit_sch_ratio['Scheme Variety']=='Grand Total')].set_index('Scheme Variety').plot.bar(title='Citizen Scheme Ratio', label = 1, subplots=True, logy=False, log=False, rot=True, fontsize=6, mouseover=True)\n",
    "cit_sch_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e64d2c3-21b3-4a96-b071-1dcc24765164",
   "metadata": {
    "id": "9e64d2c3-21b3-4a96-b071-1dcc24765164",
    "outputId": "b61a2eef-d6b5-4005-a9b7-2b5735d80ada"
   },
   "outputs": [],
   "source": [
    "x = cit_sch_ratio[~(cit_sch_ratio['Scheme Variety']=='Grand Total')]\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "\n",
    "ax.plot(x['Scheme Variety'], x[['Total Citizens','Total Cases']], marker = 'H', linestyle = '-.', animated=True)\n",
    "ax.set_xticklabels(labels=x['Scheme Variety'], fontdict={'fontsize':7.0})\n",
    "ax.legend(['Total Citizens','Total Cases'])\n",
    "ax.set(title = 'Citizen Scheme Ratio')\n",
    "for i, (xi, yi, zi) in enumerate(zip(x['Scheme Variety'], x['Total Citizens'], x['Total Cases'])):\n",
    "    ax.annotate(f'Citizens:\\n<{yi}>', (xi, yi), textcoords=\"offset points\", xytext=(0, 10), ha='right', va='top', fontsize = 6.0, fontname='fantasy', color = 'red')\n",
    "    ax.annotate(f'Cases:\\n<{zi}>', (xi, zi), textcoords=\"offset points\", xytext=(0, 10), ha='left', va='bottom', fontsize = 6.0, fontname='fantasy', color = 'red')\n",
    "\n",
    "#ax.plot(x['Scheme Variety'], x['Total Cases'], marker = '*', linestyle = '-')\n",
    "#for i, (xi, yi) in enumerate(zip(x['Scheme Variety'], x['Total Cases'])):\n",
    "#    ax.annotate(f'{yi}', (xi, yi), textcoords=\"offset pints\", xytext=(0, 10), ha='center', annotation_clip = True)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dc8458-79a6-4188-b633-179196393573",
   "metadata": {
    "id": "48dc8458-79a6-4188-b633-179196393573"
   },
   "source": [
    "# Scheme Doc Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b6f501-2fb8-4e86-9687-ca47ea63e383",
   "metadata": {
    "id": "42b6f501-2fb8-4e86-9687-ca47ea63e383",
    "outputId": "d48de4a6-f40f-4a75-ab25-0ccfb225b440"
   },
   "outputs": [],
   "source": [
    "# Scheme type Total application\n",
    "sch_doc_application = pd.DataFrame(unique_data.groupby(by = 'Scheme type')['Scheme/Doc'].count()).reset_index()\n",
    "sch_doc_application.rename(columns={'Scheme/Doc' : 'Total Applications'}, inplace=True)\n",
    "\n",
    "# Scheme type Total Benefit value\n",
    "sch_doc_application['Total BV'] = list(unique_data.groupby(by ='Scheme type')['Benefit Value'].sum())\n",
    "\n",
    "# Scheme type Total unique schemes\n",
    "sch_doc_schemes = pd.DataFrame(unique_data.groupby(by = 'Scheme type')['Scheme/Doc'].value_counts()).reset_index().drop(columns='count')\n",
    "sch_doc_schemes = pd.DataFrame(sch_doc_schemes.groupby(by='Scheme type')['Scheme/Doc'].count()).reset_index()\n",
    "sch_doc_schemes.rename(columns={'Scheme/Doc' : 'Unique Schemes'}, inplace=True)\n",
    "\n",
    "# Merging both tables\n",
    "sch_doc_ratio = sch_doc_schemes.merge(sch_doc_application, on = 'Scheme type', how = 'left')\n",
    "sch_doc_ratio[['Scheme type','Unique Schemes']].set_index('Scheme type').plot(kind='barh', title='Scheme Type Diversity', color='#F5A3C7', legend=False, xlabel=\"Unique Schemes\")\n",
    "sch_doc_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7f7f7f-b74e-4998-a558-34ea17010665",
   "metadata": {
    "id": "2c7f7f7f-b74e-4998-a558-34ea17010665"
   },
   "source": [
    "# Top Bottom HD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2a0dbc-79ae-4ee2-aece-8a3d609ee82a",
   "metadata": {
    "id": "ac2a0dbc-79ae-4ee2-aece-8a3d609ee82a",
    "outputId": "c2ffab1e-dd86-46a5-94d9-1433b577ad3d"
   },
   "outputs": [],
   "source": [
    "step = unique_data.copy() # Copying data to another variable to make some changes.\n",
    "step['HD ID'] = step['HD ID'].fillna('a') # Replacing missing values with simple character 'a'\n",
    "step['HD ID'] = step['HD ID'].astype('str') # Changing HD ID column data type to string so that all values can be converted to lower case.\n",
    "step['HD ID'] = step['HD ID'].apply(lambda x: x.lower()) # Changing values to lower case.\n",
    "if \"Benefit Received\" in step.Status:\n",
    "    step1 = pd.pivot_table(data = step[step.Status == \"Benefit Received\"], index = ['HD ID', 'HD Name','Scheme/Doc GUID'], values = 'Case Id', aggfunc = 'count') # Pivoting to get unique HD ID/ HD ID/ Scheme Name\n",
    "else:\n",
    "    step1 = pd.pivot_table(data = step, index = ['HD ID', 'HD Name','Scheme/Doc GUID'], values = 'Case Id', aggfunc = 'count') # Pivoting to get unique HD ID/ HD ID/ Scheme Name\n",
    "step1 = pd.DataFrame(step1.drop(columns='Case Id').reset_index()) # Delete unwanted column 'Case Id'\n",
    "step1 = pd.DataFrame(pd.pivot_table(data=step, index=['HD ID','HD Name'], values='Scheme/Doc GUID', aggfunc='count').reset_index()).rename(columns={'Scheme/Doc GUID' : 'Total unique schemes'}) # Pivoting to get unique HD ID/ HD ID and unique count of schemes.\n",
    "step2 = pd.DataFrame(step[step.Status == \"Benefit Received\"].groupby(by = 'HD ID')['Case Id'].count()).reset_index().rename(columns={'Case Id' : 'Total Applications'})\n",
    "step3 = step.groupby('HD ID')['Benefit Value'].sum().reset_index()\n",
    "\n",
    "if \"Benefit Received\" in step.Status:\n",
    "    step4 = pd.pivot_table(data=step[step.Status == \"Benefit Received\"], index = ['HD ID'], values='HD_Payment', aggfunc='sum').reset_index().rename(columns = {'HD_Payment' : 'Total Payment'}) # Summing up HD payment\n",
    "else:\n",
    "    step4 = pd.pivot_table(data=step, index = ['HD ID'], values='HD_Payment', aggfunc='sum').reset_index().rename(columns = {'HD_Payment' : 'Total Payment'}) # Summing up HD payment\n",
    "\n",
    "step5 = pd.pivot_table(rejectedDF, index=['HD ID'], values='HD_Payment', aggfunc='sum')\n",
    "top_bottom_hd = step1.merge(step2, on = 'HD ID', how='left').merge(step3, on = 'HD ID', how='left').merge(step4, on = 'HD ID', how='left').merge(step5, on = 'HD ID', how='left')\n",
    "\n",
    "if rejectedDF.shape[0] != 0:\n",
    "    top_bottom_hd.HD_Payment = top_bottom_hd.HD_Payment.fillna(0)\n",
    "    top_bottom_hd.rename(columns={'Benefit Value':'Benefit Value Delivered', 'HD_Payment' : 'Rejected Payment'}, inplace=True)\n",
    "    top_bottom_hd.loc[len(top_bottom_hd)] = ['Grand Total', '', top_bottom_hd['Total unique schemes'].sum(),\n",
    "                                             top_bottom_hd['Total Applications'].sum(), top_bottom_hd['Benefit Value Delivered'].sum(),\n",
    "                                             top_bottom_hd['Total Payment'].sum(), top_bottom_hd['Rejected Payment'].sum()]\n",
    "else:\n",
    "    top_bottom_hd.rename(columns={'Benefit Value':'Benefit Value Delivered'}, inplace=True)\n",
    "    top_bottom_hd.loc[len(top_bottom_hd)] = ['Grand Total', '', top_bottom_hd['Total unique schemes'].sum(),\n",
    "                                             top_bottom_hd['Total Applications'].sum(), top_bottom_hd['Benefit Value Delivered'].sum(),\n",
    "                                             top_bottom_hd['Total Payment'].sum()]\n",
    "top_bottom_hd.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337065c7-61d3-437b-8a87-d4d423550318",
   "metadata": {
    "id": "337065c7-61d3-437b-8a87-d4d423550318"
   },
   "source": [
    "# Scheme Categorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c976835-7811-4534-abdb-28d4699ed83a",
   "metadata": {
    "id": "9c976835-7811-4534-abdb-28d4699ed83a",
    "outputId": "fcbd2537-bbc9-448a-e621-98212122d78b"
   },
   "outputs": [],
   "source": [
    "Scheme_Categorisation = pd.DataFrame(pd.pivot_table(data = unique_data, index=['Scheme type', 'Scheme/Doc', 'Benefit Value'], values='Case Id', aggfunc= 'count')).reset_index()\n",
    "Scheme_Categorisation['Total BV Delivered'] = Scheme_Categorisation['Benefit Value']*Scheme_Categorisation['Case Id']\n",
    "Scheme_Categorisation.rename(columns={'Case Id':'Total Applications'}, inplace=True)\n",
    "Scheme_Categorisation.loc[len(Scheme_Categorisation)] = ['Grand Total', '', '', Scheme_Categorisation['Total Applications'].sum(), Scheme_Categorisation['Total BV Delivered'].sum()]\n",
    "Scheme_Categorisation.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3636b230-d881-4fa1-8ce5-f184ab3cf4a7",
   "metadata": {
    "id": "3636b230-d881-4fa1-8ce5-f184ab3cf4a7"
   },
   "source": [
    "# Schemewise O/S/BR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38085940-9235-4cd9-9c9a-b6d2285fbf15",
   "metadata": {
    "id": "38085940-9235-4cd9-9c9a-b6d2285fbf15",
    "outputId": "d5bdaa0c-2907-45c0-b7d8-7096a8967a2d"
   },
   "outputs": [],
   "source": [
    "Sch_O_S_BR = pd.pivot_table(data = unique_data, index='Scheme/Doc', columns='Status', values='Case Id', aggfunc='count', fill_value=0)\n",
    "Sch_O_S_BR = pd.DataFrame(Sch_O_S_BR).reset_index()\n",
    "if len(unique_data.Status.value_counts().index) > 1:\n",
    "    if 'Benefit Received' not in Sch_O_S_BR.columns:\n",
    "        if 'Submitted' not in Sch_O_S_BR.columns:\n",
    "            Sch_O_S_BR = Sch_O_S_BR[['Scheme/Doc', 'Open']] # Open' & 'Submitted\n",
    "            Sch_O_S_BR['Total'] = Sch_O_S_BR['Open'].sum(axis=1)\n",
    "            Sch_O_S_BR.loc[len(Sch_O_S_BR)] = ['Grand Total', Sch_O_S_BR.Open.sum(), Sch_O_S_BR.Total.sum()]\n",
    "        elif 'Open' not in Sch_O_S_BR.columns:\n",
    "            Sch_O_S_BR = Sch_O_S_BR[['Scheme/Doc', 'Submitted']] # Open' & 'Submitted\n",
    "            Sch_O_S_BR['Total'] = Sch_O_S_BR['Submitted'].sum(axis=1)\n",
    "            Sch_O_S_BR.loc[len(Sch_O_S_BR)] = ['Grand Total', Sch_O_S_BR.Submitted.sum(), Sch_O_S_BR.Total.sum()]\n",
    "        else:\n",
    "            Sch_O_S_BR = Sch_O_S_BR[['Scheme/Doc', 'Open', 'Submitted']]\n",
    "            Sch_O_S_BR['Total'] = Sch_O_S_BR[['Open', 'Submitted']].sum(axis=1)\n",
    "            Sch_O_S_BR.loc[len(Sch_O_S_BR)] = ['Grand Total', Sch_O_S_BR.Open.sum(), Sch_O_S_BR.Submitted.sum(), Sch_O_S_BR.Total.sum()]\n",
    "    elif 'Submitted' not in Sch_O_S_BR.columns:\n",
    "        if 'Benefit Received' not in Sch_O_S_BR.columns:\n",
    "            Sch_O_S_BR = Sch_O_S_BR[['Scheme/Doc', 'Open']] # Open' & 'Submitted\n",
    "            Sch_O_S_BR['Total'] = Sch_O_S_BR['Open'].sum(axis=1)\n",
    "            Sch_O_S_BR.loc[len(Sch_O_S_BR)] = ['Grand Total', Sch_O_S_BR.Open.sum(), Sch_O_S_BR.Total.sum()]\n",
    "        elif 'Open' not in Sch_O_S_BR.columns:\n",
    "            Sch_O_S_BR = Sch_O_S_BR[['Scheme/Doc', 'Benefit Received']] # Open' & 'Submitted\n",
    "            Sch_O_S_BR['Total'] = Sch_O_S_BR['Benefit Received'].sum(axis=1)\n",
    "            Sch_O_S_BR.loc[len(Sch_O_S_BR)] = ['Grand Total', Sch_O_S_BR['Benefit Received'].sum(), Sch_O_S_BR.Total.sum()]\n",
    "        else:\n",
    "            Sch_O_S_BR = Sch_O_S_BR[['Scheme/Doc', 'Open', 'Benefit Received']]\n",
    "            Sch_O_S_BR['Total'] = Sch_O_S_BR[['Open', 'Benefit Received']].sum(axis=1)\n",
    "            Sch_O_S_BR.loc[len(Sch_O_S_BR)] = ['Grand Total', Sch_O_S_BR.Open.sum(), Sch_O_S_BR['Benefit Received'].sum(), Sch_O_S_BR.Total.sum()]\n",
    "    elif 'Open' not in Sch_O_S_BR.columns:\n",
    "        if 'Benefit Received' not in Sch_O_S_BR.columns:\n",
    "            Sch_O_S_BR = Sch_O_S_BR[['Scheme/Doc', 'Submitted']] # Open' & 'Submitted\n",
    "            Sch_O_S_BR['Total'] = Sch_O_S_BR['Submitted'].sum(axis=1)\n",
    "            Sch_O_S_BR.loc[len(Sch_O_S_BR)] = ['Grand Total', Sch_O_S_BR.Open.sum(), Sch_O_S_BR.Total.sum()]\n",
    "        elif 'Submitted' not in Sch_O_S_BR.columns:\n",
    "            Sch_O_S_BR = Sch_O_S_BR[['Scheme/Doc', 'Benefit Received']] # Open' & 'Submitted\n",
    "            Sch_O_S_BR['Total'] = Sch_O_S_BR['Benefit Received'].sum(axis=1)\n",
    "            Sch_O_S_BR.loc[len(Sch_O_S_BR)] = ['Grand Total', Sch_O_S_BR['Benefit Received'].sum(), Sch_O_S_BR.Total.sum()]\n",
    "        else:\n",
    "            Sch_O_S_BR = Sch_O_S_BR[['Scheme/Doc', 'Submitted', 'Benefit Received']]\n",
    "            Sch_O_S_BR['Total'] = Sch_O_S_BR[['Submitted', 'Benefit Received']].sum(axis=1)\n",
    "            Sch_O_S_BR.loc[len(Sch_O_S_BR)] = ['Grand Total', Sch_O_S_BR.Submitted.sum(), Sch_O_S_BR['Benefit Received'].sum(), Sch_O_S_BR.Total.sum()]\n",
    "    else:\n",
    "        Sch_O_S_BR = Sch_O_S_BR[['Scheme/Doc', 'Open', 'Submitted', 'Benefit Received']]\n",
    "        Sch_O_S_BR['Total'] = Sch_O_S_BR[['Open', 'Submitted', 'Benefit Received']].sum(axis=1)\n",
    "        Sch_O_S_BR.loc[len(Sch_O_S_BR)] = ['Grand Total', Sch_O_S_BR.Open.sum(),  Sch_O_S_BR.Submitted.sum(), Sch_O_S_BR['Benefit Received'].sum(), Sch_O_S_BR.Total.sum()]\n",
    "\n",
    "Sch_O_S_BR.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d23062-a8c3-49a4-ba4d-17da92a8a74a",
   "metadata": {
    "id": "08d23062-a8c3-49a4-ba4d-17da92a8a74a"
   },
   "source": [
    "# Gender Bifurcation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d5bbc9-dc32-4558-aeb0-42c5888f131e",
   "metadata": {
    "id": "14d5bbc9-dc32-4558-aeb0-42c5888f131e",
    "outputId": "ec21a778-7057-41a0-bbb4-2e2a7cb1e09d"
   },
   "outputs": [],
   "source": [
    "gen_Bif = pd.DataFrame(unique_data['Gender'].value_counts()).reset_index()\n",
    "gen_Bif['% Contri.'] = round((gen_Bif['count']/unique_data['Gender'].value_counts().sum())*100,2)\n",
    "gen_Bif.rename(columns={'count':'Total Applications'},inplace=True)\n",
    "gen_Bif.loc[len(gen_Bif)] = ['Total', gen_Bif['Total Applications'].sum(), '']\n",
    "fig, ax = plt.subplots(1)\n",
    "\n",
    "if len(gen_Bif['Gender']) == 4:\n",
    "    bar_container = ax.barh(data = gen_Bif[['Gender', 'Total Applications']][0:3], y = gen_Bif['Gender'][0:3], width = gen_Bif['Total Applications'][0:3], color = '#F5A3C7')\n",
    "elif len(gen_Bif['Gender']) == 3:\n",
    "    bar_container = ax.barh(data = gen_Bif[['Gender', 'Total Applications']][0:2], y = gen_Bif['Gender'][0:2], width = gen_Bif['Total Applications'][0:2], color = '#F5A3C7')\n",
    "else:\n",
    "    bar_container = ax.barh(data = gen_Bif[['Gender', 'Total Applications']][0:1], y = gen_Bif['Gender'][0:1], width = gen_Bif['Total Applications'][0:1], color = '#F5A3C7')\n",
    "\n",
    "ax.bar_label(bar_container, fmt='{:,.0f}', fontsize=8.0, fontfamily='serif', fontweight='bold')\n",
    "ax.set(title = 'Gender Bifurcation')\n",
    "\n",
    "fig.show()\n",
    "gen_Bif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14006604-4463-4847-860c-31435e35e667",
   "metadata": {
    "id": "14006604-4463-4847-860c-31435e35e667"
   },
   "source": [
    "# Centrewise Repeat Mobile numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c1870b-50f3-4a7e-b241-fe2034b4c24d",
   "metadata": {
    "id": "00c1870b-50f3-4a7e-b241-fe2034b4c24d",
    "outputId": "5c721ae9-72ce-4a53-aa30-cb283db143f5"
   },
   "outputs": [],
   "source": [
    "repeat_mobile = pd.pivot_table(data=og_DF, index=['District', 'Mobile', 'Citizen GUID'], values='Case Id', aggfunc='count').sort_values(by='Case Id', ascending=False).reset_index()\n",
    "repeat_mobile = pd.pivot_table(data=repeat_mobile, index=['District', 'Mobile'], values='Citizen GUID', aggfunc='count').sort_values(by='Citizen GUID', ascending=False).reset_index()\n",
    "repeat_mobile = repeat_mobile[repeat_mobile['Citizen GUID']>5]\n",
    "if repeat_mobile['Citizen GUID'].sum()>0:\n",
    "    repeat_mobile.loc[len(repeat_mobile)] = ['Grand Total','',repeat_mobile['Citizen GUID'].sum()]\n",
    "repeat_mobile.rename(columns={\"Citizen GUID\":\"Total Citizens\"}, inplace=True)\n",
    "repeat_mobile.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbb3a4a-9612-4804-82bc-97f03aba542c",
   "metadata": {
    "id": "ebbb3a4a-9612-4804-82bc-97f03aba542c"
   },
   "source": [
    "# Agewise Citizen Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9e2f44-3897-4dcc-bd74-1dab432572cc",
   "metadata": {
    "id": "ab9e2f44-3897-4dcc-bd74-1dab432572cc",
    "outputId": "11e06a09-3c28-43d1-afd0-9b61053316f1"
   },
   "outputs": [],
   "source": [
    "plot = sns.displot(data=unique_data, x='Gender', y='Age', kind='hist', bins=15, legend=True,  color='#F5A3C7', aspect=2)\n",
    "plot.set(ylabel='No. of citizens')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd65dc25-9a9d-41b0-9240-a5d58d6a0dd3",
   "metadata": {
    "id": "dd65dc25-9a9d-41b0-9240-a5d58d6a0dd3"
   },
   "source": [
    "# E-Gov and DFL Status Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f08663-6c02-4e79-9612-806cf1ce5aa5",
   "metadata": {
    "id": "32f08663-6c02-4e79-9612-806cf1ce5aa5",
    "outputId": "7fbaaad9-61b7-490d-857a-4df2becacc62",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eGov_DFL_Summary = pd.pivot_table(data=og_DF, index='District', columns=['Scheme Category','Status'], values='Case Id', aggfunc='count', fill_value=0)\n",
    "eGov_DFL_Summary['Total'] = eGov_DFL_Summary.sum(axis = 1, numeric_only = 'True').values\n",
    "eGov_DFL_Summary.sort_values(by='Total',ascending=False, inplace=True)\n",
    "eGov_DFL_Summary.loc[len(eGov_DFL_Summary)] = eGov_DFL_Summary.sum(axis = 0, numeric_only = 'True').values\n",
    "eGov_DFL_Summary.rename(index={eGov_DFL_Summary.index[-1]:'Grand Total'}, inplace=True)\n",
    "eGov_DFL_Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d03fc19-d613-4a08-9934-6a46937d78ea",
   "metadata": {
    "id": "6d03fc19-d613-4a08-9934-6a46937d78ea"
   },
   "source": [
    "# Exporting data to Excel Sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5af934-8198-4a45-9a8c-0c30bb138899",
   "metadata": {
    "id": "fb5af934-8198-4a45-9a8c-0c30bb138899",
    "outputId": "ef070719-eb05-4b27-b6e5-82f36affa5fa"
   },
   "outputs": [],
   "source": [
    "# Exporting data of unique records to Excel file.\n",
    "print('Exporting unique data to Excel!')\n",
    "with pd.ExcelWriter('C:\\\\Python\\\\export\\\\'+fn.split('_')[2]+'_'+fn.split('_')[3]+' data.xlsx') as writer:\n",
    "    unique_data.to_excel(writer, sheet_name='Schemes Data', index=False) # Exporting unique data\n",
    "    projectwise_O_S_BR.to_excel(writer, sheet_name='projectwise_O_S_BR', index=False)\n",
    "    districtWise.to_excel(writer, sheet_name='Districtwise achv', index=False)\n",
    "    if repeat_mobile.shape[0]>0:\n",
    "        repeat_mobile.to_excel(writer,sheet_name='Repeat_mobile_nos', index=False)\n",
    "    Orgwise_Scheme_Diversity.to_excel(writer, sheet_name='Orgwise_Scheme_Diversity', index=False)\n",
    "    cit_sch_ratio.to_excel(writer, sheet_name='Citizen_Scheme_Ratio', index=False)\n",
    "    sch_doc_ratio.to_excel(writer, sheet_name='Scheme_Doc_Ratio', index=False)\n",
    "    #top_bottom_hd.to_excel(writer, sheet_name='Top_Bottom_HD', index=False)\n",
    "    Scheme_Categorisation.to_excel(writer, sheet_name='Scheme_Categorisation', index=False)\n",
    "    Sch_O_S_BR.to_excel(writer, sheet_name='Schwise_O_S_BR', index=False)\n",
    "    gen_Bif.to_excel(writer, sheet_name='Gender_Bifurcation', index=False)\n",
    "    if dfl.shape[0]>0:\n",
    "        eGov_DFL_Summary.to_excel(writer, sheet_name='eGov_DFL_Status_Summary')\n",
    "        dfl.to_excel(writer, sheet_name='DFL data', index=False)\n",
    "        dfl_uniques.to_excel(writer, sheet_name='DFL unique data', index=False)\n",
    "    rejectedDF.to_excel(writer, sheet_name='Abort_Rejected', index=False) # Exporting rejected data\n",
    "    duplicateData.to_excel(writer, sheet_name='Duplicate data', index=False) # Exporting duplicate data\n",
    "    parentDuplicateData.to_excel(writer, sheet_name='Parent Sch Duplicate', index=False) # Exporting parent scheme duplicate data\n",
    "    #plot.savefig(writer, sheet_name='Age_Dist')\n",
    "    writer.close()\n",
    "\n",
    "exe_end = dt.now() # Recording execution end time\n",
    "print('{0} project data exported to Excel!\\nTotal {1} unique records exported.\\nTotal {2} duplicate records exported.\\nTotal {3} rejected records exported'.format(fn.split('_')[2]+' '+fn.split('_')[3],\n",
    "                                                                                                                                                                   unique_data.shape[0],\n",
    "                                                                                                                                                                   duplicateData.shape[0],\n",
    "                                                                                                                                                                   rejectedDF.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b779f88-a6a8-4af5-9ad5-03f1b335a366",
   "metadata": {
    "id": "1b779f88-a6a8-4af5-9ad5-03f1b335a366"
   },
   "source": [
    "# Logging the execution process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0679b882-d442-46fa-82c4-a8578933569f",
   "metadata": {
    "id": "0679b882-d442-46fa-82c4-a8578933569f",
    "outputId": "aa515a89-c614-4706-b13b-19f014e8c6b1"
   },
   "outputs": [],
   "source": [
    "print('Logging Execution process...')\n",
    "exe_end = dt.now()\n",
    "log = [init_file_size,exe_start.strftime(\"%d/%m/%Y %H:%M:%S\"),exe_end.strftime(\"%d/%m/%Y %H:%M:%S\"),int(round((exe_end-exe_start).total_seconds(),0)),fn.split('_')[2]+' '+fn.split('_')[3],unique_data.shape[0],duplicateData.shape[0],rejectedDF.shape[0]]\n",
    "\n",
    "lwb = load_workbook(r'C:\\Python\\export\\Logs Remove Duplicate for Dashboard-Manas Algo.xlsx') # Loading the workbook\n",
    "lws = lwb.worksheets[0] # Setting the worksheet\n",
    "lws.append(log) # Appending the log row\n",
    "lwb.save(r'C:\\Python\\export\\Logs Remove Duplicate for Dashboard-Manas Algo.xlsx') # Saving the logged data\n",
    "lwb.close()\n",
    "print('Logging Done!\\n',log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac0d35a-a9d1-4858-9a22-abf2e03e6124",
   "metadata": {
    "id": "6118c6c8-cb79-423b-8cf3-1c5ef02aaa42"
   },
   "source": [
    "# Projection Dashboard Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55178a82-7cbd-48a3-ab25-632fc39f08f8",
   "metadata": {
    "id": "e827137a-d755-4393-8992-f5d823aa8888",
    "outputId": "89ae085f-9e89-4d03-c0a1-3a837b0428bd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "delivery_hierarchy = pd.read_excel(r\"C:\\Python\\read\\Delivery_Hierarchy_Report.xlsx\")\n",
    "delivery_hierarchy['Employee Name'] = delivery_hierarchy['Employee Name'].apply(lambda x: (' '.join(x.strip().split())).title())\n",
    "\n",
    "\n",
    "# For HUL projection dashboard update\n",
    "if PID == 'PID/HINDU13/2023/MSME-DI/0376':\n",
    "\n",
    "    # Notifying user to give consent..\n",
    "    notification = Notify()\n",
    "    notification.title = \"Consent Required\"\n",
    "    notification.message = \"Hello Akash! Waiting for your consent.\"\n",
    "    notification.audio = \"E:/Music/Ringtone/Consent required1.wav\"\n",
    "    notification.send()\n",
    "\n",
    "    # Asking consent from user for original data export.\n",
    "    consent = input(\"Do you want to update HUL projection? (Y/N)\")\n",
    "    if consent in ('y', 'Y'):\n",
    "        print(\"Updating HUL projection dashboard...\")\n",
    "        hul_data = unique_data.copy()\n",
    "        hul_data['Week'] = hul_data.Createdon.apply(lambda x: \"W1\" if x.day <=7 else \"W2\" if x.day >= 8 and x.day <=14 else \"W3\" if x.day >= 15 and x.day <= 21 else \"W4\")\n",
    "        sch_map = {'SH000A4G' : 'Credit', 'SH000B2I' : 'Credit', 'SH000AJC' : 'Credit', 'SH0009RA' : 'Credit', 'SH000A5H' : 'Credit', 'SH0009IX' : 'Credit', 'SH000DPW' : 'Credit', 'SH0003PE':'Credit',\n",
    "                   'SH0003PK':'Credit','SH000888':'Credit','SH0008BK':'Credit','SH0008PZ':'Credit','SH000971':'Credit','SH0009RA':'Credit','SH000AP8':'Credit','SH000BG8':'Credit','SH000CM7':'Credit',\n",
    "                   'SH000D1A':'Credit','SH000DG0':'Credit','DC0008R0':'Formalization','DC0008WZ':'Formalization','DC00096J':'Formalization','DC000DG3':'Formalization','SH0009RR':'YC'}\n",
    "        hul_data['HUL Category'] = hul_data['Scheme/Doc GUID'].apply(lambda x: sch_map[x] if x in sch_map.keys() else 'Scheme')\n",
    "        hul_data.State = hul_data.State.apply(lambda x: \"Punjab\" if x==\"Haryana\" else x)\n",
    "        step1 = pd.pivot_table(data=hul_data[hul_data['Createdon']>=\"2025-1-1\"], index=['Opsco name', 'HD ID'], values='Case Id', aggfunc='count').reset_index() # Create a table to get HD list against each Opsco\n",
    "        step2 = pd.pivot_table(data=step1, index='Opsco name', values='HD ID', aggfunc='count').reset_index() # Get count of HDs against each Opsco\n",
    "        step2.rename(columns={'HD ID':'HD Count'}, inplace=True)\n",
    "        hul_data = hul_data.merge(step2, on=\"Opsco name\", how='left') # Merge the count of HDs for each opsco against main data\n",
    "        hul_data.rename(columns={'HD ID_y':'HD Count'}, inplace=True) # Rename merged column.\n",
    "        hul_data['HD Count'].fillna(0, inplace=True)\n",
    "        col_ord = ['Case Id', 'HUL Category', 'Opsco name', 'Createdon', 'Scheme/Doc', 'Scheme/Doc GUID', 'Status', 'Docket Submitted Date', 'Benefit received Date', 'HD Suspected Cases', 'State', 'District',\n",
    "                   'Citizen GUID', 'Citizen Name', 'Gender', 'Mobile', 'Age', 'Citizen Block', 'Citizen Village', 'Family GUID', 'Family Name', 'HD ID', 'HD ID', 'AMS ID', 'Parent Scheme', 'Scheme type',\n",
    "                   'Benefit Value','open_price','Docket submitted price','scheme_document_received price','HD_Payment','Scheme Category', 'duplicate','parent_duplicate', 'No of cases', 'Week', 'HD Count']\n",
    "        with pd.ExcelWriter(r\"F:\\Haqdarshak Data\\HUL\\HUL New Tracker_Mar'25.xlsx\", if_sheet_exists='replace', mode='a', engine='openpyxl') as writer:\n",
    "            hul_data[col_ord].to_excel(writer, sheet_name='Schemes Data', index=False)\n",
    "            #writer.close()\n",
    "        #with pd.ExcelWriter(r\"F:\\Haqdarshak Data\\HUL\\HUL - YC and formalisation Plan_Nov-Dec 2024.xlsx\", if_sheet_exists='replace', mode='a', engine='openpyxl') as writer:\n",
    "            #hul_data[col_ord].to_excel(writer, sheet_name='Schemes Data', index=False)\n",
    "            #file_handler.close()\n",
    "        Rahul = [\"Vinayak Shivdas Mahajan\",\"Shruti Ashvin Chaudhari\",\"Komal Deshbhratar\",\"Pooja Yogeshvar Bhangale\",\"Dipali Rajendra Mahajan\",\"Sandesh Pol\",\"Ulhas Mali\",\"Matre Puja\",\"Umesh Badgujar\",\n",
    "                 \"Dipali Bharaskar\",\"Shital Bhamare\",\"Sumedha Suryawanshi\",\"Sayali Ahirrao\",\"Sushama Bhalerao\", \"Madhuri Khairnar\",\"Shilpa Shivaji Sadgir\",\"Gopal Gadilohar\",\"Sujata Adakmol\"]\n",
    "        Muneer = ['Ankit Kumar','Tarun Dangi','Jay Prakash','Bhaskar Sinha','Priti Rani','Muneer Alam Khan','Sonam Goswami']\n",
    "        hul_data.State = hul_data[['State','Opsco name']].apply(lambda x: 'Rahul' if x[1] in Rahul else 'Muneer' if x[1] in Muneer else x[0], axis=1)\n",
    "        with pd.ExcelWriter(r\"F:\\Haqdarshak Data\\HUL\\HUL Visualization.xlsx\", if_sheet_exists='replace', mode='a', engine='openpyxl') as writer:\n",
    "            hul_data[col_ord].to_excel(writer, sheet_name='Schemes Data', index=False)\n",
    "            #writer.close()\n",
    "        col_ord = ['Case Id', 'HUL Category', 'State','Opsco name', 'Createdon', 'Scheme/Doc', 'Scheme/Doc GUID', 'Status', 'Docket Submitted Date', 'Benefit received Date', 'HD Suspected Cases', 'District',\n",
    "                   'Citizen GUID', 'Citizen Name', 'Gender', 'Mobile', 'Age', 'Citizen Block', 'Citizen Village', 'Family GUID', 'Family Name', 'HD ID', 'HD ID', 'AMS ID', 'Parent Scheme', 'Scheme type',\n",
    "                   'Benefit Value','open_price','Docket submitted price','scheme_document_received price','HD_Payment','Scheme Category', 'duplicate','parent_duplicate', 'No of cases', 'Week', 'HD Count']\n",
    "        with pd.ExcelWriter(r\"F:\\Haqdarshak Data\\HUL\\HUL State-Opscowise Count.xlsx\", if_sheet_exists='replace', mode='a', engine='openpyxl') as writer:\n",
    "            hul_data[col_ord].to_excel(writer, sheet_name='Schemes Data', index=False)\n",
    "            #writer.close()\n",
    "        print(\"HUL projection updated!\")\n",
    "    else:\n",
    "        print(\"You chose NO to update HUL Projection!\")\n",
    "\n",
    "# For Colgate projection dashboard update\n",
    "if PID == 'PID/COLGA/2024/Custom/YK/0426':\n",
    "\n",
    "    # Notifying user to give consent..\n",
    "    notification = Notify()\n",
    "    notification.title = \"Consent Required\"\n",
    "    notification.message = \"Hello Akash! Waiting for your consent.\"\n",
    "    notification.audio = \"E:/Music/Ringtone/Consent required1.wav\"\n",
    "    notification.send()\n",
    "\n",
    "    # Asking consent from user for original data export.\n",
    "    consent = input(\"Do you want to update Colgate projection? (Y/N)\")\n",
    "    if consent in ('y', 'Y'):\n",
    "        print(\"Updating Colgate projection dashboard...\")\n",
    "        colgate_data = pd.concat([unique_data, dfl], ignore_index=True)\n",
    "        with pd.ExcelWriter(r'F:\\HQ Google Drive\\.shortcut-targets-by-id\\1-5hZJ2rAADkd7-Hfk7keG3YYTSN79fXE\\Haqdarshak ulfat\\Colgate Phase 2\\Colgate State-Opscowise Count.xlsx', if_sheet_exists='replace', mode='a', engine='openpyxl') as writer:\n",
    "            colgate_data.to_excel(writer, sheet_name='Sch_DFL_data', index=False)\n",
    "            #writer.close()\n",
    "        print(\"Colgate projection updated!\")\n",
    "    else:\n",
    "        print(\"You chose NO to update Colgate Projection!\")\n",
    "\n",
    "# For Colgate Phase 3 projection dashboard update\n",
    "if PID == 'PID/COLGA/2025/DI/0532':\n",
    "\n",
    "    # Notifying user to give consent..\n",
    "    notification = Notify()\n",
    "    notification.title = \"Consent Required\"\n",
    "    notification.message = \"Hello Akash! Waiting for your consent.\"\n",
    "    notification.audio = \"E:/Music/Ringtone/Consent required1.wav\"\n",
    "    notification.send()\n",
    "\n",
    "    # Asking consent from user for original data export.\n",
    "    consent = input(\"Do you want to update Colgate Phase 3 projection? (Y/N)\")\n",
    "    if consent in ('y', 'Y'):\n",
    "        print(\"Updating Colgate Phase 3 projection dashboard...\")\n",
    "        all_unique = pd.concat([unique_data,dfl], ignore_index=True) # Merging Unique data and DFL data\n",
    "        # Creating BR status pivot table\n",
    "        br_status = pd.pivot_table(all_unique[all_unique.Status=='Benefit Received'], index=['State','Opsco name','Citizen GUID'], columns='Scheme Category', values='Case Id', aggfunc='count', fill_value=0).reset_index()\n",
    "        br_status['Grand Total'] = br_status[['DFL', 'E-Gov', 'YC']].sum(axis=1)\n",
    "        \n",
    "        # YC + 1 Sch\n",
    "        YC_1_Sch = br_status[(br_status['Grand Total']==2) & (br_status.YC==1) & (br_status['E-Gov']==1)].reset_index()\n",
    "        all_unique = all_unique.merge(YC_1_Sch[['Citizen GUID','Grand Total']], on='Citizen GUID', how='left').fillna(\"No\")\n",
    "        all_unique.rename(columns={'Grand Total':\"YC+1 Sch\"}, inplace=True)\n",
    "        all_unique['YC+1 Sch'] = all_unique['YC+1 Sch'].apply(lambda x: \"Yes\" if x!=\"No\" else x)\n",
    "        YC_1_Sch = YC_1_Sch.groupby(['State', 'Opsco name'], as_index=False)['Citizen GUID'].nunique()\n",
    "        YC_1_Sch.rename(columns={'Citizen GUID' : 'YC + 1 Sch'}, inplace=True)\n",
    "        \n",
    "        # YC + 2 Sch\n",
    "        YC_2_Sch = br_status[(br_status['Grand Total']==3) & (br_status.YC==1) & (br_status['E-Gov']>=1) & (br_status.DFL==0)].reset_index()\n",
    "        all_unique = all_unique.merge(YC_2_Sch[['Citizen GUID','Grand Total']], on='Citizen GUID', how='left').fillna(\"No\")\n",
    "        all_unique.rename(columns={'Grand Total':\"YC+2 Sch\"}, inplace=True)\n",
    "        all_unique['YC+2 Sch'] = all_unique['YC+2 Sch'].apply(lambda x: \"Yes\" if x!=\"No\" else x)\n",
    "        YC_2_Sch = YC_2_Sch.groupby(['State', 'Opsco name'], as_index=False)['Citizen GUID'].nunique()\n",
    "        YC_2_Sch.rename(columns={'Citizen GUID' : 'YC + 2 Sch'}, inplace=True)\n",
    "        \n",
    "        # Merging both YC + 1 & 2 sch to get Opscowise summary\n",
    "        opscowise_unique_cit = YC_1_Sch.merge(YC_2_Sch, on = ['State','Opsco name'], how = 'outer').fillna(0)\n",
    "        \n",
    "        # Satewise Summary\n",
    "        statewise_unique_cit = opscowise_unique_cit.groupby('State',as_index=False)[['YC + 1 Sch','YC + 2 Sch']].sum()\n",
    "\n",
    "        #Exporting data to Excel\n",
    "        with pd.ExcelWriter(r'F:\\HQ Google Drive\\.shortcut-targets-by-id\\1-5hZJ2rAADkd7-Hfk7keG3YYTSN79fXE\\Haqdarshak ulfat\\Colgate Phase 3\\Colgate P3 State-Opscowise Count.xlsx', if_sheet_exists='overlay', mode='a', engine='openpyxl') as writer:\n",
    "            all_unique.to_excel(writer, sheet_name='Sch_DFL_data', index=False)\n",
    "            delivery_hierarchy[delivery_hierarchy.PID==PID].to_excel(writer, sheet_name='Delivery Hierarchy Report', index=False)\n",
    "            statewise_unique_cit[['YC + 1 Sch','YC + 2 Sch']].to_excel(writer, sheet_name='Begining to day', index=False, startcol=12, startrow=1)\n",
    "            opscowise_unique_cit.to_excel(writer, sheet_name='Begining to day', index=False, startrow=9)\n",
    "        '''    \n",
    "        sh = gc.open('Test Colgate P3')\n",
    "        ws = sh.worksheet('Sch_DFL_data')\n",
    "        ws.clear()\n",
    "        for col in all_unique.columns:\n",
    "            if all_unique[col].dtype == '<M8[ns]':\n",
    "                all_unique[col] = all_unique[col].astype('str')\n",
    "        all_unique = all_unique.replace([np.nan, np.inf, -np.inf], None)\n",
    "        ws.update([all_unique.columns.values.tolist()]+all_unique.values.tolist(),\"A1\")'''\n",
    "        print(\"Colgate Phase 3 projection updated!\")\n",
    "    else:\n",
    "        print(\"You chose NO to update Colgate Phase 3 Projection!\")\n",
    "\n",
    "# For Hyundai projection dashboard update\n",
    "if PID == 'PID/HYUND1/2024/Custom/YK/0475':\n",
    "\n",
    "    # Notifying user to give consent..\n",
    "    notification = Notify()\n",
    "    notification.title = \"Consent Required\"\n",
    "    notification.message = \"Hello Akash! Waiting for your consent.\"\n",
    "    notification.audio = \"E:/Music/Ringtone/Consent required1.wav\"\n",
    "    notification.send()\n",
    "\n",
    "    # Asking consent from user for original data export.\n",
    "    consent = input(\"Do you want to update Hyundai projection? (Y/N)\")\n",
    "    if consent in ('y', 'Y'):\n",
    "        print(\"Updating Hyundai projection dashboard...\")\n",
    "        hyundai_data = pd.concat([unique_data, dfl], ignore_index=True)\n",
    "        hyundai_data.District = hyundai_data[['State','District']].apply(lambda x: 'Gurugram' if x[0]==\"Haryana\" or x[0]==\"Delhi\" else 'Nashik' if x[1] in ['Ahmednagar','Amravati','Beed','Hingoli','Nashik','Washim'] else 'Pune' if x[1] in ['Pune','Raigad','Solapur'] else x[1], axis=1)\n",
    "        with pd.ExcelWriter(r'F:\\HQ Google Drive\\.shortcut-targets-by-id\\1-5hZJ2rAADkd7-Hfk7keG3YYTSN79fXE\\Haqdarshak ulfat\\Hyundai Phase 1\\Hyundai State-Opscowise Count.xlsx', if_sheet_exists='replace', mode='a', engine='openpyxl') as writer:\n",
    "            hyundai_data.to_excel(writer, sheet_name='Sch_DFL_data', index=False)\n",
    "            delivery_hierarchy[delivery_hierarchy.PID==PID].to_excel(writer, sheet_name='Delivery Hierarchy Report', index=False)\n",
    "        print(\"Hyundai State-Opscowise tracker updated!\")\n",
    "        #with pd.ExcelWriter(r'F:\\Haqdarshak Data\\Hyundai\\Hyundai_ Action Plan 4th Milestone.xlsx', if_sheet_exists='replace', mode='a', engine='openpyxl') as writer:\n",
    "        #    hyundai_data.to_excel(writer, sheet_name='Sch_DFL_data', index=False)\n",
    "        with pd.ExcelWriter(r'F:\\Haqdarshak Data\\Hyundai\\Hyundai - Daily Tracker.xlsx', if_sheet_exists='replace', mode='a', engine='openpyxl') as writer:\n",
    "            hyundai_data.to_excel(writer, sheet_name='Sch_DFL_data', index=False)\n",
    "        print('Hyundai Daily tracker updated!')\n",
    "            #writer.close()\n",
    "        print('Hyundai projection updated!')\n",
    "    else:\n",
    "        print(\"You chose NO to update Hyundai Projection!\")\n",
    "\n",
    "# For Hyundai phase 2 projection dashboard update\n",
    "if PID == 'PID/HYUND1/2025/DI/YK/0530':\n",
    "\n",
    "    # Notifying user to give consent..\n",
    "    notification = Notify()\n",
    "    notification.title = \"Consent Required\"\n",
    "    notification.message = \"Hello Akash! Waiting for your consent.\"\n",
    "    notification.audio = \"E:/Music/Ringtone/Consent required1.wav\"\n",
    "    notification.send()\n",
    "\n",
    "    # Asking consent from user for original data export.\n",
    "    consent = input(\"Do you want to update Hyundai phase 2 projection? (Y/N)\")\n",
    "    if consent in ('y', 'Y'):\n",
    "        print(\"Updating Hyundai phase 2 projection dashboard...\")\n",
    "        hyundai_data = pd.concat([unique_data, dfl], ignore_index=True)\n",
    "        hyundai_data.District = hyundai_data[['State','District']].apply(lambda x: 'Gurugram' if x[0] in [\"Haryana\", \"Delhi\"] else 'Nashik' if x[1] in ['Ahmednagar','Amravati','Beed','Hingoli','Nashik','Washim'] else 'Pune' if x[1] in ['Pune','Raigad','Solapur'] else x[1], axis=1)\n",
    "        with pd.ExcelWriter(r'F:\\HQ Google Drive\\.shortcut-targets-by-id\\1-5hZJ2rAADkd7-Hfk7keG3YYTSN79fXE\\Haqdarshak ulfat\\Hyundai Phase 2\\Hyundai Phase 2 State-Opscowise Count.xlsx', if_sheet_exists='replace', mode='a', engine='openpyxl') as writer:\n",
    "            hyundai_data.to_excel(writer, sheet_name='Sch_DFL_data', index=False)\n",
    "            delivery_hierarchy[delivery_hierarchy.PID==PID].to_excel(writer, sheet_name='Delivery Hierarchy Report', index=False)\n",
    "        print(\"Hyundai phase 2 State-Opscowise tracker updated!\")\n",
    "        #with pd.ExcelWriter(r'F:\\Haqdarshak Data\\Hyundai\\Hyundai_ Action Plan 4th Milestone.xlsx', if_sheet_exists='replace', mode='a', engine='openpyxl') as writer:\n",
    "        #    hyundai_data.to_excel(writer, sheet_name='Sch_DFL_data', index=False)\n",
    "        with pd.ExcelWriter(r'F:\\Haqdarshak Data\\Hyundai\\Phase 2\\Hyundai Phase 2 - Daily Tracker.xlsx', if_sheet_exists='replace', mode='a', engine='openpyxl') as writer:\n",
    "            hyundai_data.to_excel(writer, sheet_name='Sch_DFL_data', index=False)\n",
    "        print(\"Hyundai phase 2 Daily tracker updated!\")\n",
    "            #writer.close()\n",
    "        print(\"Hyundai phase 2 projection updated!\")\n",
    "    else:\n",
    "        print(\"You chose NO to update Hyundai phase 2 Projection!\")\n",
    "\n",
    "# For LTPCT Subir Phase 2 projection dashboard update\n",
    "if PID in ['PID/LTPUB/2024/DI/0431','PID/LARSE2/2025/DI/YK/0527']:\n",
    "\n",
    "    # Notifying user to give consent..\n",
    "    notification = Notify()\n",
    "    notification.title = \"Consent Required\"\n",
    "    notification.message = \"Hello Akash! Waiting for your consent.\"\n",
    "    notification.audio = \"E:/Music/Ringtone/Consent required1.wav\"\n",
    "    notification.send()\n",
    "\n",
    "    # Asking consent from user for original data export.\n",
    "    consent = input(\"Do you want to update LTPCT Subir Phase 2 KPIs? (Y/N)\")\n",
    "    if consent in ('y', 'Y'):\n",
    "        print(\"Updating LTPCT Subir Phase 2 KPIs...\")\n",
    "        LTPCT_Indicators = pd.read_excel('F:\\Haqdarshak Data\\LTPCT\\Scheme Indicator Categorization.xlsx', sheet_name='Subir')\n",
    "        subir = unique_data.merge(LTPCT_Indicators[['Scheme GUID','Indicator']], left_on=\"Scheme/Doc GUID\", right_on=\"Scheme GUID\", how='left')\n",
    "        subir.drop(columns=\"Scheme GUID\", inplace=True)\n",
    "        with pd.ExcelWriter(r'F:\\Haqdarshak Data\\LTPCT\\Phase 2\\Subir\\LTPCT Subir Phase 2 KPI Tracking.xlsx', if_sheet_exists='replace', mode='a', engine='openpyxl') as writer:\n",
    "            subir.to_excel(writer, sheet_name='Schemes Data', index=False)\n",
    "        print(\"LTPCT Subir Phase 2 KPIs updated!\")\n",
    "    else:\n",
    "        print(\"You chose NO to update LTPCT Subir Phase 2 KPIs!\")\n",
    "\n",
    "# For LTPCT Ahwa & Vikramgad Phase 2 projection dashboard update\n",
    "if PID in ['PID/LARSE2/2024/Custom/YK/0425','PID/LARSE2/2025/DI/YK/0526']:\n",
    "\n",
    "    # Notifying user to give consent..\n",
    "    notification = Notify()\n",
    "    notification.title = \"Consent Required\"\n",
    "    notification.message = \"Hello Akash! Waiting for your consent.\"\n",
    "    notification.audio = \"E:/Music/Ringtone/Consent required1.wav\"\n",
    "    notification.send()\n",
    "\n",
    "    # Asking consent from user for original data export.\n",
    "    consent = input(\"Do you want to update LTPCT Ahwa & Vikramgad Phase 2 KPIs? (Y/N)\")\n",
    "    if consent in ('y', 'Y'):\n",
    "        print(\"Updating LTPCT Ahwa & Vikramgad Phase 2 KPIs...\")\n",
    "        LTPCT_Indicators_Vikramgarh = pd.read_excel('F:\\Haqdarshak Data\\LTPCT\\Scheme Indicator Categorization.xlsx', sheet_name='Vikramgarh')\n",
    "        LTPCT_Indicators_Ahwa = pd.read_excel('F:\\Haqdarshak Data\\LTPCT\\Scheme Indicator Categorization.xlsx', sheet_name='Ahwa')\n",
    "        vikramgad = unique_data[unique_data.State == \"Maharashtra\"].merge(LTPCT_Indicators_Vikramgarh[['Scheme GUID','Indicator']], left_on=\"Scheme/Doc GUID\", right_on=\"Scheme GUID\", how='left')\n",
    "        ahwa = unique_data[unique_data.State == \"Gujarat\"].merge(LTPCT_Indicators_Ahwa[['Scheme GUID','Indicator']], left_on=\"Scheme/Doc GUID\", right_on=\"Scheme GUID\", how='left')\n",
    "        ahwa_vikramgad = pd.concat([vikramgad,ahwa], ignore_index=True)\n",
    "        ahwa_vikramgad.drop(columns=\"Scheme GUID\", inplace=True)\n",
    "        ahwa_vikramgad.drop_duplicates(subset=\"Case Id\", inplace=True)\n",
    "        with pd.ExcelWriter(r'F:\\Haqdarshak Data\\LTPCT\\Phase 2\\Ahwa & Vikramgad\\LTPCT Ahwa & Vikramgad Phase 2 KPI Tracking.xlsx', if_sheet_exists='replace', mode='a', engine='openpyxl') as writer:\n",
    "            ahwa_vikramgad.to_excel(writer, sheet_name='Schemes Data', index=False)\n",
    "        print(\"LTPCT Ahwa & Vikramgad Phase 2 KPIs updated!\")\n",
    "    else:\n",
    "        print(\"You chose NO to update LTPCT Ahwa & Vikramgad Phase 2 KPIs!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec482f9e-60db-4298-931a-7a65358668f1",
   "metadata": {
    "id": "0cfe58da-da90-4b16-9b65-d97b2b6ad291"
   },
   "source": [
    "# Weekly Tracker Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb5dfb5-f7e4-44a3-9c6d-3bc38419360e",
   "metadata": {
    "id": "fbb5dfb5-f7e4-44a3-9c6d-3bc38419360e"
   },
   "outputs": [],
   "source": [
    "if PID in ['PID/TATAP8/2025/DI/0550','PID/THERM2/2025/DI/0552','PID/SEDFU/2025/Custom/YK/0543','PID/TATAP2/2024/DI/0457','PID/TATAP7/2023/DI/0280','PID/TATAS1/2024/DI/0447','PID/THERM2/2024/DI/0437','PID/EICHE1/2024/DI/0439','PID/SEDFU/2024/Custom/YK/0454','PID/ALLIN1/2025/DI/0511','PID/BABLE/2025/DI/0510','PID/GOODV/2025/DI/0512','PID/NIITF/2025/DI/0513','PID/STEEL2/2025/DI/0509','PID/DLFFO/2024/Custom/0448','PID/DBSBA/2024/DI/YK/0452','PID/ZOMAT/2025/DI/0518','PID/TATAP8/2025/DI/0551']:\n",
    "    if PID == 'PID/TATAP2/2024/DI/0457':\n",
    "        print('Tata Power Renewables data exporting...')\n",
    "        with pd.ExcelWriter(r'F:\\Haqdarshak Data\\Tata Power\\Tata Power Renewables (TPREL, TPSSL, WREL) Multi state FY 25\\Tata Power Renew - Daily Tracker.xlsx', if_sheet_exists='replace', mode='a', engine='openpyxl') as writer:\n",
    "            unique_data.to_excel(writer, sheet_name='Schemes Data', index=False)\n",
    "            #writer.close()\n",
    "            print('Tata Power Renewables data export successful')\n",
    "    elif PID == 'PID/TATAP7/2023/DI/0280':\n",
    "        print('Tata Power (Maithon) data exporting...')\n",
    "        with pd.ExcelWriter(r'F:\\Haqdarshak Data\\Tata Power\\Tata Power (Maithon)\\Tata Power Maithon - Daily Tracker.xlsx', if_sheet_exists='replace', mode='a', engine='openpyxl') as writer:\n",
    "            unique_data.to_excel(writer, sheet_name='Schemes Data', index=False)\n",
    "            #writer.close()\n",
    "            print('Tata Power (Maithon) data export successful')\n",
    "    elif PID == 'PID/TATAS1/2024/DI/0447':\n",
    "        print('Tata Steel data exporting...')\n",
    "        with pd.ExcelWriter(r'F:\\Haqdarshak Data\\Tata Power\\Tata Steel\\Tata Steel - Daily Tracker.xlsx', if_sheet_exists='replace', mode='a', engine='openpyxl') as writer:\n",
    "            unique_data.to_excel(writer, sheet_name='Schemes Data', index=False)\n",
    "            #writer.close()\n",
    "            print('Tata Steel data export successful')\n",
    "    elif PID == 'PID/THERM2/2024/DI/0437':\n",
    "        print('Thermax data exporting...')\n",
    "        thermax_data = unique_data.copy()\n",
    "        siteNameRef = pd.read_excel(r'F:\\Haqdarshak Data\\Thermax Global Scale Up\\Site Name Mapping.xlsx')\n",
    "        siteMapCit = siteNameRef['Citizen GUID'].to_list()\n",
    "        thermax_data['Thermax site location name'] = thermax_data[['Citizen GUID','Thermax site location name']].apply(lambda x: siteNameRef[siteNameRef['Citizen GUID']==x[0]]['Client Name'].reset_index(drop=True)[0]\n",
    "                                                                                                                     if x[0] in siteMapCit else x[1], axis=1)\n",
    "        thermax_data['Thermax site location name'] = thermax_data['Thermax site location name'].apply(lambda x: x if x in [np.nan] else 'ACC Cement Kymore' if x.startswith('ACC') else 'Amneal-Palli'\n",
    "                                                                                                    if x.startswith('Amneal') else 'Biocon-Bangalore' if x.startswith('Biocon') else x.strip())\n",
    "        with pd.ExcelWriter(r'F:\\Haqdarshak Data\\Thermax Global Scale Up\\Thermax Global - Daily Tracker.xlsx', if_sheet_exists='replace', mode='a', engine='openpyxl') as writer:\n",
    "            thermax_data.to_excel(writer, sheet_name='Schemes Data', index=False)\n",
    "            #writer.close()\n",
    "        lastWeek = pd.date_range(dt.today().date()-timedelta(days=7), dt.today().date())\n",
    "        thermax_data['Last week'] = thermax_data.Createdon.apply(lambda x: \"Yes\" if dt.strftime(x,\"%Y-%m-%d\") in lastWeek else \"No\")\n",
    "        if \"Citizen District\" in data0.columns:\n",
    "            unique_worker = ['Citizen District','Citizen GUID','Citizen Name','Mobile','Scheme/Doc','No of cases','Thermax site location name','isYC','Last week']\n",
    "        else:\n",
    "            unique_worker = ['District','Citizen GUID','Citizen Name','Mobile','Scheme/Doc','No of cases','Thermax site location name','isYC','Last week']\n",
    "        with pd.ExcelWriter(r'F:\\Haqdarshak Data\\Thermax Global Scale Up\\Thermax Sitewise Scheme Diversity.xlsx', if_sheet_exists='replace', mode='a', engine='openpyxl') as writer:\n",
    "            thermax_data.to_excel(writer, sheet_name='Schemes Data', index=False)\n",
    "            unique_worker_data = thermax_data[unique_worker]\n",
    "            unique_worker_data = unique_worker_data[(unique_worker_data['No of cases']>1) & (~unique_worker_data['Thermax site location name'].isna()) & (unique_worker_data['Thermax site location name']!=\"? string:? string:CT0006E0 ? ?\") & (unique_worker_data['Thermax site location name']!=\"? string:? string:CT0006EN ? ?\") & (unique_worker_data['Thermax site location name']!=\"? string:? string:CT0006FD ? ?\")]\n",
    "            unique_worker_data.rename(columns={'No of cases':'Schemes per worker','Thermax site location name':'Client Name','isYC':'Loyalty Yojna (Y/N)'},inplace=True)\n",
    "            unique_worker_data = pd.concat([pd.DataFrame([\"List of \"+str(unique_worker_data['Citizen GUID'].value_counts().shape[0])+\" unique workers with 2 or more schemes.\"]),unique_worker_data],ignore_index=True)\n",
    "            sheet_name = \"Unique workers As of \"+str(dt.today().day-1)+\"_\"+str(dt.today().month)+\"_\"+dt.strftime(dt.today(),\"%y\")\n",
    "            unique_worker_data.to_excel(writer, sheet_name=sheet_name, index=True)\n",
    "            #writer.close()\n",
    "            print('Thermax data export successful')\n",
    "    elif PID == 'PID/THERM2/2025/DI/0552':\n",
    "        print('Thermax 3.0 data exporting...')\n",
    "        thermax3 = pd.concat([unique_data,dfl]) # Merging Unique data and DFL data\n",
    "        # Creating pivot based on BR status\n",
    "        BR_status = pd.pivot_table(thermax3[thermax3.Status=='Benefit Received'], index=['State','District','Thermax site location name','Citizen GUID'], columns='Scheme Category', values='Case Id', aggfunc='count').fillna(0).reset_index()\n",
    "        int_col = [c for c in BR_status.columns if BR_status[c].dtype == 'float64'] # Getting integer columns to summation.\n",
    "        BR_status['Grand Total'] = BR_status[int_col].sum(axis=1) # Adding the column for Grand Total.\n",
    "        thermax3_daily_tracker = BR_status.groupby(['State', 'District', 'Thermax site location name'], as_index=False)['Grand Total'].sum()\n",
    "        thermax3_daily_tracker.drop(columns='Grand Total', inplace=True)\n",
    "        thermax3_daily_tracker['concated'] = thermax3_daily_tracker['State'] + thermax3_daily_tracker['District'] + thermax3_daily_tracker['Thermax site location name']\n",
    "        thermax3_daily_tracker\n",
    "        \n",
    "        # Filtering on KPIs..\n",
    "        only_sch = BR_status.query('`E-Gov` == 1 and `Grand Total` == 1')\n",
    "        \n",
    "        only_DFL = BR_status.query('DFL == 1 and `Grand Total` == 1')\n",
    "        \n",
    "        only_YC = BR_status.query('YC == 1 and `Grand Total` == 1')\n",
    "        \n",
    "        DFL_Sch_YC = BR_status.query('`E-Gov` >= 1 and DFL == 1 and YC == 1 and `Grand Total` > 2')\n",
    "        \n",
    "        DFL_YC = BR_status.query('DFL == 1 and YC == 1 and `Grand Total` == 2')\n",
    "        \n",
    "        DFL_Sch = BR_status.query('`E-Gov` >= 1 and DFL == 1 and YC == 0 and `Grand Total` > 1')\n",
    "        \n",
    "        Sch_YC = BR_status.query('`E-Gov` >=1 and YC == 1 and DFL == 0 and `Grand Total` > 1')\n",
    "        \n",
    "        \n",
    "        # Initialising individual dataframes for each KPI\n",
    "        only_sch_BR = only_sch.groupby(['State', 'District', 'Thermax site location name'], as_index=False)['Citizen GUID'].nunique()\n",
    "        only_sch_BR['concated'] = only_sch_BR['State'] + only_sch_BR['District'] + only_sch_BR['Thermax site location name']\n",
    "        \n",
    "        only_DFL_BR = only_DFL.groupby(['State', 'District', 'Thermax site location name'], as_index=False)['Citizen GUID'].nunique()\n",
    "        only_DFL_BR['concated'] = only_DFL_BR['State'] + only_DFL_BR['District'] + only_DFL_BR['Thermax site location name']\n",
    "        \n",
    "        only_YC_BR = only_YC.groupby(['State', 'District', 'Thermax site location name'], as_index=False)['Citizen GUID'].nunique()\n",
    "        only_YC_BR['concated'] = only_YC_BR['State'] + only_YC_BR['District'] + only_YC_BR['Thermax site location name']\n",
    "        \n",
    "        DFL_Sch_YC_BR = DFL_Sch_YC.groupby(['State', 'District', 'Thermax site location name'], as_index=False)['Citizen GUID'].nunique()\n",
    "        DFL_Sch_YC_BR['concated'] = DFL_Sch_YC_BR['State'] + DFL_Sch_YC_BR['District'] + DFL_Sch_YC_BR['Thermax site location name']\n",
    "        \n",
    "        DFL_YC_BR = DFL_YC.groupby(['State', 'District', 'Thermax site location name'], as_index=False)['Citizen GUID'].nunique()\n",
    "        DFL_YC_BR['concated'] = DFL_YC_BR['State'] + DFL_YC_BR['District'] + DFL_YC_BR['Thermax site location name']\n",
    "        \n",
    "        DFL_Sch_BR = DFL_Sch.groupby(['State', 'District', 'Thermax site location name'], as_index=False)['Citizen GUID'].nunique()\n",
    "        DFL_Sch_BR['concated'] = DFL_Sch_BR['State'] + DFL_Sch_BR['District'] + DFL_Sch_BR['Thermax site location name']\n",
    "        \n",
    "        Sch_YC_BR = Sch_YC.groupby(['State', 'District', 'Thermax site location name'], as_index=False)['Citizen GUID'].nunique()\n",
    "        Sch_YC_BR['concated'] = Sch_YC_BR['State'] + Sch_YC_BR['District'] + Sch_YC_BR['Thermax site location name']\n",
    "        \n",
    "        \n",
    "        # Initialising final daily tracker dataframe.\n",
    "        variables = [only_sch_BR, only_DFL_BR, only_YC_BR, DFL_Sch_YC_BR, DFL_YC_BR, DFL_Sch_BR, Sch_YC_BR] # Collection of variables for loop.\n",
    "        col_names = ['Only Schemes (BR)', 'Only DFL (BR)', 'Only YC (BR)', 'DFL+Sch+YC (BR)', 'DFL+YC (BR)', 'DFL+Sch (BR)', 'Sch+YC (BR)'] # Collection of column names for loop.\n",
    "        for v,cn in zip(variables,col_names): # Looping through variables and column names.\n",
    "            thermax3_daily_tracker = thermax3_daily_tracker.merge(v[['concated', 'Citizen GUID']], on = 'concated', how = 'left').fillna(0)\n",
    "            thermax3_daily_tracker.rename(columns = {'Citizen GUID' : cn}, inplace=True)\n",
    "        thermax3_daily_tracker.drop(columns='concated', inplace = True)        \n",
    "        with pd.ExcelWriter(r'F:\\Haqdarshak Data\\Thermax Global Scale Up\\Phase 3\\Thermax Phase 3 - Daily Tracker.xlsx', if_sheet_exists='replace', mode='a', engine='openpyxl') as writer:\n",
    "            thermax3.to_excel(writer, sheet_name='Schemes Data', index=False)\n",
    "            thermax3_daily_tracker.to_excel(writer, sheet_name='Sitewise Unique Citizens', index=False)\n",
    "        print('Thermax 3.0 data export successful')\n",
    "    \n",
    "    elif PID == 'PID/EICHE1/2024/DI/0439':\n",
    "        print('Eicher data exporting...')\n",
    "        eicher = unique_data.copy()\n",
    "        eicher_locWise = pd.pivot_table(eicher, index=['State','District','Citizen GUID','Citizen Name','Mobile','Scheme/Doc','No of cases','isYC'], values='Case Id', aggfunc='count')\n",
    "        eicher_locWise = pd.DataFrame(eicher_locWise).reset_index()\n",
    "        eicher_locWise.rename(columns={'No of cases':'Schemes per Driver','isYC':'Loyalty Yojna (Y/N)'}, inplace=True)\n",
    "        eicher_locWise.drop(columns='Case Id', inplace=True)\n",
    "        with pd.ExcelWriter('C:\\\\Python\\\\export\\\\'+fn.split('_')[2]+'_'+fn.split('_')[3]+' data.xlsx', if_sheet_exists='replace', mode='a', engine='openpyxl') as writer:\n",
    "            sheet_name = \"Unique drivers As of \"+str(dt.today().day-1)+\"_\"+str(dt.today().month)+\"_\"+dt.strftime(dt.today(),\"%y\")\n",
    "            eicher_locWise.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "            #writer.close()\n",
    "            print('Eicher data export successful')\n",
    "    \n",
    "    elif PID == 'PID/SEDFU/2024/Custom/YK/0454':\n",
    "        print('SED Fund data exporting...')\n",
    "        sed_fund = pd.concat([unique_data,dfl_uniques])\n",
    "        col_ord = ['Case Id','Createdon','Scheme Category','Scheme/Doc','Scheme/Doc GUID','Status','Docket Submitted Date','Benefit received Date','HD Suspected Cases','State','District','Citizen GUID','Citizen Name','Gender','Mobile','Age','Citizen Block','Citizen Village','Family GUID','Family Name','HD ID','HD Name','Opsco name','AMS ID','isYC','Parent Scheme','Scheme type','Benefit Value','open_price','Docket submitted price','scheme_document_received price','HD_Payment','duplicate','parent_duplicate','No of cases']\n",
    "        with pd.ExcelWriter(r'F:\\Haqdarshak Data\\SED Fund\\SED Fund - Daily Tracker.xlsx', if_sheet_exists='replace', mode='a', engine='openpyxl') as writer:\n",
    "            sed_fund[col_ord].to_excel(writer, sheet_name='Sch_DFL Data', index=False)\n",
    "        col_ord=['Case Id','Createdon','Scheme/Doc','Scheme/Doc GUID','Status','Docket Submitted Date','Benefit received Date','HD Suspected Cases','State','Opsco name','HD Name','District','Citizen GUID','Citizen Name','Gender','Mobile','Age','Citizen Block','Citizen Village','Family GUID','Family Name','HD ID','AMS ID','isYC','Parent Scheme','Scheme type','Benefit Value','open_price','Docket submitted price','scheme_document_received price','HD_Payment','Scheme Category','duplicate','parent_duplicate','No of cases']\n",
    "        with pd.ExcelWriter(r'F:\\Haqdarshak Data\\SED Fund\\SED Fund HD_State wise weekly achvmt.xlsx', if_sheet_exists='replace', mode='a', engine='openpyxl') as writer:\n",
    "            sed_fund[col_ord].to_excel(writer, sheet_name='Sch_DFL Data', index=False)\n",
    "            #writer.close()\n",
    "            print('SED Fund data export successful')\n",
    "    \n",
    "    elif PID == 'PID/SEDFU/2025/Custom/YK/0543':\n",
    "        print('SED Fund 2.0 data exporting...')\n",
    "        sed_fund = pd.concat([unique_data,dfl_uniques])\n",
    "        col_ord=['Case Id','Createdon','Scheme/Doc','Scheme/Doc GUID','Status','Docket Submitted Date','Benefit received Date','HD Suspected Cases','State','Opsco name','HD Name','District','Citizen GUID','Citizen Name','Gender','Mobile','Age','Citizen Block','Citizen Village','Family GUID','Family Name','HD ID','AMS ID','isYC','Parent Scheme','Scheme type','Benefit Value','open_price','Docket submitted price','scheme_document_received price','HD_Payment','Scheme Category','duplicate','parent_duplicate','No of cases']\n",
    "        with pd.ExcelWriter(r'F:\\Haqdarshak Data\\SED Fund\\Phase 2\\SED Fund 2.0 HD_State wise weekly achvmt.xlsx', if_sheet_exists='replace', mode='a', engine='openpyxl') as writer:\n",
    "            sed_fund[col_ord].to_excel(writer, sheet_name='Sch_DFL Data', index=False)\n",
    "            #writer.close()\n",
    "            print('SED Fund 2.0 data export successful')\n",
    "    \n",
    "    elif PID == 'PID/STEEL2/2025/DI/0509':\n",
    "        print('Steel Authority of India Limited (IISCO Burnpur) data exporting...')\n",
    "        #sed_fund = pd.concat([unique_data,dfl_uniques])\n",
    "        with pd.ExcelWriter(r'F:\\Haqdarshak Data\\Steel Authority of India Limited (IISCO Burnpur)\\Steel Authority of India Limited (IISCO Burnpur) - Daily Tracker.xlsx', if_sheet_exists='replace', mode='a', engine='openpyxl') as writer:\n",
    "            unique_data.to_excel(writer, sheet_name='Schemes Data', index=False)\n",
    "            #writer.close()\n",
    "            print('Steel Authority of India Limited (IISCO Burnpur) data export successful')\n",
    "    \n",
    "    elif PID == 'PID/ALLIN1/2025/DI/0511':\n",
    "        print('UNICEF (Urban)- AISECT data exporting...')\n",
    "        with pd.ExcelWriter(r'F:\\Haqdarshak Data\\UNICEF\\AISECT\\UNICEF AISECT - Daily Tracker.xlsx', if_sheet_exists='replace', mode='a', engine='openpyxl') as writer:\n",
    "            unique_data.to_excel(writer, sheet_name='Schemes Data', index=False)\n",
    "            #writer.close()\n",
    "            print('UNICEF (Urban)- AISECT data export successful')\n",
    "    \n",
    "    elif PID == 'PID/BABLE/2025/DI/0510':\n",
    "        print('UNICEF (Urban)- B ABLE data exporting...')\n",
    "        with pd.ExcelWriter(r'F:\\Haqdarshak Data\\UNICEF\\B ABLE\\UNICEF B ABLE - Daily Tracker.xlsx', if_sheet_exists='replace', mode='a', engine='openpyxl') as writer:\n",
    "            unique_data.to_excel(writer, sheet_name='Schemes Data', index=False)\n",
    "            #writer.close()\n",
    "            print('UNICEF (Urban)- B ABLE data export successful')\n",
    "    \n",
    "    elif PID == 'PID/GOODV/2025/DI/0512':\n",
    "        print('UNICEF (Urban)- Good Vision data exporting...')\n",
    "        with pd.ExcelWriter(r'F:\\Haqdarshak Data\\UNICEF\\Good Vision\\UNICEF Good Vision - Daily Tracker.xlsx', if_sheet_exists='replace', mode='a', engine='openpyxl') as writer:\n",
    "            unique_data.to_excel(writer, sheet_name='Schemes Data', index=False)\n",
    "            #writer.close()\n",
    "            print('UNICEF (Urban)- Good Vision data export successful')\n",
    "    \n",
    "    elif PID == 'PID/NIITF/2025/DI/0513':\n",
    "        print('UNICEF (Urban)- NIIT data exporting...')\n",
    "        with pd.ExcelWriter(r'F:\\Haqdarshak Data\\UNICEF\\NIIT\\UNICEF NIIT - Daily Tracker.xlsx', if_sheet_exists='replace', mode='a', engine='openpyxl') as writer:\n",
    "            unique_data.to_excel(writer, sheet_name='Schemes Data', index=False)\n",
    "            #writer.close()\n",
    "            print('UNICEF (Urban)- NIIT data export successful')\n",
    "    \n",
    "    elif PID == 'PID/DLFFO/2024/Custom/0448':\n",
    "        print('DLF Suvidha data exporting...')\n",
    "        with pd.ExcelWriter(r'F:\\Haqdarshak Data\\DLF\\DLF Suvidha - Daily Tracker.xlsx', if_sheet_exists='replace', mode='a', engine='openpyxl') as writer:\n",
    "            unique_data.to_excel(writer, sheet_name='Schemes Data', index=False)\n",
    "        print('DLF Suvidha data export successful')\n",
    "    \n",
    "    elif PID == 'PID/ZOMAT/2025/DI/0518':\n",
    "        print('Zomato 2025 data exporting...')\n",
    "        with pd.ExcelWriter(r'F:\\Haqdarshak Data\\Zomato\\Phase 2\\Zomato 2025 - Daily Tracker.xlsx', if_sheet_exists='replace', mode='a', engine='openpyxl') as writer:\n",
    "            unique_data.to_excel(writer, sheet_name='Schemes Data', index=False)\n",
    "        print('Zomato 2025 data export successful')\n",
    "\n",
    "    elif PID == 'PID/TATAP8/2025/DI/0551':\n",
    "        print(\"Tata Power Hydros (Mulshi)'25-26 data exporting...\")\n",
    "        tata_pwr_mulshi = pd.concat([unique_data,dfl])\n",
    "        with pd.ExcelWriter(r\"F:\\Haqdarshak Data\\Tata Power\\Tata Power Hydros (Mulshi)\\Tata Power Hydros (Mulshi)'25-26 - Daily Tracker.xlsx\", if_sheet_exists='replace', mode='a', engine='openpyxl') as writer:\n",
    "            tata_pwr_mulshi.to_excel(writer, sheet_name='Schemes Data', index=False)\n",
    "        print(\"Tata Power Hydros (Mulshi)'25-26 data export successful\")\n",
    "\n",
    "    elif PID == 'PID/TATAP8/2025/DI/0550':\n",
    "        print(\"Tata Power Hydros (Maval)'25-26 data exporting...\")\n",
    "        tata_pwr_maval = pd.concat([unique_data,dfl])\n",
    "        with pd.ExcelWriter(r\"F:\\Haqdarshak Data\\Tata Power\\Tata Power Hydros (Maval)\\Tata Power Hydros (Maval)'25-26 - Daily Tracker.xlsx\", if_sheet_exists='replace', mode='a', engine='openpyxl') as writer:\n",
    "            tata_pwr_maval.to_excel(writer, sheet_name='Schemes Data', index=False)\n",
    "        print(\"Tata Power Hydros (Maval)'25-26 data export successful\")\n",
    "    \n",
    "    elif PID == 'PID/DBSBA/2024/DI/YK/0452':\n",
    "        print('DBS 2.0 data exporting...')\n",
    "        with pd.ExcelWriter(r'F:\\Haqdarshak Data\\DBS\\Phase 2\\DBS 2.0 - All Data - '+dt.today().strftime('%d-%m-%Y')+'.xlsx', engine='openpyxl') as writer:\n",
    "            og_DF.to_excel(writer, sheet_name='Schemes Data', index=False)\n",
    "            rejectedDF.to_excel(writer, sheet_name='Abort_Rejected', index=False)\n",
    "            duplicateData.to_excel(writer, sheet_name='Duplicate data', index=False)\n",
    "            parentDuplicateData.to_excel(writer, sheet_name='Parent Sch Duplicate', index=False)\n",
    "        print('DBS 2.0 data export successful')\n",
    "        # Creating DBS 2.0 Weekly Table\n",
    "        #- This table will show various KPIs as per management\n",
    "        #- This table will diretly update the in the DBS 2.0 tracker\n",
    "        print('DBS 2.0 weekly table generating...')\n",
    "        # Filtering the dates\n",
    "        yesterday = pd.Timestamp((dt.today()-timedelta(days=1)).strftime(\"%Y-%m-%d 00:00:00\"))\n",
    "        eight_days_ago = pd.Timestamp((dt.today()-timedelta(days=7)).strftime(\"%Y-%m-%d 00:00:00\"))\n",
    "\n",
    "        # BR  Pivot till yesterday\n",
    "        dbs_pivot = pd.pivot_table(og_DF[(og_DF.Status==\"Benefit Received\") & (og_DF['Benefit received Date'] <= yesterday)], index=['State','Citizen GUID'], columns='Scheme Category', values='Case Id', aggfunc='count', fill_value=0)\n",
    "        dbs_pivot = dbs_pivot.reset_index()\n",
    "        category_cols = [c for c in dbs_pivot.columns if c not in ['State','Citizen GUID']]\n",
    "        dbs_pivot['Grand Total'] = dbs_pivot[category_cols].sum(axis=1)\n",
    "\n",
    "\n",
    "        # BR Pivot till 7 days ago\n",
    "        dbs_pivot_t8 = pd.pivot_table(og_DF[(og_DF.Status==\"Benefit Received\") & (og_DF['Benefit received Date'] <= eight_days_ago)], index=['State','Citizen GUID'], columns='Scheme Category', values='Case Id', aggfunc='count', fill_value=0)\n",
    "        dbs_pivot_t8 = dbs_pivot_t8.reset_index()\n",
    "        category_cols = [c for c in dbs_pivot_t8.columns if c not in ['State','Citizen GUID']]\n",
    "        dbs_pivot_t8['Grand Total'] = dbs_pivot_t8[category_cols].sum(axis=1)\n",
    "\n",
    "        # only Schemes\n",
    "        only_sch = pd.DataFrame(dbs_pivot[(dbs_pivot.DFL==0) & (dbs_pivot['E-Gov']>0) & (dbs_pivot.EDP==0) & (dbs_pivot.MSMEYC==0) & (dbs_pivot.MSME_Sch==0) & (dbs_pivot.YC==0)].groupby('State')['Citizen GUID'].count()).reset_index()\n",
    "        only_sch_t8 = pd.DataFrame(dbs_pivot_t8[(dbs_pivot_t8.DFL==0) & (dbs_pivot_t8['E-Gov']>0) & (dbs_pivot_t8.EDP==0) & (dbs_pivot_t8.MSMEYC==0) & (dbs_pivot_t8.MSME_Sch==0) & (dbs_pivot_t8.YC==0)].groupby('State')['Citizen GUID'].count()).reset_index()\n",
    "\n",
    "        # YC + DFL\n",
    "        yc_dfl = pd.DataFrame(dbs_pivot[(dbs_pivot.DFL==1) & (dbs_pivot['E-Gov']==0) & (dbs_pivot.EDP==0) & (dbs_pivot.MSMEYC==0) & (dbs_pivot.MSME_Sch==0) & (dbs_pivot.YC==1)].groupby('State')['Citizen GUID'].count()).reset_index()\n",
    "        yc_dfl_t8 = pd.DataFrame(dbs_pivot_t8[(dbs_pivot_t8.DFL>0) & (dbs_pivot_t8['E-Gov']==0) & (dbs_pivot_t8.EDP==0) & (dbs_pivot_t8.MSMEYC==0) & (dbs_pivot_t8.MSME_Sch==0) & (dbs_pivot_t8.YC>0)].groupby('State')['Citizen GUID'].count()).reset_index()\n",
    "\n",
    "        # DFL + YC + 1 Scheme\n",
    "        dfl_yc_1 = pd.DataFrame(dbs_pivot[(dbs_pivot.DFL==1) & (dbs_pivot['E-Gov']==1) & (dbs_pivot.EDP==0) & (dbs_pivot.MSMEYC==0) & (dbs_pivot.MSME_Sch==0) & (dbs_pivot.YC==1)].groupby('State')['Citizen GUID'].count()).reset_index()\n",
    "        dfl_yc_1_t8 = pd.DataFrame(dbs_pivot_t8[(dbs_pivot_t8.DFL>0) & (dbs_pivot_t8['E-Gov']==1) & (dbs_pivot_t8.EDP==0) & (dbs_pivot_t8.MSMEYC==0) & (dbs_pivot_t8.MSME_Sch==0) & (dbs_pivot_t8.YC>0)].groupby('State')['Citizen GUID'].count()).reset_index()\n",
    "\n",
    "        # DFL + YC + 2 or more scheme\n",
    "        dfl_yc_2more = pd.DataFrame(dbs_pivot[(dbs_pivot.DFL==1) & (dbs_pivot['E-Gov']>1) & (dbs_pivot.EDP==0) & (dbs_pivot.MSMEYC==0) & (dbs_pivot.MSME_Sch==0) & (dbs_pivot.YC==1)].groupby('State')['Citizen GUID'].count()).reset_index()\n",
    "        dfl_yc_2more_t8 = pd.DataFrame(dbs_pivot_t8[(dbs_pivot_t8.DFL>0) & (dbs_pivot_t8['E-Gov']>1) & (dbs_pivot_t8.EDP==0) & (dbs_pivot_t8.MSMEYC==0) & (dbs_pivot_t8.MSME_Sch==0) & (dbs_pivot_t8.YC>0)].groupby('State')['Citizen GUID'].count()).reset_index()\n",
    "\n",
    "        # DFL Only\n",
    "        only_dfl = pd.DataFrame(dbs_pivot[(dbs_pivot.DFL==1) & (dbs_pivot['E-Gov']==0) & (dbs_pivot.EDP==0) & (dbs_pivot.MSMEYC==0) & (dbs_pivot.MSME_Sch==0) & (dbs_pivot.YC==0)].groupby('State')['Citizen GUID'].count()).reset_index()\n",
    "        only_dfl_t8 = pd.DataFrame(dbs_pivot_t8[(dbs_pivot_t8.DFL>0) & (dbs_pivot_t8['E-Gov']==0) & (dbs_pivot_t8.EDP==0) & (dbs_pivot_t8.MSMEYC==0) & (dbs_pivot_t8.MSME_Sch==0) & (dbs_pivot_t8.YC==0)].groupby('State')['Citizen GUID'].count()).reset_index()\n",
    "\n",
    "        # YC Only\n",
    "        # Changing Pivot filters to all status till Yesterday\n",
    "        dbs_pivot_only_yc = pd.pivot_table(og_DF[og_DF['Createdon'] <= yesterday], index=['State','Citizen GUID'], columns='Scheme Category', values='Case Id', aggfunc='count', fill_value=0)\n",
    "        dbs_pivot_only_yc = dbs_pivot_only_yc.reset_index()\n",
    "        category_cols = [c for c in dbs_pivot_only_yc.columns if c not in ['State','Citizen GUID']]\n",
    "        dbs_pivot_only_yc['Grand Total'] = dbs_pivot_only_yc[category_cols].sum(axis=1)\n",
    "        only_yc = pd.DataFrame(dbs_pivot[(dbs_pivot['Grand Total']==1) & (dbs_pivot.YC==1)].groupby('State')['Citizen GUID'].count()).reset_index() # Change the dataframe as per the reuirement\n",
    "                                                                                                                                                    # from \"dbs_pivot_t8\" to \"dbs_pivot_only_yc_t8\"\n",
    "        # Changing Pivot filters to all status till 7 days before\n",
    "        dbs_pivot_only_yc_t8 = pd.pivot_table(og_DF[og_DF['Createdon'] <= eight_days_ago], index=['State','Citizen GUID'], columns='Scheme Category', values='Case Id', aggfunc='count', fill_value=0)\n",
    "        dbs_pivot_only_yc_t8 = dbs_pivot_only_yc_t8.reset_index()\n",
    "        category_cols = [c for c in dbs_pivot_only_yc_t8.columns if c not in ['State','Citizen GUID']]\n",
    "        dbs_pivot_only_yc_t8['Grand Total'] = dbs_pivot_only_yc_t8[category_cols].sum(axis=1)\n",
    "        only_yc_t8 = pd.DataFrame(dbs_pivot_t8[(dbs_pivot_t8['Grand Total']==1) & (dbs_pivot_t8.YC==1)].groupby('State')['Citizen GUID'].count()).reset_index() # Change the dataframe as per the reuirement\n",
    "                                                                                                                                                                # from \"dbs_pivot_t8\" to \"dbs_pivot_only_yc_t8\"\n",
    "\n",
    "        # DFL + Schemes\n",
    "        dfl_sch = pd.DataFrame(dbs_pivot[(dbs_pivot.DFL==1) & (dbs_pivot['E-Gov']>0) & (dbs_pivot.EDP==0) & (dbs_pivot.MSMEYC==0) & (dbs_pivot.MSME_Sch==0) & (dbs_pivot.YC==0)].groupby('State')['Citizen GUID'].count()).reset_index()\n",
    "        dfl_sch_t8 = pd.DataFrame(dbs_pivot_t8[(dbs_pivot_t8.DFL>0) & (dbs_pivot_t8['E-Gov']>0) & (dbs_pivot_t8.EDP==0) & (dbs_pivot_t8.MSMEYC==0) & (dbs_pivot_t8.MSME_Sch==0) & (dbs_pivot_t8.YC==0)].groupby('State')['Citizen GUID'].count()).reset_index()\n",
    "\n",
    "        # YC + Schemes\n",
    "        yc_sch = pd.DataFrame(dbs_pivot[(dbs_pivot.DFL==0) & (dbs_pivot['E-Gov']>0) & (dbs_pivot.EDP==0) & (dbs_pivot.MSMEYC==0) & (dbs_pivot.MSME_Sch==0) & (dbs_pivot.YC==1) & (dbs_pivot['Grand Total']>=2)].groupby('State')['Citizen GUID'].count()).reset_index()\n",
    "        yc_sch_t8 = pd.DataFrame(dbs_pivot_t8[(dbs_pivot_t8.DFL==0) & (dbs_pivot_t8['E-Gov']>0) & (dbs_pivot_t8.EDP==0) & (dbs_pivot_t8.MSMEYC==0) & (dbs_pivot_t8.MSME_Sch==0) & (dbs_pivot_t8.YC==1) & (dbs_pivot_t8['Grand Total']>=2)].groupby('State')['Citizen GUID'].count()).reset_index()\n",
    "\n",
    "        # Only Women\n",
    "        dbs_women = pd.pivot_table(og_DF[(og_DF.Status==\"Benefit Received\") & (og_DF['Benefit received Date'] <= yesterday) & (og_DF['Gender'] == 'Female')], index=['State','Citizen GUID'], values='Case Id', aggfunc='count', fill_value=0)\n",
    "        dbs_women = dbs_women.reset_index()\n",
    "        dbs_women = pd.DataFrame(dbs_women.groupby(by='State')['Citizen GUID'].count().reset_index())\n",
    "\n",
    "        # Elderly citizens above 60 age\n",
    "        dbs_60_above = pd.pivot_table(og_DF[(og_DF.Status==\"Benefit Received\") & (og_DF['Benefit received Date'] <= yesterday) & (og_DF['Age'] > 60.0)], index=['State','Citizen GUID'], values='Case Id', aggfunc='count', fill_value=0)\n",
    "        dbs_60_above = dbs_60_above.reset_index()\n",
    "        dbs_60_above = pd.DataFrame(dbs_60_above.groupby(by='State')['Citizen GUID'].count().reset_index())\n",
    "\n",
    "        # Youth below 25 age\n",
    "        dbs_25_below = pd.pivot_table(og_DF[(og_DF.Status==\"Benefit Received\") & (og_DF['Benefit received Date'] <= yesterday) & (og_DF['Age'] < 25.0)], index=['State','Citizen GUID'], values='Case Id', aggfunc='count', fill_value=0)\n",
    "        dbs_25_below = dbs_25_below.reset_index()\n",
    "        dbs_25_below = pd.DataFrame(dbs_25_below.groupby(by='State')['Citizen GUID'].count().reset_index())\n",
    "\n",
    "\n",
    "\n",
    "        # Creating a DBS 2.0 weekly tracker\n",
    "        dbs_tracker_weekly = {'State' : only_sch.State,\n",
    "                              'Only Schemes Till '+str(yesterday.day)+'th '+yesterday.strftime(\"%b\") : only_sch['Citizen GUID'],\n",
    "                              'Only Schemes Till '+str(eight_days_ago.day)+'th '+eight_days_ago.strftime(\"%b\") : only_sch_t8['Citizen GUID'],\n",
    "                              'YC + DFL Till '+str(yesterday.day)+'th '+yesterday.strftime(\"%b\") : yc_dfl['Citizen GUID'],\n",
    "                              'YC + DFL Till '+str(eight_days_ago.day)+'th '+eight_days_ago.strftime(\"%b\") : yc_dfl_t8['Citizen GUID'],\n",
    "                              'DFL + YC + 1 Scheme Till '+str(yesterday.day)+'th '+yesterday.strftime(\"%b\") : dfl_yc_1['Citizen GUID'],\n",
    "                              'DFL + YC + 1 Scheme Till '+str(eight_days_ago.day)+'th '+eight_days_ago.strftime(\"%b\") : dfl_yc_1_t8['Citizen GUID'],\n",
    "                              'DFL + YC + 2 or more scheme Till '+str(yesterday.day)+'th '+yesterday.strftime(\"%b\") : dfl_yc_2more['Citizen GUID'],\n",
    "                              'DFL + YC + 2 or more scheme Till '+str(eight_days_ago.day)+'th '+eight_days_ago.strftime(\"%b\") : dfl_yc_2more_t8['Citizen GUID'],\n",
    "                              'DFL Only Till '+str(yesterday.day)+'th '+yesterday.strftime(\"%b\") : only_dfl['Citizen GUID'],\n",
    "                              'DFL Only Till '+str(eight_days_ago.day)+'th '+eight_days_ago.strftime(\"%b\") : only_dfl_t8['Citizen GUID'],\n",
    "                              'YC Only Till '+str(yesterday.day)+'th '+yesterday.strftime(\"%b\") : only_yc['Citizen GUID'],\n",
    "                              'YC Only Till '+str(eight_days_ago.day)+'th '+eight_days_ago.strftime(\"%b\") : only_yc_t8['Citizen GUID'],\n",
    "                              'DFL + Schemes Till '+str(yesterday.day)+'th '+yesterday.strftime(\"%b\") : dfl_sch['Citizen GUID'],\n",
    "                              'DFL + Schemes Till '+str(eight_days_ago.day)+'th '+eight_days_ago.strftime(\"%b\") : dfl_sch_t8['Citizen GUID'],\n",
    "                              'YC + Schemes Till '+str(yesterday.day)+'th '+yesterday.strftime(\"%b\") : yc_sch['Citizen GUID'],\n",
    "                              'YC + Schemes Till '+str(eight_days_ago.day)+'th '+eight_days_ago.strftime(\"%b\") : yc_sch_t8['Citizen GUID'],\n",
    "                              'women 50%' : dbs_women['Citizen GUID'],\n",
    "                              'elderly (above 60) 20%' : dbs_60_above['Citizen GUID'],\n",
    "                              'Youth age <25 (30%)' : dbs_25_below['Citizen GUID'],\n",
    "                      }\n",
    "        dbs_tracker_weekly = pd.DataFrame(dbs_tracker_weekly) # Convert dictionary to padas dataframe.\n",
    "\n",
    "        # Only YC MSME\n",
    "        yc_msme = pd.DataFrame(dbs_pivot[(dbs_pivot.DFL==0) & (dbs_pivot['E-Gov']==0) & (dbs_pivot.EDP==0) & (dbs_pivot.MSMEYC==1) & (dbs_pivot.MSME_Sch==0) & (dbs_pivot.YC==0)].groupby('State')['Citizen GUID'].count()).reset_index()\n",
    "        dbs_tracker_weekly = dbs_tracker_weekly.merge(yc_msme, on='State', how='left').fillna(0)\n",
    "        dbs_tracker_weekly.rename(columns={'Citizen GUID' : 'YC MSME'}, inplace=True)\n",
    "\n",
    "        # EDP + MSME YC\n",
    "        EDP_ycmsme = pd.DataFrame(dbs_pivot[(dbs_pivot.DFL==0) & (dbs_pivot['E-Gov']==0) & (dbs_pivot.EDP==1) & (dbs_pivot.MSMEYC==1) & (dbs_pivot.MSME_Sch==0) & (dbs_pivot.YC==0)].groupby('State')['Citizen GUID'].count()).reset_index()\n",
    "        dbs_tracker_weekly = dbs_tracker_weekly.merge(EDP_ycmsme, on='State', how='left').fillna(0)\n",
    "        dbs_tracker_weekly.rename(columns={'Citizen GUID' : 'EDP +YC(MSME)'}, inplace=True)\n",
    "\n",
    "        # EDP + Schemes\n",
    "        EDP_sch = pd.DataFrame(dbs_pivot[(dbs_pivot.DFL==0) & (dbs_pivot['E-Gov']>0) & (dbs_pivot.EDP==1) & (dbs_pivot.MSMEYC==0) & (dbs_pivot.MSME_Sch==0) & (dbs_pivot.YC==0)].groupby('State')['Citizen GUID'].count()).reset_index()\n",
    "        dbs_tracker_weekly = dbs_tracker_weekly.merge(EDP_sch, on='State', how='left').fillna(0)\n",
    "        dbs_tracker_weekly.rename(columns={'Citizen GUID' : 'EDP+Scheme'}, inplace=True)\n",
    "\n",
    "        # EDP + MSME YC + Schemes\n",
    "        EDP_ycmsme_sch = pd.DataFrame(dbs_pivot[(dbs_pivot.DFL==0) & (dbs_pivot['E-Gov']>0) & (dbs_pivot.EDP==1) & (dbs_pivot.MSMEYC==1) & (dbs_pivot.MSME_Sch==0) & (dbs_pivot.YC==0)].groupby('State')['Citizen GUID'].count()).reset_index()\n",
    "        dbs_tracker_weekly = dbs_tracker_weekly.merge(EDP_ycmsme_sch, on='State', how='left').fillna(0)\n",
    "        dbs_tracker_weekly.rename(columns={'Citizen GUID' : 'EDP+YC(MSME)+Scheme'}, inplace=True)\n",
    "\n",
    "        # EDP Only\n",
    "        edp_only = pd.DataFrame(dbs_pivot[(dbs_pivot.DFL==0) & (dbs_pivot['E-Gov']==0) & (dbs_pivot.EDP==1) & (dbs_pivot.MSMEYC==0) & (dbs_pivot.MSME_Sch==0) & (dbs_pivot.YC==0)].groupby('State')['Citizen GUID'].count()).reset_index()\n",
    "        dbs_tracker_weekly = dbs_tracker_weekly.merge(edp_only, on='State', how='left').fillna(0)\n",
    "        dbs_tracker_weekly.rename(columns={'Citizen GUID' : 'EDP Only'}, inplace=True)\n",
    "\n",
    "        # Statewise Total BR Case count\n",
    "        total_BR = pd.DataFrame(dbs_pivot.groupby(\"State\")['Grand Total'].sum()).reset_index()\n",
    "        dbs_tracker_weekly['Cases (BR)'] = total_BR['Grand Total']\n",
    "\n",
    "        \n",
    "        #Only Schemes atleast 2\n",
    "        Only_2_Sch = dbs_pivot[(dbs_pivot[\"E-Gov\"]>1) & (dbs_pivot.DFL==0) & (dbs_pivot.YC==0) & (dbs_pivot.EDP==0) & (dbs_pivot.MSMEYC==0) & (dbs_pivot.MSME_Sch==0)].groupby(\"State\",as_index = False)['Citizen GUID'].nunique()\n",
    "        dbs_tracker_weekly['Only_2_Sch'] = Only_2_Sch['Citizen GUID'] \n",
    "\n",
    "        # All status KPI tracking\n",
    "        # All status Pivot till yesterday\n",
    "        dbs_pivot_all = pd.pivot_table(og_DF[og_DF.Createdon <= yesterday], index=['State','Citizen GUID'], columns='Scheme Category', values='Case Id', aggfunc='count', fill_value=0)\n",
    "        dbs_pivot_all = dbs_pivot_all.reset_index()\n",
    "        category_cols = [c for c in dbs_pivot_all.columns if c not in ['State','Citizen GUID']]\n",
    "        dbs_pivot_all['Grand Total'] = dbs_pivot_all[category_cols].sum(axis=1)\n",
    "\n",
    "        # All status till 7 days ago\n",
    "        dbs_pivot_t8_all = pd.pivot_table(og_DF[og_DF.Createdon <= eight_days_ago], index=['State','Citizen GUID'], columns='Scheme Category', values='Case Id', aggfunc='count', fill_value=0)\n",
    "        dbs_pivot_t8_all = dbs_pivot_t8_all.reset_index()\n",
    "        category_cols = [c for c in dbs_pivot_t8_all.columns if c not in ['State','Citizen GUID']]\n",
    "        dbs_pivot_t8_all['Grand Total'] = dbs_pivot_t8_all[category_cols].sum(axis=1)\n",
    "\n",
    "        # Unique Citizens\n",
    "        unique_citizens = pd.DataFrame(dbs_pivot_all.groupby('State', as_index=False)['Citizen GUID'].count())\n",
    "\n",
    "        # YC + DFL All status\n",
    "        yc_dfl_all = dbs_pivot_all[(dbs_pivot_all.YC==1) & (dbs_pivot_all.DFL==1) & (dbs_pivot_all['Grand Total']==2)].groupby('State', as_index=False)['Citizen GUID'].nunique()\n",
    "\n",
    "        # YC + DFL + Scheme All Status\n",
    "        yc_dfl_sch_all = dbs_pivot_all[(dbs_pivot_all.YC==1) & (dbs_pivot_all.DFL==1) & (dbs_pivot_all['E-Gov']>0)].groupby('State', as_index=False)['Citizen GUID'].nunique()\n",
    "\n",
    "        # Scheme + YC All Status\n",
    "        sch_yc_all = dbs_pivot_all[(dbs_pivot_all.YC==1) & (dbs_pivot_all['E-Gov']>0) & (dbs_pivot_all.DFL==0) & (dbs_pivot_all['Grand Total']>=2) & (dbs_pivot_all.EDP==0) & (dbs_pivot_all.MSMEYC==0) & (dbs_pivot_all.MSME_Sch==0)].groupby('State', as_index=False)['Citizen GUID'].nunique()\n",
    "\n",
    "        # Only Women All Status\n",
    "        dbs_women_all = pd.pivot_table(og_DF[og_DF['Gender'] == 'Female'], index=['State','Citizen GUID'], values='Case Id', aggfunc='count', fill_value=0)\n",
    "        dbs_women_all = dbs_women_all.reset_index()\n",
    "        dbs_women_all = pd.DataFrame(dbs_women_all.groupby(by='State', as_index=False)['Citizen GUID'].nunique())\n",
    "\n",
    "        # Elderly citizens above 60 age All Status\n",
    "        dbs_60_above_all = pd.pivot_table(og_DF[(og_DF.Createdon <= yesterday) & (og_DF['Age'] > 60.0)], index=['State','Citizen GUID'], values='Case Id', aggfunc='count', fill_value=0)\n",
    "        dbs_60_above_all = dbs_60_above_all.reset_index()\n",
    "        dbs_60_above_all = pd.DataFrame(dbs_60_above_all.groupby(by='State', as_index=False)['Citizen GUID'].count())\n",
    "\n",
    "        # Youth below 25 age All Status\n",
    "        dbs_25_below_all = pd.pivot_table(og_DF[(og_DF.Createdon <= yesterday) & (og_DF['Age'] < 25.0)], index=['State','Citizen GUID'], values='Case Id', aggfunc='count', fill_value=0)\n",
    "        dbs_25_below_all = dbs_25_below_all.reset_index()\n",
    "        dbs_25_below_all = pd.DataFrame(dbs_25_below_all.groupby(by='State', as_index=False)['Citizen GUID'].count())\n",
    "\n",
    "        dbs_tracker_weekly_all_status={'State' : only_sch.State,\n",
    "                                      'unique_citizens' : unique_citizens['Citizen GUID'],\n",
    "                                      'YC+DFL (35%)' : yc_dfl_all['Citizen GUID'],\n",
    "                                      'YC+Scheme+DFL (15%)' : yc_dfl_sch_all['Citizen GUID'],\n",
    "                                      'Scheme+YC (50%)' : sch_yc_all['Citizen GUID'],\n",
    "                                      'women 50%' : dbs_women_all['Citizen GUID'],\n",
    "                                      'elderly (above 60) 20%' : dbs_60_above_all['Citizen GUID'],\n",
    "                                      'Youth age <25 (30%)' : dbs_25_below_all['Citizen GUID']\n",
    "                                       }\n",
    "\n",
    "        dbs_tracker_weekly_all_status = pd.DataFrame(dbs_tracker_weekly_all_status) # Convert dictionary to padas dataframe.\n",
    "\n",
    "        # Only YC MSME All Status\n",
    "        ycmsme_all = dbs_pivot_all[(dbs_pivot_all.DFL==0) & (dbs_pivot_all['E-Gov']==0) & (dbs_pivot_all.EDP==0) & (dbs_pivot_all.MSMEYC==1) & (dbs_pivot_all.MSME_Sch==0) & (dbs_pivot_all.YC==0)].groupby('State', as_index=False)['Citizen GUID'].nunique()\n",
    "        dbs_tracker_weekly_all_status = dbs_tracker_weekly_all_status.merge(ycmsme_all, on='State', how='left').fillna(0)\n",
    "        dbs_tracker_weekly_all_status.rename(columns={'Citizen GUID':'YC MSME'}, inplace=True)\n",
    "\n",
    "        # EDP + MSME YC All Status\n",
    "        edp_msmeYC_all = dbs_pivot_all[(dbs_pivot_all.MSMEYC==1) & (dbs_pivot_all.EDP==1) & (dbs_pivot_all['Grand Total']==2)].groupby('State', as_index=False)['Citizen GUID'].nunique()\n",
    "        dbs_tracker_weekly_all_status = dbs_tracker_weekly_all_status.merge(edp_msmeYC_all, on='State', how='left').fillna(0)\n",
    "        dbs_tracker_weekly_all_status.rename(columns={'Citizen GUID':'EDP +YC(MSME)'}, inplace=True)\n",
    "\n",
    "        # EDP + Scheme All Status\n",
    "        EDP_sch_all = dbs_pivot_all[(dbs_pivot_all['E-Gov']>0) & (dbs_pivot_all.MSME_Sch==0) & (dbs_pivot_all.EDP==1)].groupby('State', as_index=False)['Citizen GUID'].nunique()\n",
    "        dbs_tracker_weekly_all_status = dbs_tracker_weekly_all_status.merge(EDP_sch_all, on='State', how='left').fillna(0)\n",
    "        dbs_tracker_weekly_all_status.rename(columns={'Citizen GUID':'EDP+Scheme'}, inplace=True)\n",
    "\n",
    "        # EDP + MSME YC + Scheme All Status\n",
    "        edp_msmeYC_sch_all = dbs_pivot_all[(dbs_pivot_all.MSMEYC>0) & (dbs_pivot_all['E-Gov']>0) & (dbs_pivot_all.MSME_Sch==0) & (dbs_pivot_all.EDP==1)].groupby('State', as_index=False)['Citizen GUID'].nunique()\n",
    "        dbs_tracker_weekly_all_status = dbs_tracker_weekly_all_status.merge(edp_msmeYC_sch_all, on='State', how='left').fillna(0)\n",
    "        dbs_tracker_weekly_all_status.rename(columns={'Citizen GUID':'EDP+YC(MSME)+Scheme'}, inplace=True)\n",
    "\n",
    "        # EDP Only All Status\n",
    "        only_EDP_all = dbs_pivot_all[(dbs_pivot_all['Grand Total']==1) & (dbs_pivot_all.EDP==1)].groupby('State', as_index=False)['Citizen GUID'].nunique()\n",
    "        dbs_tracker_weekly_all_status = dbs_tracker_weekly_all_status.merge(only_EDP_all, on='State', how='left').fillna(0)\n",
    "        dbs_tracker_weekly_all_status.rename(columns={'Citizen GUID':'EDP Only'}, inplace=True)\n",
    "\n",
    "        #dbs_pivot = pd.pivot_table(og_DF[(og_DF.Status==\"Submitted\" and og_DF.Status==\"Benefit Received\") & (og_DF[\"Docket Submitted Date\"] <= yesterday and og_DF['Benefit received Date'] <= yesterday) & (og_DF[\"isYC\"] == 'Yes') & (og_DF[\"Scheme Category\"] == \"E-Gov\")], index=['State','Citizen GUID'], columns='Scheme Category', values='Case Id', aggfunc='count', fill_value=0)\n",
    "        # Ds  Pivot till yesterday\n",
    "        dbs_pivot = pd.pivot_table(og_DF[(og_DF.Status==\"Submitted\") & (og_DF[\"Docket Submitted Date\"] <= yesterday)], index=['State','Citizen GUID'], columns='Scheme Category', values='Case Id', aggfunc='count', fill_value=0)\n",
    "        dbs_pivot = dbs_pivot.reset_index()\n",
    "        category_cols = [c for c in dbs_pivot.columns if c not in ['State','Citizen GUID']]\n",
    "        dbs_pivot['Grand Total'] = dbs_pivot[category_cols].sum(axis=1)\n",
    "\n",
    "        #DS only 2 schemes\n",
    "        try:\n",
    "            DS_2_sch = dbs_pivot[(dbs_pivot[\"E-Gov\"]>1) & (dbs_pivot.DFL==0) & (dbs_pivot.YC==0) & (dbs_pivot.EDP==0) & (dbs_pivot.MSMEYC==0) & (dbs_pivot.MSME_Sch==0)].groupby(\"State\",as_index = False)['Citizen GUID'].nunique()\n",
    "            dbs_tracker_weekly_all_status[\"DS_2_sch\"] = DS_2_sch[\"Citizen GUID\"]\n",
    "        except:\n",
    "            DS_2_sch = dbs_pivot[(dbs_pivot[\"E-Gov\"]>1) & (dbs_pivot.DFL==0) & (dbs_pivot.EDP==0)].groupby(\"State\",as_index = False)['Citizen GUID'].nunique()\n",
    "            dbs_tracker_weekly_all_status[\"DS_2_sch\"] = DS_2_sch[\"Citizen GUID\"]\n",
    "\n",
    "        dbs_pivot = pd.pivot_table(og_DF[(og_DF.Status==\"Benefit Received\") & (og_DF['Benefit received Date'] <= yesterday)], index=['State','Citizen GUID'], columns='Scheme Category', values='Case Id', aggfunc='count', fill_value=0)\n",
    "        dbs_pivot = dbs_pivot.reset_index()\n",
    "        category_cols = [c for c in dbs_pivot.columns if c not in ['State','Citizen GUID']]\n",
    "        dbs_pivot['Grand Total'] = dbs_pivot[category_cols].sum(axis=1)\n",
    "        \n",
    "        #citizen with any 1 services\n",
    "        any_1_service = dbs_pivot.query('`EDP`==0 and `Grand Total`==1').groupby('State' , as_index=False)['Citizen GUID'].nunique()\n",
    "        any_2_ormore_service = dbs_pivot.query('`EDP`==0 and `Grand Total`>1').groupby('State' , as_index=False)['Citizen GUID'].nunique()\n",
    "        \n",
    "        print(\"Any 1 Service:\\n\",any_1_service)\n",
    "        print(\"*******************************\")\n",
    "        print(\"Any 2 Service:\\n\",any_2_ormore_service)\n",
    "        \n",
    "        # Exporting Data to DBS Tracker\n",
    "        print('DBS 2.0 weekly table generation completed! Now exporting to Excel...')\n",
    "        with pd.ExcelWriter(r'F:\\HQ Google Drive\\Shared drives\\Programs Data Team\\DBS Tracker.xlsx', if_sheet_exists='overlay', mode='a', engine='openpyxl') as writer:\n",
    "            dbs_tracker_weekly.to_excel(writer, sheet_name='Weekly Table', index=False)\n",
    "            dbs_tracker_weekly_all_status.to_excel(writer, sheet_name='Weekly Table', index=False, startrow=10)\n",
    "        print('DBS 2.0 weekly table exported successfully...')\n",
    "\n",
    "#    elif PID == 'PID/DBSBA/2023/DI/YK/0266':\n",
    "#        with pd.ExcelWriter(r'F:\\Haqdarshak Data\\DBS\\DBS - Daily Tracker.xlsx', if_sheet_exists='replace', mode='a', engine='openpyxl') as writer:\n",
    "#            pd.concat([unique_data, dfl_uniques], ignore_index=True).to_excel(writer, sheet_name='Schemes Data', index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e53822a7-6b5c-415c-bc37-79d5e2040f74",
   "metadata": {},
   "source": [
    "og_DF = pd.read_excel(r'F:\\Haqdarshak Data\\DBS\\Phase 2\\DBS 2.0 - All Data - 22-10-2025.xlsx', sheet_name='Schemes Data')\n",
    "og_DF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea9c3d3-5c4f-4422-b9e6-e81a55e4d197",
   "metadata": {
    "id": "6ea9c3d3-5c4f-4422-b9e6-e81a55e4d197",
    "outputId": "56f90ab7-969a-4f16-8f8a-005f3a0359e8"
   },
   "outputs": [],
   "source": [
    "og_DF.Gender.value_counts().plot.barh(color='#F5A3C7')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0390504-df65-4078-910f-cb31251755df",
   "metadata": {
    "id": "a0390504-df65-4078-910f-cb31251755df",
    "outputId": "7be5fa37-a6d8-4097-f13c-ceec397e2718"
   },
   "outputs": [],
   "source": [
    "og_DF.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96dd2ce-f719-43db-bbd8-cd7e8afdde95",
   "metadata": {
    "id": "e96dd2ce-f719-43db-bbd8-cd7e8afdde95"
   },
   "source": [
    "# Exporting original transformed data to excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14acd2a-e7c0-4f5e-a526-3871dca1718f",
   "metadata": {
    "id": "c14acd2a-e7c0-4f5e-a526-3871dca1718f",
    "outputId": "95bc8ebc-d8c5-4610-a439-160f9f759168"
   },
   "outputs": [],
   "source": [
    "# Notifying user to give consent..\n",
    "notification = Notify()\n",
    "notification.title = \"Consent Required\"\n",
    "notification.message = \"Hello Akash! Waiting for your consent.\"\n",
    "notification.audio = \"E:/Music/Ringtone/Consent required1.wav\"\n",
    "notification.send()\n",
    "\n",
    "# Asking consent from user for original data export.\n",
    "consent = input(\"Do you want to export original data? (Y/N)\\n\")\n",
    "\n",
    "if consent in [\"Y\",\"y\"]:\n",
    "    if len(og_DF) > 1048576:\n",
    "        for state in og_DF.State.value_counts().index.to_list():\n",
    "            with pd.ExcelWriter('C:\\\\Python\\\\export\\\\'+state[0:3]+'_'+fn.split('_')[2]+' og_data.xlsx') as writer:            \n",
    "                print(f\"Exporting full {state} data.\")\n",
    "                og_DF[og_DF.State == state].to_excel(writer, sheet_name=state, index=False)\n",
    "                rejectedDF[rejectedDF.State == state].to_excel(writer, sheet_name='Abort_Rejected', index=False) # Exporting statewise rejected data\n",
    "                #writer.close()\n",
    "                print(\"Export to excel success!\")\n",
    "    else:\n",
    "        with pd.ExcelWriter('C:\\\\Python\\\\export\\\\'+fn.split('_')[2]+'_'+fn.split('_')[3]+' og_data.xlsx') as writer:\n",
    "            print(f\"Exporting full data.\")\n",
    "            og_DF.to_excel(writer, sheet_name='Raw Data', index=False)\n",
    "            rejectedDF.to_excel(writer, sheet_name='Abort_Rejected', index=False) # Exporting rejected data\n",
    "            #writer.close()\n",
    "            print(\"Export to excel success!\")\n",
    "else:\n",
    "    print(\"You dont want original data!\")\n",
    "\n",
    "# Asking consent from user for original data export.\n",
    "consent = input(\"Do you want to append LTPCT data? (Y/N)\\n\")\n",
    "\n",
    "if consent in [\"Y\",\"y\"]:\n",
    "    # Asking consent from user for original data export.\n",
    "    consent = input(\"Have you mapped indicators? (Y/N)\\n\")\n",
    "\n",
    "    if consent in [\"N\",\"n\"]:\n",
    "        print(\"Cant proceed further unless indicator's mapped to the data. Kindly, comback after mapping the indicators!\")\n",
    "    elif consent in [\"Y\",\"y\"]:\n",
    "        # Asking for project file name from the user\n",
    "        file_Name = input('Please enter file name = ')\n",
    "        src = r\"C:\\Users\\akash\\Downloads\"\n",
    "        dest = \"C:\\\\Python\\\\read\\\\\"\n",
    "        if file_Name in os.listdir(src):\n",
    "            shutil.move(os.path.join(src,file_Name),os.path.join(dest,file_Name))\n",
    "\n",
    "        ltpct = csvORexcel()\n",
    "\n",
    "        #with pd.ExcelWriter(r'F:\\Haqdarshak Data\\LTPCT\\Ahwa & Vikramgad\\LTPCT (Ahwa, Vikramgad & Subir)_Monthly Dashboard.xlsx', if_sheet_exists='replace', mode='a', engine='openpyxl') as writer:\n",
    "        #        ltpct.to_excel(writer, sheet_name='Schemes Data', index=False)\n",
    "                #writer.close()\n",
    "        #print(\"LTPCT (Ahwa, Vikramgad & Subir) Monthly Dashboard Updated!\")\n",
    "        with pd.ExcelWriter(r'F:\\Haqdarshak Data\\LTPCT\\Phase 2\\LTPCT P2 (Ahwa, Vikramgad & Subir)_Quarterly Dashboard.xlsx', if_sheet_exists='replace', mode='a', engine='openpyxl') as writer:\n",
    "            print(\"Updating LTPCT P2 Quarterly Dashboard.\")\n",
    "            ltpct.to_excel(writer, sheet_name='Schemes Data', index=False)\n",
    "        with pd.ExcelWriter(r'F:\\Haqdarshak Data\\LTPCT\\Phase 2\\LTPCT P2 (Ahwa, Vikramgad & Subir)_Monthly Dashboard.xlsx', if_sheet_exists='replace', mode='a', engine='openpyxl') as writer:\n",
    "            print(\"Updating LTPCT P2 Monthly Dashboard.\")\n",
    "            ltpct.to_excel(writer, sheet_name='Schemes Data', index=False)\n",
    "                #writer.close()\n",
    "        print(\"LTPCT (Ahwa, Vikramgad & Subir) Quarterly & Monthly Dashboard Updated!\")\n",
    "    else:\n",
    "        print(\"Abort!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d48514e-9c21-4917-a853-362b44c5e549",
   "metadata": {
    "id": "7d48514e-9c21-4917-a853-362b44c5e549",
    "outputId": "14ef220b-1c16-49a6-cb16-1dc37329d111"
   },
   "outputs": [],
   "source": [
    "# Changing status values to O_S_BR\n",
    "og_summary = pd.DataFrame(og_DF[og_DF['Scheme Category']=='E-Gov'].Status.value_counts())\n",
    "og_summary.rename(columns={'count':'Total Application'}, inplace=True)\n",
    "og_summary.reset_index(inplace=True)\n",
    "og_summary.loc[len(og_summary.index)] = ['Grand Total', og_summary['Total Application'].sum()]\n",
    "try:\n",
    "    og_summary = og_summary.iloc[[2,1,0,3]]\n",
    "    plt.pie(og_summary['Total Application'][0:3], labels=og_summary['Status'][0:3], rotatelabels=True, autopct=\"%1.1f%%\", explode=[0.01,0.05,0.09])\n",
    "except IndexError:\n",
    "    og_summary = og_summary.iloc[[1,0,2]]\n",
    "    plt.pie(og_summary['Total Application'][0:2], labels=og_summary['Status'][0:2], rotatelabels=True, autopct=\"%1.1f%%\", explode=[0.01,0.05])\n",
    "og_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8aad7a7-ea73-4cba-bed9-cb58c0fbeee8",
   "metadata": {
    "id": "c8aad7a7-ea73-4cba-bed9-cb58c0fbeee8"
   },
   "source": [
    "# Notifying user using Notification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618b6a2e-a4c5-4595-904a-2089f94a151b",
   "metadata": {
    "id": "618b6a2e-a4c5-4595-904a-2089f94a151b",
    "outputId": "c65e4227-f143-4460-cab5-4ec8c38214c8"
   },
   "outputs": [],
   "source": [
    "notification.title = \"Process Execution Alert\"\n",
    "notification.message = \"Hello Akash! Python script execution has completed.\"\n",
    "notification.audio = \"E:/Music/Ringtone/Process Execution Alert.wav\"\n",
    "notification.send(block=False)\n",
    "print('Script execution completed!')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a5f28f65-f33f-4aa9-a1be-4038be8b4045",
   "metadata": {
    "id": "6435154a-aa5c-4186-867c-85d9c5b62a72",
    "outputId": "a4a60fee-c251-4597-d527-1ab21e1b0e72"
   },
   "source": [
    "unique_data.isna().sum()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bd6567d8-fd9a-4ec1-a45a-3d52deede84e",
   "metadata": {
    "id": "939d8e4f-0f9b-4da1-ba58-3437ec463a07"
   },
   "source": [
    "cat_col = []\n",
    "num_col = []\n",
    "\n",
    "for c in unique_data.select_dtypes(exclude='datetime'):\n",
    "    if unique_data[c].dtype == 'object':\n",
    "        if c not in ['Case Id','Scheme/Doc GUID','Citizen GUID','Family GUID']:\n",
    "            cat_col.append(c)\n",
    "    elif unique_data[c].dtype in ['int64','float64']:\n",
    "        if c not in ['Mobile','AMS ID']:\n",
    "            num_col.append(c)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "046a632b-c5fb-457f-9c97-9f746ebcbae4",
   "metadata": {
    "id": "72e87086-e8ff-4e8f-882b-84e4e9a4c030",
    "outputId": "ac408dc4-4f23-43b1-d5e2-6b35a18c8595"
   },
   "source": [
    "cat_col = ['Status',\n",
    " 'HD Suspected Cases',\n",
    " 'State',\n",
    " 'District','Gender','HD ID','Opsco name',\n",
    " 'isYC',\n",
    " 'Parent Scheme',\n",
    " 'Scheme type',\n",
    " 'Scheme Category',]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "107a2088-38fa-4700-a7d6-f13ce84fca21",
   "metadata": {
    "id": "dbefbc8b-1050-4f7e-b75b-ef197bc01d3c",
    "outputId": "7db086c9-0c11-456c-80ed-57f6e8105e46"
   },
   "source": [
    "num_col"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c2b49ab8-4987-4a19-8599-766f67cfccc8",
   "metadata": {
    "id": "47fbf4de-c303-4e6a-8f2b-7cdcc0622f7f",
    "outputId": "c061d173-18ce-47db-f3da-313abaa42345"
   },
   "source": [
    "scaler = StandardScaler()\n",
    "unique_data_scaled = scaler.fit_transform(unique_data[num_col])\n",
    "unique_data_scaled_numcol = pd.DataFrame(unique_data_scaled, columns=num_col)\n",
    "unique_data_scaled_numcol.head()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "81683a1d-5558-41d4-bb6e-a5a5af82bb38",
   "metadata": {
    "id": "797ce6a5-f4a8-4c6d-a91b-f350d60d4d78",
    "outputId": "c4dbb804-f12d-4b00-f3b0-a53344df9648"
   },
   "source": [
    "sns.heatmap(unique_data_scaled_numcol.corr())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cc612ae3-37a5-4f42-9640-00ab7113d581",
   "metadata": {
    "id": "5dd7e166-3b7b-430a-ae8e-c10dbb044b35"
   },
   "source": [
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "#unique_data_scaled = pd.DataFrame(unique_data_scaled, columns=num_col)\n",
    "unique_encoded = encoder.fit_transform(unique_data[cat_col])\n",
    "unique_data_encode = pd.DataFrame(unique_encoded, columns=encoder.get_feature_names_out(cat_col))\n",
    "one_hot_unique = pd.concat([unique_data_scaled_numcol,unique_data_encode], axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5820c3ed-35a0-4490-b648-5c65df806f48",
   "metadata": {
    "id": "1f78524d-cd7d-47ab-a9d3-2771dd5ea7fb",
    "outputId": "975f5730-4d89-4f6f-b4be-b03d78b4c3cc"
   },
   "source": [
    "one_hot_unique.head()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5e8de706-0a71-4d8c-ba4d-b52d92dbfc52",
   "metadata": {
    "id": "54858e17-3ac9-40ba-827b-1b6bb889d3fd"
   },
   "source": [
    "train_data = one_hot_unique.sample(frac=0.8,random_state=1).reset_index(drop=True)\n",
    "test_data = one_hot_unique.drop(index=train_data.index).reset_index(drop=True)\n",
    "print(\"Data for modelling - Training \"+str(train_data.shape))\n",
    "print(\"Data for prediction - Testing\"+str(test_data.shape))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7ea760d7-4d50-4905-a3bb-10f836d90c0d",
   "metadata": {},
   "source": [
    "from pycaret.classification import *\n",
    "from pycaret.clustering import *\n",
    "classification_model = setup(data=train_data,session_id=1243)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "44b29bb9-6df8-4b1b-81be-d62c7b43ed70",
   "metadata": {},
   "source": [
    "kmean = create_model('kmeans')\n",
    "kmean"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f50ef327-9649-4126-9dae-883781c189ad",
   "metadata": {},
   "source": [
    "clustered = assign_model(kmean)\n",
    "clustered.head()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bf43b6f4-20bd-48d2-80de-a9fb560ff585",
   "metadata": {},
   "source": [
    "evaluate_model(kmean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead8f26a-b7c6-444e-b078-127fc976ac8d",
   "metadata": {},
   "source": [
    "# Delete Variables and clean the space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09931fc1-c79c-47df-9e8d-418800205d2c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "consent = input(\"Do you want to delete custom variables\")\n",
    "if consent in ['Y','y']:\n",
    "    for name in globals():\n",
    "        if not name.startswith(('__', '_')):  # Exclude built-in names like __name__, __doc__ etc.\n",
    "            del name\n",
    "            print(f\"Deleted the {name} variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c24f461-e456-4bf5-9723-ebf389838529",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c486dc05-690f-4580-a261-b04309785240",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7420009b-5835-447a-b380-efda92a096da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
