{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaa119e-eb6e-4d3b-b883-661f7f340532",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install unstructured[pdf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3fa4cabd-e6a1-4b0b-8b6e-9fbf61e2c6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredURLLoader\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import unstructured\n",
    "from PyPDF2 import PdfReader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "43880f8b-dfd9-448f-a358-cbf4fd666851",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://mohua.gov.in/upload/uploadfiles/files/PMSVANidhi%20Guideline_English.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ac9fbe3d-ccca-46bc-b4a3-dcefe6859f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error fetching or processing https://mohua.gov.in/upload/uploadfiles/files/PMSVANidhi%20Guideline_English.pdf, exception: partition_pdf is not available. Install the pdf dependencies with pip install \"unstructured[pdf]\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "loader = UnstructuredURLLoader([url])\n",
    "loader.load()\n",
    "#texts = text_splitter.split_text(loader.load)\n",
    "#embeddings = OpenAIEmbeddings()\n",
    "#embedding_vectors = embeddings.embed_documents(texts)\n",
    "#faiss_index = FAISS.from_documents(embedding_vectors)\n",
    "#faiss_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068134eb-08b1-44b8-94fc-5454407977bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(pdf):\n",
    "    # Create a PdfReader object to read the PDF file\n",
    "    pdf_reader = PdfReader(pdf)\n",
    "    \n",
    "    # Initialize an empty string to store the extracted text from the PDF\n",
    "    text = \"\"\n",
    "    \n",
    "    # Iterate through each page in the PDF\n",
    "    for page in pdf_reader.pages:\n",
    "        # Extract the text content from the current page and append it to the 'text' variable\n",
    "        text += page.extract_text()\n",
    "    \n",
    "    # Use the text_to_doc_splitter function to split the extracted text into a document\n",
    "    document = text_to_doc_splitter(text)\n",
    "    \n",
    "    # Return the resulting document\n",
    "    return document   \n",
    "\n",
    "load_pdf(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01d05be5-23b5-4e47-aad8-b74f41b773db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai, requests, pickle, os, streamlit as st\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA \n",
    "\n",
    "# Load OpenAI API key from .config file\n",
    "def load_api_key():\n",
    "    try:\n",
    "        with open(r'C:\\Users\\akash\\Documents\\Haqdarshak\\Work\\Scheme_Research_Application\\.config', 'r') as file:\n",
    "            api_key = file.readline().strip()\n",
    "            os.environ[\"OPENAI_API_KEY\"] = api_key # Creating virtual environment to store Open AI API key virtuallly.\n",
    "            return api_key\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error loading API key: {e}\")\n",
    "        return None\n",
    "\n",
    "api_key = load_api_key()\n",
    "openai.api_key = api_key\n",
    "\n",
    "def fetch_article_content(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status() \n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error fetching content from {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Defining a function to vectorize the text for embedding\n",
    "def generate_embeddings(text, api_key):\n",
    "    try:\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=10, separator=\"\\n,.:\")\n",
    "        texts = text_splitter.split_text(text)\n",
    "        embeddings = OpenAIEmbeddings(openai_api_key=api_key)\n",
    "        return embeddings.embed_documents(texts)\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error generating embeddings: {e}\")\n",
    "        return e\n",
    "\n",
    "# Defining a function to store indexes of embeddings\n",
    "def save_faiss_index(index, path):\n",
    "    try:\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(index, f)\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error saving FAISS index: {e}\")\n",
    "\n",
    "# Defining a function to load indexes from the pickle store\n",
    "def load_faiss_index(path):\n",
    "    try:\n",
    "        with open(path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error loading FAISS index: {e}\")\n",
    "        return None\n",
    "\n",
    "# Defining a function to generate summary of the document\n",
    "def get_summary(faiss_index, openai_api_key):\n",
    "   llm = ChatOpenAI(temperature=0.5, model_name='gpt-4o', openai_api_key = openai_api_key)\n",
    "   pdf_qa = RetrievalQA.from_chain_type(\n",
    "       llm,\n",
    "       retriever=faiss_index.as_retriever(search_kwargs={'k': 4}),\n",
    "       chain_type=\"stuff\",\n",
    "       )\n",
    "   # Defining the Query. \n",
    "   query = \"\"\" Write a summary for the document passed to you. You are getting a PDF document and your job \\\n",
    "        is to interpret the information in the document and to provide a summary. This summary should highlight \\\n",
    "        key points like Scheme Benefits, Scheme Application Process, Eligibility, and Documents required\"\"\"\n",
    "    \n",
    "    # Executing the RetreivalIQA model with the defined query. \n",
    "   result = pdf_qa.run(query)\n",
    "\n",
    "   return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c6b6851-0032-4a1c-bb9c-210fab27abaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "openai.RateLimitError(\"Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_embeddings(fetch_article_content(\"https://github.com/JPSchloss/Open-AI-Summarizer-App/blob/main/ChatGPTSummarizer.pdf\"), api_key)\n",
    "#generate_embeddings(fetch_article_content(\"https://arxiv.org/pdf/1706.03762\"), api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9e10ca-d53c-46a8-9e7b-493122f01f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title of web interface.\n",
    "st.title(\"Scheme Research Application\")\n",
    "\n",
    "# Getting URL of the file from user.\n",
    "urls = st.sidebar.text_area(\"Enter URLs (one per line):\")\n",
    "\n",
    "# Creating action button \"Process URL\" and steps to follow.\n",
    "if st.sidebar.button(\"Process URLs\"):\n",
    "    urls = urls.split(\"\\n\")\n",
    "    all_texts = []\n",
    "    for url in urls:\n",
    "        with st.spinner(f\"Fetching content from: {url}\"):\n",
    "            content = fetch_article_content(url)\n",
    "        if content:\n",
    "            all_texts.append(content)\n",
    "    \n",
    "    combined_text = \"\\n\".join(all_texts)\n",
    "\n",
    "\n",
    "    if combined_text:\n",
    "        with st.spinner(\"Generating embeddings...\"):\n",
    "            embedding_vectors = generate_embeddings(combined_text, api_key)\n",
    "        \n",
    "        if embedding_vectors:\n",
    "            st.write(\"Indexing with FAISS...\")\n",
    "            faiss_index = FAISS.from_documents(embedding_vectors)\n",
    "            save_faiss_index(faiss_index, r'C:\\Users\\akash\\Documents\\Haqdarshak\\Work\\Scheme_Research_Application\\faiss_store_openai.pkl')\n",
    "            st.success(\"Processing complete and FAISS index saved.\")\n",
    "            with st.spinner(\"Generating summary...\"):\n",
    "                summary = get_summary(faiss_index, api_key)\n",
    "                st.write(summary)\n",
    "\n",
    "        else:\n",
    "            st.error(\"Failed to generate embeddings.\")\n",
    "    else:\n",
    "        st.error(\"Failed to fetch or combine content from URLs.\")\n",
    "\n",
    "question = st.text_input(\"Enter your question:\")\n",
    "if st.button(\"Ask Question\"):\n",
    "    faiss_index = load_faiss_index(r'C:\\Users\\akash\\Documents\\Haqdarshak\\Work\\Scheme_Research_Application\\faiss_store_openai.pkl')\n",
    "    if faiss_index and question:\n",
    "        st.write(\"Fetching answer...\")\n",
    "        similar_docs = faiss_index.similarity_search(question)\n",
    "        if similar_docs:\n",
    "            answer = similar_docs[0]['text']\n",
    "            st.write(f\"Answer: {answer}\")\n",
    "            st.write(f\"Source URL: {similar_docs[0]['metadata']['source']}\")\n",
    "        else:\n",
    "            st.error(\"No relevant documents found.\")\n",
    "    else:\n",
    "        st.error(\"Failed to load FAISS index or no question provided.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
