{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad9c36a-a3b5-46fd-9246-841bbf271893",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "102c4109-217c-4869-9ef6-66b306610f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from IPython.display import display, Markdown, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "02297a9a-6be2-4e15-ba16-2ef2acb92c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key has imported!\n"
     ]
    }
   ],
   "source": [
    "f = open('C:\\Python\\geminiAPIKey.txt')\n",
    "key = f.read()\n",
    "genai.configure(api_key=key)\n",
    "print(\"API key has imported!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eb4774f5-87d4-46c5-8321-2275c5c88f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/embedding-gecko-001\n",
      "models/gemini-2.5-flash\n",
      "models/gemini-2.5-pro\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.5-flash-preview-tts\n",
      "models/gemini-2.5-pro-preview-tts\n",
      "models/gemma-3-1b-it\n",
      "models/gemma-3-4b-it\n",
      "models/gemma-3-12b-it\n",
      "models/gemma-3-27b-it\n",
      "models/gemma-3n-e4b-it\n",
      "models/gemma-3n-e2b-it\n",
      "models/gemini-flash-latest\n",
      "models/gemini-flash-lite-latest\n",
      "models/gemini-pro-latest\n",
      "models/gemini-2.5-flash-lite\n",
      "models/gemini-2.5-flash-image-preview\n",
      "models/gemini-2.5-flash-image\n",
      "models/gemini-2.5-flash-preview-09-2025\n",
      "models/gemini-2.5-flash-lite-preview-09-2025\n",
      "models/gemini-3-pro-preview\n",
      "models/gemini-3-flash-preview\n",
      "models/gemini-3-pro-image-preview\n",
      "models/nano-banana-pro-preview\n",
      "models/gemini-robotics-er-1.5-preview\n",
      "models/gemini-2.5-computer-use-preview-10-2025\n",
      "models/deep-research-pro-preview-12-2025\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n",
      "models/gemini-embedding-001\n",
      "models/aqa\n",
      "models/imagen-4.0-generate-preview-06-06\n",
      "models/imagen-4.0-ultra-generate-preview-06-06\n",
      "models/imagen-4.0-generate-001\n",
      "models/imagen-4.0-ultra-generate-001\n",
      "models/imagen-4.0-fast-generate-001\n",
      "models/veo-2.0-generate-001\n",
      "models/veo-3.0-generate-001\n",
      "models/veo-3.0-fast-generate-001\n",
      "models/veo-3.1-generate-preview\n",
      "models/veo-3.1-fast-generate-preview\n",
      "models/gemini-2.5-flash-native-audio-latest\n",
      "models/gemini-2.5-flash-native-audio-preview-09-2025\n",
      "models/gemini-2.5-flash-native-audio-preview-12-2025\n"
     ]
    }
   ],
   "source": [
    "for m in genai.list_models():\n",
    "    print(m.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "565116a5-ea8a-42f0-a2e3-ed2e02793997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "In our solar system, Earth is a terrestrial planet, the third from the Sun, and uniquely known for harboring complex life. It is distinguished by vast oceans of liquid water covering over 70% of its surface, and a dynamic atmosphere rich in nitrogen and oxygen. These crucial elements, combined with its protective magnetic field and a stable axial tilt influenced by its single natural satellite, the Moon, create conditions ideal for diverse ecosystems. Earth is also the densest of all the planets in our solar system, making it geologically active."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = genai.GenerativeModel(model_name='gemini-2.5-flash')\n",
    "custom_config = genai.types.GenerationConfig(temperature=1.0)\n",
    "prompt = \"\"\"Generate some factual information to complete the following sentence in 5-6 lines:\n",
    "            In our solar system, Eath is a\"\"\"\n",
    "def genAIoutput(prompt, custom_config=custom_config):\n",
    "    response = model.generate_content(prompt,generation_config=custom_config)\n",
    "    display(Markdown(response.text))\n",
    "\n",
    "genAIoutput(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "df047d35-507e-4471-b83f-f5d2e6b9f5a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "In our solar system, Earth is a **terrestrial planet, the third from the Sun.** It is the **only known astronomical object to harbor life.** Earth's surface is **covered mostly by water (about 71%), with the remaining 29% being continents and islands.** It has a **single natural satellite, the Moon,** and a **magnetic field that shields it from harmful solar radiation.** The planet's atmosphere is **composed primarily of nitrogen and oxygen.**\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = genai.GenerativeModel(model_name='gemini-2.0-flash')\n",
    "custom_config = genai.types.GenerationConfig(temperature=1.0)\n",
    "prompt = \"\"\"Generate some factual information to complete the following sentence in 5-6 lines:\n",
    "            In our solar system, Eath is a\"\"\"\n",
    "def genAIoutput(prompt, custom_config=custom_config):\n",
    "    response = model.generate_content(prompt,generation_config=custom_config)\n",
    "    display(Markdown(response.text))\n",
    "\n",
    "genAIoutput(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc2b971-5026-4f16-8f5b-8bd0cc17eb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"What is Machin Learning and also explain in details about supervised and unsupervised ML\"\"\"\n",
    "genAIoutput(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc5d2de2-ad0c-42f4-a77e-157e179cf967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from IPython.display import display, Markdown, HTML\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9a90861-b359-4d7f-a9cf-6ce388797662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key has imported!\n"
     ]
    }
   ],
   "source": [
    "f = open('C:\\Python\\geminiAPIKey.txt')\n",
    "key = f.read()\n",
    "genai.configure(api_key=key)\n",
    "print(\"API key has imported!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04dfdb8a-ae05-4aeb-9ea5-a87792b22c4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGoogleGenerativeAI(model='models/gemini-2.5-flash', google_api_key=SecretStr('**********'), temperature=1.0, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000002348A387E50>, default_metadata=(), model_kwargs={})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = init_chat_model(model=\"gemini-2.5-flash\", model_provider=\"google-genai\", api_key=key, temperature=1)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c209b7c-4902-4aa8-a144-f0b809001c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the text to translate: Hi, My name is Akash! I'm here to learn Artificial Intellegence.\n",
      "Enter the translation language: Marathi\n"
     ]
    }
   ],
   "source": [
    "system_template = \"Translate the following text into {language}\"\n",
    "user_input = input(\"Enter the text to translate:\")\n",
    "language = input(\"Enter the translation language:\")\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")\n",
    "\n",
    "prompt = prompt_template.invoke({\"language\": language, \"text\": user_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "758e434f-cd52-4cb3-b5f1-b2759239982e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "‡§®‡§Æ‡§∏‡•ç‡§ï‡§æ‡§∞, ‡§Æ‡§æ‡§ù‡•á ‡§®‡§æ‡§µ ‡§Ü‡§ï‡§æ‡§∂ ‡§Ü‡§π‡•á! ‡§Æ‡•Ä ‡§Ø‡•á‡§•‡•á ‡§ï‡•É‡§§‡•ç‡§∞‡§ø‡§Æ ‡§¨‡•Å‡§¶‡•ç‡§ß‡§ø‡§Æ‡§§‡•ç‡§§‡§æ ‡§∂‡§ø‡§ï‡§£‡•ç‡§Ø‡§æ‡§∏‡§æ‡§†‡•Ä ‡§Ü‡§≤‡•ã ‡§Ü‡§π‡•á."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = model.invoke(prompt)\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1af3272d-5f47-4422-9c03-f65cb0565ed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "10"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ:  None\n"
     ]
    }
   ],
   "source": [
    "print(\"ü§ñ: \",display(Markdown(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ea315d-6053-4f87-b1d8-9790acbb3fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User: Hi, how can i create a virtual environment for a specific python version?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "ü§ñ : Hello! As a Data Scientist, I can definitely help you with that. Creating a virtual environment for a specific Python version is a common and very good practice.\n",
       "\n",
       "Here's how you can do it, primarily using the built-in `venv` module (which is generally preferred for simple Python projects) or `conda` (very popular in data science):\n",
       "\n",
       "### Method 1: Using `venv` (Built-in Python Module)\n",
       "\n",
       "This method relies on having the *specific Python version already installed* on your system.\n",
       "\n",
       "1.  **Check Available Python Versions:**\n",
       "    First, verify which Python executables you have. You can often find them like `python3.8`, `python3.9`, `python3.10`, etc.\n",
       "    ```bash\n",
       "    which python3.8\n",
       "    which python3.9\n",
       "    # or just list python versions in common install locations\n",
       "    ls /usr/bin/python*\n",
       "    ls /usr/local/bin/python*\n",
       "    ```\n",
       "\n",
       "2.  **Use the Specific Python Executable to Create the Environment:**\n",
       "    Once you identify the path to the desired Python version's executable, you use *that specific executable* to create the virtual environment.\n",
       "\n",
       "    Let's say you want to create an environment using Python 3.9:\n",
       "    ```bash\n",
       "    /usr/bin/python3.9 -m venv my_project_env_py39\n",
       "    # OR, if python3.9 is in your PATH\n",
       "    python3.9 -m venv my_project_env_py39\n",
       "    ```\n",
       "    This command tells Python 3.9 to create a virtual environment named `my_project_env_py39` in the current directory.\n",
       "\n",
       "3.  **Activate the Environment:**\n",
       "    ```bash\n",
       "    source my_project_env_py39/bin/activate\n",
       "    ```\n",
       "    (On Windows, it would be `my_project_env_py39\\Scripts\\activate`)\n",
       "\n",
       "4.  **Verify the Python Version:**\n",
       "    After activation, check the Python version:\n",
       "    ```bash\n",
       "    python --version\n",
       "    ```\n",
       "    It should now show `Python 3.9.x`.\n",
       "\n",
       "### Method 2: Using `conda` (Anaconda/Miniconda)\n",
       "\n",
       "If you use Anaconda or Miniconda, it's very straightforward and can even download and install the specified Python version if you don't have it.\n",
       "\n",
       "1.  **Create the Environment with a Specific Python Version:**\n",
       "    ```bash\n",
       "    conda create -n my_project_env_py38 python=3.8\n",
       "    ```\n",
       "    Replace `my_project_env_py38` with your desired environment name and `python=3.8` with your target version (e.g., `python=3.9`, `python=3.10`).\n",
       "\n",
       "2.  **Activate the Environment:**\n",
       "    ```bash\n",
       "    conda activate my_project_env_py38\n",
       "    ```\n",
       "\n",
       "3.  **Verify the Python Version:**\n",
       "    ```bash\n",
       "    python --version\n",
       "    ```\n",
       "    It should show `Python 3.8.x`.\n",
       "\n",
       "Both methods are effective. For data science projects, `conda` often provides a more comprehensive environment management solution, especially when dealing with complex scientific libraries that have C/C++ dependencies. For simpler Python projects, `venv` is perfectly adequate and doesn't require any additional installation beyond Python itself."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User: how to register this new venv in jupyter lab kernel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "ü§ñ : That's a great question and a common task when working with different project environments! As a Data Scientist's virtual assistant, I can definitely guide you through registering a new virtual environment (venv) as a Jupyter Lab kernel.\n",
       "\n",
       "Here's how you can do it:\n",
       "\n",
       "1.  **Activate your new virtual environment:**\n",
       "    First, you need to activate the venv where you want to install the kernel.\n",
       "    ```bash\n",
       "    # On Linux/macOS\n",
       "    source /path/to/your/venv/bin/activate\n",
       "\n",
       "    # On Windows (Command Prompt)\n",
       "    \\path\\to\\your\\venv\\Scripts\\activate.bat\n",
       "\n",
       "    # On Windows (PowerShell)\n",
       "    \\path\\to\\your\\venv\\Scripts\\Activate.ps1\n",
       "    ```\n",
       "    Replace `/path/to/your/venv` with the actual path to your virtual environment.\n",
       "\n",
       "2.  **Install `ipykernel` within your venv:**\n",
       "    Once your venv is active, you need to install the `ipykernel` package. This package provides the Python kernel for Jupyter.\n",
       "    ```bash\n",
       "    pip install ipykernel\n",
       "    ```\n",
       "\n",
       "3.  **Register the venv as a Jupyter kernel:**\n",
       "    Now, you can tell Jupyter about this new kernel. You'll use the `ipykernel install` command.\n",
       "    ```bash\n",
       "    python -m ipykernel install --user --name my_new_venv_kernel --display-name \"My New Venv (Python 3.x)\"\n",
       "    ```\n",
       "    *   `--user`: Installs the kernel for the current user, so it appears in all Jupyter instances you run.\n",
       "    *   `--name my_new_venv_kernel`: This is the internal name Jupyter will use for the kernel (e.g., when you look at `jupyter kernelspec list`). It should be unique and ideally lowercase with underscores.\n",
       "    *   `--display-name \"My New Venv (Python 3.x)\"`: This is the friendly name that will appear in the Jupyter Lab \"New Launcher\" or when you change kernels in a notebook. Make it descriptive!\n",
       "\n",
       "4.  **Restart Jupyter Lab (if it's running):**\n",
       "    If you had Jupyter Lab open while performing these steps, you might need to close and restart it for the new kernel to appear in the launcher or the kernel selection list.\n",
       "\n",
       "After these steps, you should see \"My New Venv (Python 3.x)\" (or whatever you named it) as an available kernel in Jupyter Lab!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "system_template = \"\"\"Act as a virtual assistance of Data Scientist and this will be your primary role but if user asks any questions out of the context then politely deny to answer\n",
    "                    but ask if user want to change your role to answer the question related to specific domain and if user confirms then change your role to \n",
    "                    the specific domain and after answering change your role back to Data Scientist.\"\"\"\n",
    "\n",
    "def chat_bot(user_input):\n",
    "    prompt_template = ChatPromptTemplate.from_messages(\n",
    "        [(\"system\",system_template),(\"user\",\"{text}\")])\n",
    "    prompt = prompt_template.invoke({\"text\":user_input})\n",
    "    response = model.invoke(prompt)\n",
    "    display(Markdown(\"ü§ñ : \"+response.content))\n",
    "\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"User:\")\n",
    "    print(\"\\n\")\n",
    "    if user_input.lower() in ['bye','exit','finish','end']:\n",
    "        break\n",
    "    chat_bot(user_input)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c7448f0-42f6-498b-b25e-a154befa6dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Python\\genai venv\\genai\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "user_query = 'How are you?'\n",
    "promtTemplate = ChatPromptTemplate.from_template(f'''You are an chatbot assistant\n",
    "                                                      who will help user with their queries.\n",
    "                                                      Please keep your answers very brief and short\n",
    "                                                      but to the point and try to save tokens\n",
    "                                                      as much as you can.\n",
    "                                                      query:{user_query}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53afab90-2297-4816-b091-be502acc5dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are an chatbot assistant\\n                                                      who will help user with their queries.\\n                                                      Please keep your answers very brief and short\\n                                                      but to the point and try to save tokens\\n                                                      as much as you can.\\n                                                      query:How are you?'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "promtTemplate.messages[0].prompt.template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28094b8b-ef12-4d33-977e-b7742ca047e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'GenerationConfig' has no attribute 'MediaResolution'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_google_genai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatGoogleGenerativeAI\n\u001b[32m      2\u001b[39m chatbot = ChatGoogleGenerativeAI(model=\u001b[33m'\u001b[39m\u001b[33mgemini-2.5-flash\u001b[39m\u001b[33m'\u001b[39m, api_key=\u001b[33m'\u001b[39m\u001b[33mAIzaSyC2S9_n4uuNuU72YXPJNQD3ud_e4Ll8J3c\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m response = chatbot.invoke(promtTemplate.messages[\u001b[32m0\u001b[39m].prompt.template)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mG:\\Python\\genai venv\\genai\\Lib\\site-packages\\langchain_google_genai\\__init__.py:60\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"**LangChain Google Generative AI Integration**.\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[33;03mThis module integrates Google's Generative AI models, specifically the Gemini series, with the LangChain framework. It provides classes for interacting with chat models and generating embeddings, leveraging Google's advanced AI capabilities.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     57\u001b[39m \n\u001b[32m     58\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_google_genai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_enums\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     61\u001b[39m     HarmBlockThreshold,\n\u001b[32m     62\u001b[39m     HarmCategory,\n\u001b[32m     63\u001b[39m     MediaResolution,\n\u001b[32m     64\u001b[39m     Modality,\n\u001b[32m     65\u001b[39m )\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_google_genai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_models\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatGoogleGenerativeAI\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_google_genai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01membeddings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GoogleGenerativeAIEmbeddings\n",
      "\u001b[36mFile \u001b[39m\u001b[32mG:\\Python\\genai venv\\genai\\Lib\\site-packages\\langchain_google_genai\\_enums.py:6\u001b[39m\n\u001b[32m      4\u001b[39m HarmCategory = genai.HarmCategory\n\u001b[32m      5\u001b[39m Modality = genai.GenerationConfig.Modality\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m MediaResolution = \u001b[43mgenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGenerationConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMediaResolution\u001b[49m\n\u001b[32m      8\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mHarmBlockThreshold\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mHarmCategory\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mModality\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mMediaResolution\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mAttributeError\u001b[39m: type object 'GenerationConfig' has no attribute 'MediaResolution'"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "chatbot = ChatGoogleGenerativeAI(model='gemini-2.5-flash', api_key='AIzaSyC2S9_n4uuNuU72YXPJNQD3ud_e4Ll8J3c')\n",
    "response = chatbot.invoke(promtTemplate.messages[0].prompt.template)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAI_venv",
   "language": "python",
   "name": "genai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
